{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementar un MLP con Backpropagation para resolver el problema de la función XOR \n",
    "\n",
    "<img src=\"figs/fig-MLP_XOR.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "1. **Definir la arquitectura de la red**:  \n",
    "   - La red tendrá 2 entradas (los valores binarios del XOR), una capa oculta con 2 neuronas, y una neurona de salida.\n",
    "   - Usar la función de activación sigmoide en las neuronas de la capa oculta y de salida.\n",
    "   - Establecer una tasa de aprendizaje (ej. 0.5) y el número de épocas de entrenamiento.\n",
    "\n",
    "   Por ejemplo, para la capa de salida (2 neuronas en la capa oculta, 1 neurona de salida):\n",
    " $$ W^{(2)} \\in \\mathbb{R}^{1 \\times 2} $$\n",
    " $$ b^{(2)} \\in \\mathbb{R}^{1 \\times 1} $$\n",
    "\n",
    "2. **Inicializar los pesos y los sesgos**:  \n",
    "   - Inicializar los pesos de las conexiones de la capa de entrada a la capa oculta y de la capa oculta a la capa de salida, de manera aleatoria (puedes usar la inicialización Xavier).\n",
    "   - También inicializar los sesgos de cada capa.\n",
    "\n",
    "3. **Propagación hacia adelante (Forward pass)**:  \n",
    "   - Para cada entrada, multiplicar las entradas por los pesos de la capa oculta y sumar el sesgo.\n",
    "   - Aplicar la función de activación (sigmoide) para obtener las activaciones de la capa oculta.\n",
    "   - Repetir el proceso con los valores de la capa oculta para calcular la activación de la capa de salida.\n",
    "\n",
    "4. **Calcular el error**:  \n",
    "   - Calcular el error en la salida utilizando una función de error, como el Error Cuadrático Medio (MSE).\n",
    "\n",
    "5. **Backpropagation (Propagación hacia atrás)**:  \n",
    "   - Calcular los gradientes de error en la capa de salida\n",
    "   - Propagar el error hacia la capa oculta, calculando el gradiente de error en la capa oculta.\n",
    "   \n",
    "6. **Actualizar los pesos y sesgos**:  \n",
    "   - Usar los gradientes obtenidos para ajustar los pesos y los sesgos de la capa de salida y de la capa oculta utilizando el gradiente descendente.\n",
    "   \n",
    "7. **Repetir el entrenamiento**:  \n",
    "   - Repetir los pasos de forward, cálculo de error y backpropagation por el número de épocas definido hasta que el error disminuya significativamente.\n",
    "\n",
    "8. **Evaluar el modelo**:  \n",
    "   - Después del entrenamiento, probar la red con las entradas XOR y verificar que las salidas estén cerca de los valores esperados (0 o 1).\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 0, Error: 1.2986343258576976\n",
      "\n",
      "Resultados finales:\n",
      "Entrada: [0 0], Salida estimada: [[0.08620875]], Salida real: [0]\n",
      "Entrada: [0 1], Salida estimada: [[0.91729343]], Salida real: [1]\n",
      "Entrada: [1 0], Salida estimada: [[0.91705169]], Salida real: [1]\n",
      "Entrada: [1 1], Salida estimada: [[0.09166514]], Salida real: [0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Función de activación sigmoide\n",
    "def sigmoid(x):\n",
    "    # TODO: Implementar la función sigmoide\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivada de la sigmoide: Se usa en la fase de backpropagation para ajustar los pesos correctamente.\n",
    "def sigmoid_derivative(x): #Le pasamos la activacion de la neurona x = sigma(z) \n",
    "    #Implementar la derivada de la función sigmoide considerar que el valor x es  sigma(x)\n",
    "    derivada = x * (1 - x) #\n",
    "\n",
    "    return derivada\n",
    "\n",
    "# Definimos los datos de entrada para XOR\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "# Salidas esperadas para XOR\n",
    "y = np.array([[0], \n",
    "              [1], \n",
    "              [1], \n",
    "              [0]])\n",
    "\n",
    "# Inicializamos los pesos y bias aleatoriamente: Al principio la red no sabe nada, así que empezamos con valores aleatorios.\n",
    "#Con el entrenamiento, esos valores se irán ajustando hasta que la red aprenda.\n",
    "# Se establece la semilla para la generación de números pseudoaleatorios\n",
    "np.random.seed(42)  # Para reproducibilidad de los experimentos\n",
    "\n",
    "W1 =np.random.rand(2,2)  # Pesos entre capa de entrada y capa oculta\n",
    "B1 = np.random.rand(1,2)  # Bias de la capa oculta\n",
    "W2 = np.random.rand(2,1)  # Pesos entre capa oculta y capa de salida\n",
    "B2 = np.random.rand(1,1)  # Bias de la capa de salida\n",
    "\n",
    "# Definimos la tasa de aprendizaje:\n",
    "#Si es muy alto → el modelo puede “saltar” y no aprender bien.\n",
    "#Si es muy bajo → aprende muy despacio.\n",
    "learning_rate = 0.5 # que tan rapido aprende la red\n",
    "\n",
    "# Número de iteraciones de entrenamiento\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs): # Bucle de entrenamiento: Ejecuta todo el proceso de aprendizaje 10,000 veces.\n",
    "\n",
    "    #----------------------------------------------\n",
    "    # 1. Propagación hacia adelante (Forward pass)\n",
    "    #----------------------------------------------\n",
    "    #Capa oculta: Calcula la suma ponderada Z (z_c1) para la capa oculta\n",
    "    z_c1 =  X@W1 + B1  # Suma ponderada de la capa oculta: Así calculamos la energía de activación que recibe cada neurona en la capa oculta.\n",
    "    #Activación de la capa oculta: Calcula la activación de la capa oculta usando la función sigmoide\n",
    "    a_c1  = sigmoid(z_c1)  # Aplica la función sigmoide a cada valor de z_c1: Convierte las sumas ponderadas en señales “suaves” entre 0 y 1.\n",
    "    #Capa de salida — cálculo de entrada ponderada: Calcular la suma ponderada Z (z_c2)  para la capa de salida \n",
    "    z_c2 = a_c1@W2 + B2 # Suma ponderada de la capa de salida: Este valor z_c2 será la entrada a la última neurona, la que da la predicción final.\n",
    "    #Activación de la capa de salida: Calcula la activación de la capa de salida usando la función sigmoide\n",
    "    y_pred = sigmoid(z_c2) # Activación de la capa de salida: Queremos una salida entre 0 y 1 (porque el XOR solo da 0 o 1).\n",
    "\n",
    "    #----------------------------------------------\n",
    "    # 2. Cálculo del error con MSE (qué tanto se equivocó)\n",
    "    #----------------------------------------------\n",
    "    #Calcular el error cuadrático medio (MSE) : Mide la diferencia entre lo que la red predijo (y_pred) y lo que debería haber dicho (y).\n",
    "    #Es necesario saber cuánto se equivocó para luego corregir los pesos en la siguiente fase (backpropagation)\n",
    "    error = (y - y_pred) ** 2 # error por cada muestra del batch de entrenamiento\n",
    "    mse = np.mean(error) # error medio del batch de entrenamiento\n",
    "    total_error = np.sum(error) # error total del batch de entrenamiento\n",
    "\n",
    "    #----------------------------------------------\n",
    "    # 3. Propagación hacia atrás (Backward pass)\n",
    "    #----------------------------------------------\n",
    "    #----------------------------------------------\n",
    "    # Gradiente de la salida\n",
    "    #----------------------------------------------\n",
    "    #Calcular la derivada del error con respecto a la salida y\n",
    "    dE_dy_pred  = -2 * (y - y_pred) # Derivada del error con respecto a la salida y_pred:Calcula cómo cambia el error si cambia la salida (y_pred). \n",
    "    #Es el primer paso para saber qué dirección tomar para reducir el error.\n",
    "\n",
    "    #Calcular la derivada de la activación de la salida con respecto a z_c2 \n",
    "    d_y_pred_d_zc2 = sigmoid_derivative(y_pred) # Derivada de la activación de la salida con respecto a z_c2: Calcula cómo cambia la salida respecto a la entrada (z_c2).\n",
    "    #Sirve para propagar el error hacia atrás a través de la función sigmoide.\n",
    "\n",
    "    #Calcular delta de la capa de salida\n",
    "    delta_c2 = dE_dy_pred * d_y_pred_d_zc2 # delta de la capa de salida : Combina ambos efectos: cómo el error depende de la salida y cómo la salida depende de la entrada.\n",
    "    #delta_c2 es el “error local” de la capa de salida. Este valor servirá para ajustar sus pesos (W2) y su bias (B2).\n",
    "    \n",
    "    \n",
    "    #----------------------------------------------\n",
    "    # Gradiente en la capa oculta: Aquí empieza la parte donde el error, que se calculó al final de la red, viaja hacia atrás para decirle a las neuronas anteriores cuánto contribuyeron a ese error.\n",
    "    #----------------------------------------------\n",
    "    #Propagar el error hacia la capa oculta. Esto calcula el error local de la capa oculta, llamado delta_c1:nos dice cuánto corregir cada neurona de la capa oculta.\n",
    "    #Recordemos que ya habíamos calculado delta_c2, que es el error en la capa de salida. Ahora queremos saber:¿Qué tanto fue culpa de la capa oculta que la salida se equivocara?\n",
    "    delta_c1 = delta_c2 @ W2.T * sigmoid_derivative(a_c1) # delta de la capa oculta\n",
    "\n",
    "    #----------------------------------------------\n",
    "    # 4. Actualización de los pesos y biases :Ahora viene la parte donde la red neuronal realmente “aprende”, es decir: donde ajusta sus pesos y sesgos (biases) para cometer menos error en la siguiente vuelta.\n",
    "    #Después de calcular los errores locales (delta_c2 y delta_c1), la red ya sabe:\n",
    "    #Qué tan equivocada estuvo cada neurona.\n",
    "    #Qué tanto debe cambiar cada conexión (peso) para mejorar.\n",
    "    #----------------------------------------------\n",
    "    #Actualizar los pesos y bias de la capa de salida\n",
    "    W2  = W2 - a_c1.T @ delta_c2 * learning_rate #Actualiza los pesos entre la capa oculta y la capa de salida: Multiplica las salidas de la capa oculta (a_c1) por el error local de la capa de salida (delta_c2). Este producto nos dice cuánto debe ajustarse cada conexión entre neurona oculta y de salida. Si learning_rate es 0.5, significa “ajusta a la mitad de la magnitud del error”.Restamos porque queremos disminuir el error. Si el gradiente apunta “hacia arriba en la montaña del error”, restarlo nos lleva “cuesta abajo”.\n",
    "    B2 =  B2 - np.sum(delta_c2, axis=0, keepdims=True) * learning_rate #Ajusta el bias (sesgo) de la capa de salida. Suma el error de todas las neuronas (o ejemplos) para tener una corrección global del bias. Restamos para movernos hacia la dirección que reduce el error.\n",
    "    \n",
    "    #Actualizar los pesos y bias de la capa oculta\n",
    "    W1 = W1 - X.T @ delta_c1 * learning_rate #Ajusta los pesos entre la capa de entrada y la capa oculta:→ Mide cuánto contribuyó cada entrada (de X) al error de cada neurona oculta. Multiplicamos por el learning rate. Restamos para disminuir el error.\n",
    "    B1 = B1 - np.sum(delta_c1, axis=0, keepdims=True) * learning_rate #Ajusta los biases de las neuronas ocultas\n",
    "    # Imprimir el error cada 1000 épocas\n",
    "    if epoch % 1000 == 0: \n",
    "        print(f\"Época {epoch}, Error: {total_error}\") # Imprimir el error cada 1000 épocas\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Comprobar los resultados del modelo entrenado\n",
    "# Recordar: al final del entrenamiento los parámetros de la red ya se ajustaron, es decir, las matrices Ws y Bias Bs se usan para predecir nuevos datos através de la red.\n",
    "print(\"\\nResultados finales:\")\n",
    "\n",
    "for i in range(len(X)):\n",
    "    #Realizar la propagación hacia adelante para cada entrada de prueba\n",
    "    x = X[i] # Ejemplo de entrada i \n",
    "    x = x[np.newaxis, :] # Añade una nueva dimensión al vector x, para convertirlo de un vector a una matriz 1xN.\n",
    "    z_hidden = x @ W1 + B1  # Suma ponderada de la capa oculta\n",
    "    a_hidden = sigmoid(z_hidden)  # Activación de la capa oculta\n",
    "    z_output = a_hidden @ W2 + B2  # Suma ponderada de la capa de salida: Aquí la red combina las señales procesadas por las neuronas ocultas para producir el valor final antes de aplicar la activación.\n",
    "    y_pred = sigmoid(z_output)  # Activación de la capa de salida\n",
    "    #y_pred =     # Predicción para el ejemplo i\n",
    "    # Mostrar las predicciones\n",
    "    print(f\"Entrada: {X[i]}, Salida estimada: {y_pred}, Salida real: {y[i]}\") # Mostrar las predicciones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
