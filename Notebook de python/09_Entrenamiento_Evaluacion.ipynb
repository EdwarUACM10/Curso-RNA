{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificaci√≥n de Textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/fig-diagrama-clasificador.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar al clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificador: Red Neuronal Multicapa\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/fig-MLP_XOR.png\" width=\"600\" style=\"background-color:white;\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de ejemplos de entrenamiento\n",
      "klass\n",
      "nonaggressive    3655\n",
      "aggressive       1477\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_json(\"./data_aggressiveness_es.json\", lines=True)\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de entrenamiento\")\n",
    "print(dataset.klass.value_counts()) # muestra el conteo de cada clase\n",
    "# Extracci√≥n de los textos en arreglos de numpy\n",
    "X = dataset['text'].to_numpy()\n",
    "# Extracci√≥n de las etiquetas o clases de entrenamiento\n",
    "Y = dataset['klass'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Codificar las categor√≠as (clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases:\n",
      "['aggressive' 'nonaggressive']\n",
      "Clases codificadas:\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# Normalizar las etiquetas a una codificaci√≥n ordinal para entrada del clasificador\n",
    "Y_encoded= le.fit_transform(Y)\n",
    "print(\"Clases:\")\n",
    "print(le.classes_)\n",
    "print(\"Clases codificadas:\")\n",
    "print(le.transform(le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparar los conjuntos de datos  (datasets) para entrenamiento y para probar el rendimiento del clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el conjunto de datos en conjunto de entrenamiento (80%) y conjunto de pruebas (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y_encoded, test_size=0.2, stratify=Y_encoded, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Crear Matriz Documento-T√©rmino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulario:  10609\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "_STOPWORDS = stopwords.words(\"spanish\")  # agregar m√°s palabras a esta lista si es necesario\n",
    "\n",
    "# Normalizaci√≥n del texto\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¬ø?¬°!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuaci√≥n, True deja intacta la puntuaci√≥n)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los n√∫meros, True deja intactos los acentos)\n",
    "        max_dup=2 (n√∫mero m√°ximo de s√≠mbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n",
    "\n",
    "# Preprocesamiento personalizado \n",
    "def mi_preprocesamiento(texto):\n",
    "    #convierte a min√∫sculas el texto antes de normalizar\n",
    "    tokens = word_tokenize(texto.lower())\n",
    "    texto = \" \".join(tokens)\n",
    "    texto = normaliza_texto(texto)\n",
    "    return texto\n",
    "    \n",
    "# Tokenizador personalizado \n",
    "def mi_tokenizador(texto):\n",
    "    # Elimina stopwords: palabras que no se consideran de contenido y que no agregan valor sem√°ntico al texto\n",
    "    #print(\"antes: \", texto)\n",
    "    texto = [t for t in texto.split() if t not in _STOPWORDS]\n",
    "    #print(\"despu√©s:\",texto)\n",
    "    return texto\n",
    "\n",
    "\n",
    "vec_tfidf = TfidfVectorizer(analyzer=\"word\", preprocessor=mi_preprocesamiento, tokenizer=mi_tokenizador,  ngram_range=(1,1))\n",
    "X_tfidf = vec_tfidf.fit_transform(X_train)\n",
    "print(\"vocabulario: \", len(vec_tfidf.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Crear el clasificador: Crear la clase MLP_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# Funci√≥n de activaci√≥n sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivada de la sigmoide\n",
    "def sigmoid_derivative(x):\n",
    "    # return sigmoid(x) * (1 - sigmoid(x))\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Establece la semilla para la generaci√≥n de n√∫meros aleatorios\n",
    "def seed(random_state=33):\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "def xavier_initialization(input_size, output_size):\n",
    "    # Calcular el l√≠mite de la distribuci√≥n uniforme\n",
    "    limit = np.sqrt(6 / (input_size + output_size))\n",
    "    # Generar la matriz de pesos con distribuci√≥n uniforme en el rango [-limit, limit]\n",
    "    W = np.random.uniform(-limit, limit, (input_size, output_size))\n",
    "    return W\n",
    "\n",
    "\n",
    "def create_minibatches(X, y, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.permutation(n_samples)  # Mezcla los √≠ndices aleatoriamente\n",
    "    X_shuffled, y_shuffled = X[indices], y[indices]  # Reordena X e y seg√∫n los √≠ndices aleatorios\n",
    "    \n",
    "    # Divide los datos en minibatches\n",
    "    for X_batch, y_batch in zip(np.array_split(X_shuffled, np.ceil(n_samples / batch_size)), \n",
    "                                np.array_split(y_shuffled, np.ceil(n_samples / batch_size))):\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "    \n",
    "class MLP_TODO:\n",
    "    def __init__(self, num_entradas, num_neuronas_ocultas, num_salidas, epochs, batch_size=32, learning_rate=0.5, random_state=42):\n",
    "\n",
    "        seed(random_state)\n",
    "        # Definir la tasa de aprendizaje\n",
    "        self.learning_rate = learning_rate\n",
    "        # Definir el n√∫mero de √©pocas\n",
    "        self.epochs = epochs\n",
    "        # Definir el tama√±o del batch de procesamiento\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # definir las capas\n",
    "        self.W1 = xavier_initialization(num_neuronas_ocultas, num_entradas)  # Pesos entre capa de entrada y capa oculta\n",
    "        self.b1 = np.zeros((1, num_neuronas_ocultas))   # Bias de la capa oculta\n",
    "        self.W2 = xavier_initialization(num_salidas, num_neuronas_ocultas)  # Pesos entre capa oculta y capa de salida\n",
    "        self.b2 = np.zeros((1, num_salidas)) # Bias de la capa de salida\n",
    "\n",
    "    def forward(self, X):\n",
    "        # TODO: implementar el forward pass\n",
    "        #----------------------------------------------\n",
    "        # 1. Propagaci√≥n hacia adelante (Forward pass)\n",
    "        #----------------------------------------------\n",
    "        # TODO: Calcular la suma ponderada Z (z_c1) para la capa oculta \n",
    "        self.X = X\n",
    "        self.z_c1 = 0\n",
    "        # TODO: Calcular la activaci√≥n de la capa oculta usando la funci√≥n sigmoide\n",
    "        self.a_c1 = 0\n",
    "        # TODO: Calcular la suma ponderada Z (z_c2)  para la capa de salida \n",
    "        self.z_c2 = 0\n",
    "        # TODO: Calcular la activaci√≥n de la capa de salida usando la funci√≥n sigmoide\n",
    "        y_pred = 0  # Activaci√≥n capa de salida\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "    def loss_function_MSE(self, y_pred, y):\n",
    "        #----------------------------------------------\n",
    "        # 2. C√°lculo del error con MSE\n",
    "        #----------------------------------------------\n",
    "        # TODO: Calcular el error cuadr√°tico medio (MSE)\n",
    "        self.y_pred = y_pred\n",
    "        self.y = y\n",
    "        error = 0.5 * np.mean((y_pred - y) ** 2)\n",
    "        return error\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        # TODO: implementar el backward pass\n",
    "        # calcular los gradientes para la arquitectura de la figura anterior\n",
    "        #----------------------------------------------\n",
    "        # 3. Propagaci√≥n hacia atr√°s (Backward pass)\n",
    "        #----------------------------------------------\n",
    "        \n",
    "        #----------------------------------------------\n",
    "        # Gradiente de la salida\n",
    "        #----------------------------------------------\n",
    "        # TODO: Calcular la derivada del error con respecto a la salida y\n",
    "        dE_dy_pred = 0 # Derivada del error respecto a la predicci√≥n con  N ejemplos\n",
    "        # TODO: Calcular la derivada de la activaci√≥n de la salida con respecto a z_c2 \n",
    "        d_y_pred_d_zc2 = 0\n",
    "        # TODO: Calcular delta de la capa de salida\n",
    "        delta_c2 = 0\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Gradiente en la capa oculta\n",
    "        #----------------------------------------------\n",
    "        # calcular la derivada de las suma ponderada respecto a las activaciones de la capa 1\n",
    "        d_zc2_d_a_c1 = 0\n",
    "        # TODO: Propagar el error hacia la capa oculta, calcular deltas de la capa 1\n",
    "        delta_c1 = 0\n",
    "\n",
    "        #calcula el gradiente de la funci√≥n de error respecto a los pesos de la capa 2\n",
    "        self.dE_dW2 = 0\n",
    "        self.dE_db2 =0\n",
    "        self.dE_dW1 =  0\n",
    "        self.dE_db1 =  0\n",
    "\n",
    "\n",
    "    def update(self):  # Ejecuci√≥n de la actualizaci√≥n de param√°metros\n",
    "        # TODO: implementar la actualizaci√≥n de los pesos y el bias\n",
    "        #----------------------------------------------\n",
    "        # Actualizaci√≥n de pesos de la capa de salida\n",
    "        #---------------------------------------------- \n",
    "        \n",
    "        # TODO: Actualizar los pesos y bias de la capa de salida\n",
    "        self.W2 = 0\n",
    "        self.b2 = 0\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Actuailzaci√≥n de pesos de la capa oculta\n",
    "        #----------------------------------------------\n",
    "        #calcula el gradiente de la funci√≥n de error respecto a los pesos de la capa 1\n",
    "        self.W1 = 0\n",
    "        self.b1 = 0\n",
    "\n",
    "    def predict(self, X):  # Predecir la categor√≠a para datos nuevos\n",
    "        # TODO: implementar la predicci√≥n \n",
    "        y_pred = self.forward(X)\n",
    "        # Obtener la clase para el clasificador binario\n",
    "        y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "        return y_pred\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        #implementar el entrenamiento de la red\n",
    "        for epoch in range(self.epochs):\n",
    "            for X_batch, y_batch in create_minibatches(X, Y, self.batch_size):\n",
    "                y_pred = self.forward(X_batch)\n",
    "                error = self.loss_function_MSE(y_pred, y_batch)\n",
    "                self.backward() # c√°lculo de los gradientes\n",
    "                self.update() # actualizaci√≥n de los pesos y bias\n",
    "\n",
    "                # Imprimir el error cada N √©pocas\n",
    "                if epoch % 100 == 0:\n",
    "                    print(f\"√âpoca {epoch}, Error: {error}\")\n",
    "                    \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLASE MLP_TODO.PY Y FUNCIONES AUXILIARES\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "# ====================================================\n",
    "# Funciones para activaci√≥n y  su derivada\n",
    "# ====================================================\n",
    "def cargar_dataset(ruta):\n",
    "    # Cargar el archivo CSV con pandas\n",
    "    datos = pd.read_csv(ruta)\n",
    "    # Separar las caracter√≠sticas (todas menos la √∫ltima columna)\n",
    "    X = datos.iloc[:, :-1].values  \n",
    "    # Separar la columna objetivo (√∫ltima columna)\n",
    "    y = datos.iloc[:, -1].values  \n",
    "    # Asegurar que y sea un vector columna (n x 1)\n",
    "    y = y.reshape(-1, 1)\n",
    "    # Mostrar informaci√≥n del dataset cargado\n",
    "    print(f\"Conjunto de datos cargado desde: {ruta}\")\n",
    "    print(f\"   ‚Üí Ejemplos: {X.shape[0]}, Caracter√≠sticas: {X.shape[1]}\")\n",
    "    print(f\"   ‚Üí Clases √∫nicas en y: {np.unique(y).ravel()}\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Funci√≥n de activaci√≥n sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivada de la sigmoide\n",
    "def sigmoid_derivative(x):\n",
    "    # return sigmoid(x) * (1 - sigmoid(x))\n",
    "    return x * (1 - x)\n",
    "\n",
    "# ====================================================\n",
    "# Funciones para manejo de la semilla\n",
    "# ====================================================\n",
    "\n",
    "# Establece la semilla para la generaci√≥n de n√∫meros aleatorios\n",
    "def seed(random_state=33):\n",
    "    np.random.seed(random_state)\n",
    "    random.seed(random_state)\n",
    "\n",
    "# ====================================================\n",
    "# Funciones para inicializaci√≥n y normalizaci√≥n\n",
    "# ====================================================\n",
    "\n",
    "# Inicializaci√≥n Xavier\n",
    "def xavier_initialization(input_size, output_size): \n",
    "    return np.random.randn(input_size, output_size) * np.sqrt(1 / input_size)\n",
    "# Inicializaci√≥n normal\n",
    "def normal_initialization(input_size, output_size):\n",
    "    return np.random.randn(input_size, output_size)\n",
    "# Normalizaci√≥n Z-score\n",
    "def zscore_normalization(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X_norm = (X - mean) / std\n",
    "    return X_norm\n",
    "\n",
    "#Funci√≥n para crear minibatches\n",
    "def create_minibatches(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    Genera los lotes de datos (batchs) de acuerdo al par√°metro batch_size de forma aleatoria para el procesamiento. \n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.permutation(n_samples)  # Mezcla los √≠ndices aleatoriamente\n",
    "    X_shuffled, y_shuffled = X[indices], y[indices]  # Reordena X e y seg√∫n los √≠ndices aleatorios\n",
    "    \n",
    "    # Divide los datos en minibatches\n",
    "    for X_batch, y_batch in zip(np.array_split(X_shuffled, np.ceil(n_samples / batch_size)), \n",
    "                                np.array_split(y_shuffled, np.ceil(n_samples / batch_size))):\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "    \n",
    "class MLP_TODO:\n",
    "    def __init__(self, num_entradas, num_neuronas_ocultas, num_salidas, epochs, batch_size=128, learning_rate=0.2, normalizacion=\"none\",inicializacion=\"xavier\",random_state=42):\n",
    "\n",
    "        # ====================================================\n",
    "        # Inicializaci√≥n general del modelo\n",
    "        # ====================================================\n",
    "\n",
    "        # üîπ NUEVO: Usar el par√°metro random_state recibido para controlar la semilla\n",
    "        seed(33)\n",
    "        self.random_state = random_state  # üîπ NUEVO: guardar la semilla para usarla tambi√©n en create_minibatches\n",
    "\n",
    "        # Definir la tasa de aprendizaje\n",
    "        self.learning_rate = learning_rate\n",
    "        # Definir el n√∫mero de √©pocas\n",
    "        self.epochs = epochs\n",
    "        # Definir el tama√±o del batch de procesamiento\n",
    "        self.batch_size = batch_size\n",
    "        # Definir el tipo de normalizaci√≥n\n",
    "        self.normalizacion = normalizacion\n",
    "        # Definir el tipo de inicializaci√≥n\n",
    "        self.inicializacion = inicializacion\n",
    "        # definir las \n",
    "        self.num_neuronas_ocultas = num_neuronas_ocultas\n",
    "\n",
    "        # Inicializaci√≥n de pesos y bias\n",
    "        self.W1 = self.inicializar_pesos(num_entradas, self.num_neuronas_ocultas) # Pesos entre capa de entrada y capa oculta\n",
    "        self.b1 = np.zeros((1, self.num_neuronas_ocultas))   # Bias de la capa oculta\n",
    "        self.W2 = self.inicializar_pesos(self.num_neuronas_ocultas,num_salidas)  # Pesos entre capa oculta y capa de salida\n",
    "        self.b2 = np.zeros((1, num_salidas)) # Bias de la capa de salida\n",
    "\n",
    "        # Historial de errores\n",
    "        self.errores_history = []\n",
    "        # Historial de accuracy\n",
    "        self.accuracy_history = []\n",
    "\n",
    "    # ====================================================\n",
    "    # Funciones para forward, backward, update, predict y train\n",
    "    # ====================================================\n",
    "\n",
    "    def forward(self, X):\n",
    "        #implementar el forward pass\n",
    "        #----------------------------------------------\n",
    "        # 1. Propagaci√≥n hacia adelante (Forward pass)\n",
    "        #----------------------------------------------\n",
    "        # Calcular la suma ponderada Z (z_c1) para la capa oculta \n",
    "        self.X = X\n",
    "        self.z_c1 = X@self.W1 + self.b1\n",
    "        #Calcular la activaci√≥n de la capa oculta usando la funci√≥n sigmoide\n",
    "        self.a_c1 = sigmoid(self.z_c1)  # Activaci√≥n capa oculta\n",
    "        #Calcular la suma ponderada Z (z_c2)  para la capa de salida \n",
    "        self.z_c2  = self.a_c1 @ self.W2 + self.b2\n",
    "        #Calcular la activaci√≥n de la capa de salida usando la funci√≥n sigmoide\n",
    "        y_pred = sigmoid(self.z_c2)  # Activaci√≥n capa salida\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "    def loss_function_MSE(self, y_pred, y):\n",
    "        #----------------------------------------------\n",
    "        # 2. C√°lculo del error con MSE\n",
    "        #----------------------------------------------\n",
    "        #Calcular el error cuadr√°tico medio (MSE)\n",
    "        self.y_pred = y_pred\n",
    "        self.y = y\n",
    "        error = 0.5 * np.mean((y_pred - y) ** 2)\n",
    "        return error\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        #implementar el backward pass\n",
    "        # calcular los gradientes para la arquitectura de la figura anterior\n",
    "        #----------------------------------------------\n",
    "        # 3. Propagaci√≥n hacia atr√°s (Backward pass)\n",
    "        #----------------------------------------------\n",
    "        \n",
    "        #----------------------------------------------\n",
    "        # Gradiente de la salida\n",
    "        #----------------------------------------------\n",
    "        #Calcular la derivada del error con respecto a la salida y\n",
    "        dE_dy_pred = (self.y_pred - self.y)  # Derivada del error respecto a la predicci√≥n con  N ejemplos\n",
    "        #Calcular la derivada de la activaci√≥n de la salida con respecto a z_c2 \n",
    "        d_y_pred_d_zc2 = sigmoid_derivative(self.y_pred)\n",
    "        #Calcular delta de la capa de salida\n",
    "        delta_c2 = dE_dy_pred * d_y_pred_d_zc2  # (N, 1)\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Gradiente en la capa oculta\n",
    "        #----------------------------------------------\n",
    "        # calcular la derivada de las suma ponderada respecto a las activaciones de la capa 1\n",
    "        d_zc2_d_a_c1 = self.W2  \n",
    "        #Propagar el error hacia la capa oculta, calcular deltas de la capa 1\n",
    "        delta_c1 = delta_c2 @ d_zc2_d_a_c1.T * sigmoid_derivative(self.a_c1)  \n",
    "\n",
    "        #calcula el gradiente de la funci√≥n de error respecto a los pesos de la capa 2\n",
    "        self.dE_dW2 = self.a_c1.T @ delta_c2\n",
    "        self.dE_db2 = np.sum(delta_c2, axis=0, keepdims=True)\n",
    "        self.dE_dW1 = self.X.T @ delta_c1 \n",
    "        self.dE_db1 = np.sum(delta_c1, axis=0, keepdims=True) \n",
    "\n",
    "\n",
    "    def update(self):  # Ejecuci√≥n de la actualizaci√≥n de param√°metros\n",
    "        #implementar la actualizaci√≥n de los pesos y el bias\n",
    "        #----------------------------------------------\n",
    "        # Actualizaci√≥n de pesos de la capa de salida\n",
    "        #---------------------------------------------- \n",
    "        #Actualizar los pesos y bias de la capa de salida\n",
    "        self.W2 = self.W2 - self.dE_dW2 * self.learning_rate\n",
    "        self.b2 = self.b2 - self.dE_db2 * self.learning_rate\n",
    "        #----------------------------------------------\n",
    "        # Actuailzaci√≥n de pesos de la capa oculta\n",
    "        #----------------------------------------------\n",
    "        #calcula el gradiente de la funci√≥n de error respecto a los pesos de la capa 1\n",
    "        self.W1 = self.W1 - self.dE_dW1 * self.learning_rate\n",
    "        self.b1 = self.b1 - self.dE_db1 * self.learning_rate\n",
    "\n",
    "    def predict(self, X):  # Predecir la categor√≠a para datos nuevos\n",
    "        # TODO: implementar la predicci√≥n \n",
    "        y_pred = self.forward(X)\n",
    "        # Obtener la clase para el clasificador binario\n",
    "        y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "        return y_pred\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        #implementar el entrenamiento de la red\n",
    "            # üîπ Normalizar los datos seg√∫n el tipo configurado\n",
    "        X = self.normalize(X)\n",
    "        for epoch in range(self.epochs):\n",
    "            num_batch = 0\n",
    "            epoch_error  = 0\n",
    "            for X_batch, y_batch in create_minibatches(X, Y, self.batch_size):\n",
    "                y_pred = self.forward(X_batch)\n",
    "                error = self.loss_function_MSE(y_pred, y_batch)\n",
    "                epoch_error += error    \n",
    "                self.backward() # c√°lculo de los gradientes\n",
    "                self.update() # actualizaci√≥n de los pesos y bias\n",
    "                num_batch += 1\n",
    "                # Imprimir el error cada N √©pocas\n",
    "                if epoch % 100 == 0:\n",
    "                    print(f\"√âpoca {epoch}, Error batch {num_batch}: {error}\")\n",
    "            # Guardar el error promedio de la √©poca\n",
    "            self.errores_history.append(epoch_error/num_batch)\n",
    "            #Calcular Accuracy en todo el dataset\n",
    "            acc_epoch = self.accuracy(X, Y)\n",
    "            self.accuracy_history.append(acc_epoch)\n",
    "            # Imprimir el error y accuracy cada N √©pocas\n",
    "            if epoch % 100 == 0:\n",
    "                    print(f\"√âpoca {epoch}, Error: {epoch_error/num_batch}%\")\n",
    "\n",
    "    def evaluar(self, X, y):\n",
    "        \"\"\"\n",
    "        Eval√∫a el desempe√±o del modelo en un conjunto de prueba.\n",
    "        Muestra todas las predicciones junto con las salidas esperadas en columnas paralelas.\n",
    "        Calcula la precisi√≥n total.\n",
    "        \"\"\"\n",
    "        # Normalizar los datos de entrada con la misma t√©cnica usada en entrenamiento\n",
    "        X = self.normalize(X)\n",
    "\n",
    "        # Obtener las predicciones\n",
    "        y_pred = self.predict(X)\n",
    "\n",
    "        # Combinar esperadas y predicciones en columnas paralelas\n",
    "        resultados = np.column_stack((y, y_pred))\n",
    "        \n",
    "        # Mostrar resultados\n",
    "        print(\"\\nüîç Evaluaci√≥n del modelo (esperada | predicha):\")\n",
    "        for idx, (esperada, predicha) in enumerate(resultados):\n",
    "            print(f\"{idx+1:02d}: {esperada} | {predicha}\")\n",
    "\n",
    "        # Calcular precisi√≥n global\n",
    "        accuracy = np.mean(y_pred == y)\n",
    "        print(f\"\\n‚úÖ Precisi√≥n del modelo: {accuracy * 100:.2f}%\")\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    \n",
    "    # ====================================================\n",
    "    # Funciones para inicializaci√≥n, normalizaci√≥n y accuracy\n",
    "    # ====================================================\n",
    "    # Normalizaci√≥n de los datos\n",
    "    def normalize(self, X):\n",
    "        if self.normalizacion == \"z-score\":\n",
    "            return zscore_normalization(X)  # üîπ Llamada a la funci√≥n existente\n",
    "        else:  # sin normalizar\n",
    "            return X\n",
    "        \n",
    "    # Inicializaci√≥n de los pesos  \n",
    "    def inicializar_pesos(self, tama√±o_entrada, tama√±o_salida):\n",
    "        if self.inicializacion == \"xavier\":\n",
    "            return xavier_initialization(tama√±o_entrada, tama√±o_salida)\n",
    "        elif self.inicializacion == \"normal\":\n",
    "            return normal_initialization(tama√±o_entrada, tama√±o_salida)\n",
    "        else:\n",
    "            raise ValueError(\"Tipo de inicializaci√≥n no soportado\")\n",
    "    # C√°lculo de accuracy    \n",
    "    def accuracy(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        acc = np.mean(y_pred == y)  # compara predicciones con valores reales\n",
    "        return acc\n",
    "    \n",
    "    # ====================================================\n",
    "    # Funciones para graficar Error, Acurery y ambas\n",
    "    # ====================================================\n",
    "    \n",
    "    # Gr√°fica del error despues del entrenamiento\n",
    "    def plot_error(self):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(self.errores_history, label=\"Error MSE\", linewidth=2)\n",
    "        plt.xlabel(\"√âpocas\")\n",
    "        plt.ylabel(\"Error cuadr√°tico medio (MSE)\")\n",
    "        #T√≠tulo din√°mico con configuraci√≥n\n",
    "        plt.title(f\"Entrenamiento MLP - Capacas ocultas: {self.num_neuronas_ocultas}, Inicializacion: {self.inicializacion}, Normalizaci√≥n: {self.normalizacion}, \"\n",
    "                f\"LR: {self.learning_rate}, Batch_size: {self.batch_size},Funcion de activacion: Sigmoid, √âpocas: {self.epochs}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Entrenar la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√âpoca 0, Error batch 1: 0.23776988783038916\n",
      "√âpoca 0, Error batch 2: 0.21341728095102683\n",
      "√âpoca 0, Error batch 3: 0.1977315946282544\n",
      "√âpoca 0, Error batch 4: 0.1291934741311775\n",
      "√âpoca 0, Error batch 5: 0.05944379154596904\n",
      "√âpoca 0, Error batch 6: 0.07933819233199192\n",
      "√âpoca 0, Error batch 7: 0.05451363614486498\n",
      "√âpoca 0, Error batch 8: 0.08169615137739744\n",
      "√âpoca 0, Error batch 9: 0.045407231313687374\n",
      "√âpoca 0, Error batch 10: 0.13656207816313629\n",
      "√âpoca 0, Error batch 11: 0.0939662963205969\n",
      "√âpoca 0, Error batch 12: 0.17928227935526111\n",
      "√âpoca 0, Error batch 13: 0.15106093321546069\n",
      "√âpoca 0, Error batch 14: 0.18241541741728723\n",
      "√âpoca 0, Error batch 15: 0.09815009310415439\n",
      "√âpoca 0, Error batch 16: 0.12684743883813118\n",
      "√âpoca 0, Error batch 17: 0.08733031624956522\n",
      "√âpoca 0, Error batch 18: 0.2049092694104731\n",
      "√âpoca 0, Error batch 19: 0.14322449611068777\n",
      "√âpoca 0, Error batch 20: 0.0938735900890027\n",
      "√âpoca 0, Error batch 21: 0.13748096176211527\n",
      "√âpoca 0, Error batch 22: 0.10979092860250222\n",
      "√âpoca 0, Error batch 23: 0.11806564781853404\n",
      "√âpoca 0, Error batch 24: 0.22219951639974633\n",
      "√âpoca 0, Error batch 25: 0.12473692994719981\n",
      "√âpoca 0, Error batch 26: 0.22330981813804412\n",
      "√âpoca 0, Error batch 27: 0.16163222022326346\n",
      "√âpoca 0, Error batch 28: 0.1820576314563881\n",
      "√âpoca 0, Error batch 29: 0.18374254841038754\n",
      "√âpoca 0, Error batch 30: 0.1564138119729554\n",
      "√âpoca 0, Error batch 31: 0.09327147615151057\n",
      "√âpoca 0, Error batch 32: 0.15055099664614896\n",
      "√âpoca 0, Error batch 33: 0.08256454555491229\n",
      "√âpoca 0, Error batch 34: 0.15884713680580365\n",
      "√âpoca 0, Error batch 35: 0.12171042024779861\n",
      "√âpoca 0, Error batch 36: 0.18828986880794835\n",
      "√âpoca 0, Error batch 37: 0.06318481035051993\n",
      "√âpoca 0, Error batch 38: 0.09391997795010061\n",
      "√âpoca 0, Error batch 39: 0.20504180689776072\n",
      "√âpoca 0, Error batch 40: 0.16368428122509332\n",
      "√âpoca 0, Error batch 41: 0.19745142144904265\n",
      "√âpoca 0, Error batch 42: 0.15646370306931237\n",
      "√âpoca 0, Error batch 43: 0.11332782315342112\n",
      "√âpoca 0, Error batch 44: 0.15034695259482786\n",
      "√âpoca 0, Error batch 45: 0.1517072310800024\n",
      "√âpoca 0, Error batch 46: 0.24746490141686814\n",
      "√âpoca 0, Error batch 47: 0.08358359075661298\n",
      "√âpoca 0, Error batch 48: 0.12480067819244076\n",
      "√âpoca 0, Error batch 49: 0.08140157122304123\n",
      "√âpoca 0, Error batch 50: 0.14730356852937568\n",
      "√âpoca 0, Error batch 51: 0.17423936029068238\n",
      "√âpoca 0, Error batch 52: 0.1429001853716984\n",
      "√âpoca 0, Error batch 53: 0.09638625467333597\n",
      "√âpoca 0, Error batch 54: 0.15083882508446533\n",
      "√âpoca 0, Error batch 55: 0.06707410283225168\n",
      "√âpoca 0, Error batch 56: 0.09270291873051106\n",
      "√âpoca 0, Error batch 57: 0.09457099134911143\n",
      "√âpoca 0, Error batch 58: 0.15279029302471253\n",
      "√âpoca 0, Error batch 59: 0.15270835690671658\n",
      "√âpoca 0, Error batch 60: 0.2589845378299029\n",
      "√âpoca 0, Error batch 61: 0.02119854323696518\n",
      "√âpoca 0, Error batch 62: 0.08589797059455606\n",
      "√âpoca 0, Error batch 63: 0.1826866092144213\n",
      "√âpoca 0, Error batch 64: 0.16797269289137137\n",
      "√âpoca 0, Error batch 65: 0.03366604949769307\n",
      "√âpoca 0, Error batch 66: 0.24965546568042657\n",
      "√âpoca 0, Error batch 67: 0.24412393618530445\n",
      "√âpoca 0, Error batch 68: 0.24605631559089108\n",
      "√âpoca 0, Error batch 69: 0.09365125363724722\n",
      "√âpoca 0, Error batch 70: 0.15519725740208012\n",
      "√âpoca 0, Error batch 71: 0.09217015490550132\n",
      "√âpoca 0, Error batch 72: 0.18123925421867934\n",
      "√âpoca 0, Error batch 73: 0.14637237302194256\n",
      "√âpoca 0, Error batch 74: 0.23819792379957197\n",
      "√âpoca 0, Error batch 75: 0.15967972439563502\n",
      "√âpoca 0, Error batch 76: 0.1813115725429072\n",
      "√âpoca 0, Error batch 77: 0.07030532217715901\n",
      "√âpoca 0, Error batch 78: 0.13857315472413023\n",
      "√âpoca 0, Error batch 79: 0.11917703842925284\n",
      "√âpoca 0, Error batch 80: 0.05201359986600896\n",
      "√âpoca 0, Error batch 81: 0.14620960001386846\n",
      "√âpoca 0, Error batch 82: 0.12065271974631837\n",
      "√âpoca 0, Error batch 83: 0.1248271406221039\n",
      "√âpoca 0, Error batch 84: 0.15952360740604837\n",
      "√âpoca 0, Error batch 85: 0.15227156839055633\n",
      "√âpoca 0, Error batch 86: 0.24448953536769344\n",
      "√âpoca 0, Error batch 87: 0.2352270465996971\n",
      "√âpoca 0, Error batch 88: 0.23498835333175208\n",
      "√âpoca 0, Error batch 89: 0.09285423682590609\n",
      "√âpoca 0, Error batch 90: 0.18598695388377695\n",
      "√âpoca 0, Error batch 91: 0.15438586941553484\n",
      "√âpoca 0, Error batch 92: 0.1245618161810189\n",
      "√âpoca 0, Error batch 93: 0.09330147025533858\n",
      "√âpoca 0, Error batch 94: 0.15617720653321832\n",
      "√âpoca 0, Error batch 95: 0.08424002351510138\n",
      "√âpoca 0, Error batch 96: 0.18111263362452049\n",
      "√âpoca 0, Error batch 97: 0.08239558331267927\n",
      "√âpoca 0, Error batch 98: 0.2569205357086658\n",
      "√âpoca 0, Error batch 99: 0.13996660900336505\n",
      "√âpoca 0, Error batch 100: 0.1308378618408678\n",
      "√âpoca 0, Error batch 101: 0.0686867652906166\n",
      "√âpoca 0, Error batch 102: 0.2937629627505302\n",
      "√âpoca 0, Error batch 103: 0.13527427767229677\n",
      "√âpoca 0, Error batch 104: 0.14748216457318547\n",
      "√âpoca 0, Error batch 105: 0.14936303834884718\n",
      "√âpoca 0, Error batch 106: 0.1297258906799845\n",
      "√âpoca 0, Error batch 107: 0.1622341184194357\n",
      "√âpoca 0, Error batch 108: 0.2515607099421371\n",
      "√âpoca 0, Error batch 109: 0.30317568418162383\n",
      "√âpoca 0, Error batch 110: 0.13468324181682031\n",
      "√âpoca 0, Error batch 111: 0.14951748886867466\n",
      "√âpoca 0, Error batch 112: 0.13912030892362545\n",
      "√âpoca 0, Error batch 113: 0.049224151213562405\n",
      "√âpoca 0, Error batch 114: 0.09157033362014222\n",
      "√âpoca 0, Error batch 115: 0.09275512626027684\n",
      "√âpoca 0, Error batch 116: 0.13146116474289327\n",
      "√âpoca 0, Error batch 117: 0.11236330293062408\n",
      "√âpoca 0, Error batch 118: 0.17970832471284692\n",
      "√âpoca 0, Error batch 119: 0.2467932043696517\n",
      "√âpoca 0, Error batch 120: 0.16766774964615566\n",
      "√âpoca 0, Error batch 121: 0.12171213583198225\n",
      "√âpoca 0, Error batch 122: 0.16621921334075385\n",
      "√âpoca 0, Error batch 123: 0.05125281465199852\n",
      "√âpoca 0, Error batch 124: 0.20007574514534027\n",
      "√âpoca 0, Error batch 125: 0.056797104560467014\n",
      "√âpoca 0, Error batch 126: 0.16917967812766252\n",
      "√âpoca 0, Error batch 127: 0.10686688790050829\n",
      "√âpoca 0, Error batch 128: 0.0937215855442595\n",
      "√âpoca 0, Error batch 129: 0.1866793744152548\n",
      "√âpoca 0, Error batch 130: 0.20955794781345488\n",
      "√âpoca 0, Error batch 131: 0.15484249208584525\n",
      "√âpoca 0, Error batch 132: 0.22307318550754157\n",
      "√âpoca 0, Error batch 133: 0.09082370631275724\n",
      "√âpoca 0, Error batch 134: 0.12573017736434605\n",
      "√âpoca 0, Error batch 135: 0.06280518073050194\n",
      "√âpoca 0, Error batch 136: 0.09520982998910218\n",
      "√âpoca 0, Error batch 137: 0.10823932065895797\n",
      "√âpoca 0, Error batch 138: 0.146637888967798\n",
      "√âpoca 0, Error batch 139: 0.11220060968011958\n",
      "√âpoca 0, Error batch 140: 0.21136322992279094\n",
      "√âpoca 0, Error batch 141: 0.12094963525975319\n",
      "√âpoca 0, Error batch 142: 0.09517749338323657\n",
      "√âpoca 0, Error batch 143: 0.1539757513173466\n",
      "√âpoca 0, Error batch 144: 0.0621363930964222\n",
      "√âpoca 0, Error batch 145: 0.16945883166013537\n",
      "√âpoca 0, Error batch 146: 0.09757924587151864\n",
      "√âpoca 0, Error batch 147: 0.12262163020460617\n",
      "√âpoca 0, Error batch 148: 0.09309782476994236\n",
      "√âpoca 0, Error batch 149: 0.1970651358618302\n",
      "√âpoca 0, Error batch 150: 0.16426457413122064\n",
      "√âpoca 0, Error batch 151: 0.08172051903579647\n",
      "√âpoca 0, Error batch 152: 0.16838045472913532\n",
      "√âpoca 0, Error batch 153: 0.08407790791761327\n",
      "√âpoca 0, Error batch 154: 0.09954585558438343\n",
      "√âpoca 0, Error batch 155: 0.21485118446851237\n",
      "√âpoca 0, Error batch 156: 0.1276061246058617\n",
      "√âpoca 0, Error batch 157: 0.11295923592726423\n",
      "√âpoca 0, Error batch 158: 0.07880166767027949\n",
      "√âpoca 0, Error batch 159: 0.13244682559509502\n",
      "√âpoca 0, Error batch 160: 0.0014199884122280257\n",
      "√âpoca 0, Error batch 161: 0.16139748470731008\n",
      "√âpoca 0, Error batch 162: 0.12365861666092665\n",
      "√âpoca 0, Error batch 163: 0.18652578198026204\n",
      "√âpoca 0, Error batch 164: 0.12164364030704086\n",
      "√âpoca 0, Error batch 165: 0.15707695408171413\n",
      "√âpoca 0, Error batch 166: 0.09373338790352989\n",
      "√âpoca 0, Error batch 167: 0.11710330298118707\n",
      "√âpoca 0, Error batch 168: 0.136150481639618\n",
      "√âpoca 0, Error batch 169: 0.10723957042854124\n",
      "√âpoca 0, Error batch 170: 0.09396546341672583\n",
      "√âpoca 0, Error batch 171: 0.12470834624169785\n",
      "√âpoca 0, Error batch 172: 0.0948203339319513\n",
      "√âpoca 0, Error batch 173: 0.060370215211376345\n",
      "√âpoca 0, Error batch 174: 0.15733054710527067\n",
      "√âpoca 0, Error batch 175: 0.15376513394234506\n",
      "√âpoca 0, Error batch 176: 0.09357880213537142\n",
      "√âpoca 0, Error batch 177: 0.10782155092297277\n",
      "√âpoca 0, Error batch 178: 0.1241814796669953\n",
      "√âpoca 0, Error batch 179: 0.18334121739826112\n",
      "√âpoca 0, Error batch 180: 0.2450838423148735\n",
      "√âpoca 0, Error batch 181: 0.154261084977241\n",
      "√âpoca 0, Error batch 182: 0.12722712547239592\n",
      "√âpoca 0, Error batch 183: 0.12474142205754203\n",
      "√âpoca 0, Error batch 184: 0.09407213185657153\n",
      "√âpoca 0, Error batch 185: 0.21246711689300263\n",
      "√âpoca 0, Error batch 186: 0.20186124174509776\n",
      "√âpoca 0, Error batch 187: 0.034282284777331584\n",
      "√âpoca 0, Error batch 188: 0.2137810593737782\n",
      "√âpoca 0, Error batch 189: 0.12058876004479364\n",
      "√âpoca 0, Error batch 190: 0.15478633568928302\n",
      "√âpoca 0, Error batch 191: 0.19752833007858472\n",
      "√âpoca 0, Error batch 192: 0.1815141555004345\n",
      "√âpoca 0, Error batch 193: 0.10637368399391743\n",
      "√âpoca 0, Error batch 194: 0.1055303144398144\n",
      "√âpoca 0, Error batch 195: 0.09064487299108483\n",
      "√âpoca 0, Error batch 196: 0.14067755022287146\n",
      "√âpoca 0, Error batch 197: 0.11687133335779487\n",
      "√âpoca 0, Error batch 198: 0.19170138379491342\n",
      "√âpoca 0, Error batch 199: 0.12027657318715126\n",
      "√âpoca 0, Error batch 200: 0.09098288350979328\n",
      "√âpoca 0, Error batch 201: 0.13404700216279597\n",
      "√âpoca 0, Error batch 202: 0.11325108596528911\n",
      "√âpoca 0, Error batch 203: 0.21023919787566595\n",
      "√âpoca 0, Error batch 204: 0.07904531095376952\n",
      "√âpoca 0, Error batch 205: 0.13712821282090087\n",
      "√âpoca 0, Error batch 206: 0.16621922088473218\n",
      "√âpoca 0, Error batch 207: 0.09421089251596876\n",
      "√âpoca 0, Error batch 208: 0.12795130161480658\n",
      "√âpoca 0, Error batch 209: 0.08320081941663693\n",
      "√âpoca 0, Error batch 210: 0.07186399094738483\n",
      "√âpoca 0, Error batch 211: 0.13041762364564194\n",
      "√âpoca 0, Error batch 212: 0.14405062051589726\n",
      "√âpoca 0, Error batch 213: 0.10399078385750625\n",
      "√âpoca 0, Error batch 214: 0.12375300667581532\n",
      "√âpoca 0, Error batch 215: 0.17895756102685798\n",
      "√âpoca 0, Error batch 216: 0.12503990673867738\n",
      "√âpoca 0, Error batch 217: 0.11441627518499761\n",
      "√âpoca 0, Error batch 218: 0.22658837457058617\n",
      "√âpoca 0, Error batch 219: 0.1113438059385939\n",
      "√âpoca 0, Error batch 220: 0.21824938372896016\n",
      "√âpoca 0, Error batch 221: 0.145864818449603\n",
      "√âpoca 0, Error batch 222: 0.12141467038430412\n",
      "√âpoca 0, Error batch 223: 0.22069584225208397\n",
      "√âpoca 0, Error batch 224: 0.065213216762793\n",
      "√âpoca 0, Error batch 225: 0.12496522752337065\n",
      "√âpoca 0, Error batch 226: 0.058230097658209454\n",
      "√âpoca 0, Error batch 227: 0.22011906204248555\n",
      "√âpoca 0, Error batch 228: 0.12741674252552176\n",
      "√âpoca 0, Error batch 229: 0.15037001668811578\n",
      "√âpoca 0, Error batch 230: 0.15334883011563333\n",
      "√âpoca 0, Error batch 231: 0.06291579313778883\n",
      "√âpoca 0, Error batch 232: 0.2395784379210546\n",
      "√âpoca 0, Error batch 233: 0.15566935067216844\n",
      "√âpoca 0, Error batch 234: 0.1519677190754164\n",
      "√âpoca 0, Error batch 235: 0.2218908027736627\n",
      "√âpoca 0, Error batch 236: 0.18262788662387552\n",
      "√âpoca 0, Error batch 237: 0.145524905687719\n",
      "√âpoca 0, Error batch 238: 0.12855323847802463\n",
      "√âpoca 0, Error batch 239: 0.11824451953765153\n",
      "√âpoca 0, Error batch 240: 0.1790350102302613\n",
      "√âpoca 0, Error batch 241: 0.1107988543925319\n",
      "√âpoca 0, Error batch 242: 0.08970965878302697\n",
      "√âpoca 0, Error batch 243: 0.0006097814320119854\n",
      "√âpoca 0, Error batch 244: 0.06729300936947807\n",
      "√âpoca 0, Error batch 245: 0.06203451125259492\n",
      "√âpoca 0, Error batch 246: 0.24377444919000535\n",
      "√âpoca 0, Error batch 247: 0.2174819746091087\n",
      "√âpoca 0, Error batch 248: 0.09369287063971145\n",
      "√âpoca 0, Error batch 249: 0.21411344762367837\n",
      "√âpoca 0, Error batch 250: 0.3094413685116437\n",
      "√âpoca 0, Error batch 251: 0.0342048068754579\n",
      "√âpoca 0, Error batch 252: 0.0626859029595875\n",
      "√âpoca 0, Error batch 253: 0.13546114528678535\n",
      "√âpoca 0, Error batch 254: 0.19152320351056645\n",
      "√âpoca 0, Error batch 255: 0.09723008278656271\n",
      "√âpoca 0, Error batch 256: 0.16677385869658476\n",
      "√âpoca 0, Error batch 257: 0.13063022813186947\n",
      "√âpoca 0, Error: 0.13953207217368377%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_tr = X_tfidf.toarray()\n",
    "Y_tr = Y_train[:, np.newaxis] # Agregar una dimensi√≥n adicional para representar 1 ejemplo de entrenamiento por fila\n",
    "\n",
    "num_entradas= X_tr.shape[1] # tama√±o de la matriz Documento-T√©rmino\n",
    "num_neuronas_ocultas = 128\n",
    "num_salidas = 1\n",
    "epochs = 10 \n",
    "batch_size = 16\n",
    "learning_rate = 0.1\n",
    "random_state = 33\n",
    "norm= \"none\"\n",
    "init= \"normal\"\n",
    "\n",
    "clasificador_mlp = MLP_TODO(\n",
    "                        num_entradas,\n",
    "                        num_neuronas_ocultas,\n",
    "                        num_salidas,\n",
    "                        epochs,\n",
    "                        batch_size,\n",
    "                        learning_rate,\n",
    "                        norm,\n",
    "                        init\n",
    "                    )\n",
    "# Entrenamos al clasificador\n",
    "clasificador_mlp.train(X_tr, Y_tr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicci√≥n de datos nuevos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "['nonaggressive']\n"
     ]
    }
   ],
   "source": [
    "ejemplos_nuevos = [\"maldito perro\",]\n",
    "# Suponer que se cuenta con el objeto vec_tfidf entrenado con el vocabulario del conjunto de entrenamiento\n",
    "X_ejemplos_tfidf = vec_tfidf.transform(ejemplos_nuevos)\n",
    "X_ejemplos_tfidf = X_ejemplos_tfidf.toarray()\n",
    "print(X_ejemplos_tfidf)\n",
    "\n",
    "y_pred_nuevo = clasificador_mlp.predict(X_ejemplos_tfidf)\n",
    "y_pred_nuevo = y_pred_nuevo.flatten()\n",
    "print(le.inverse_transform(y_pred_nuevo))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Predecir los datos del conjuntos de prueba con el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "X_test_tfidf = vec_tfidf.transform(X_test)\n",
    "X_t = X_test_tfidf.toarray()\n",
    "Y_t = Y_test[:, np.newaxis] # Agregar una dimensi√≥n adicional para representar 1 ejemplo de entrenamiento por fila\n",
    "\n",
    "y_pred_test = clasificador_mlp.predict(X_t)\n",
    "print(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecci√≥n de los resultados de los primeros N ejemplos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textos:  ['@USUARIO @USUARIO Callate el hocico puta, ya quisieras ser mexicana Puta paname√±a de mierda tu pais es un asco igual que las personas que viven ahi puro puto negro de mierda que asco y luego son bien putos llorones ojala el bolillo se muera junto con toda la puta seleccion paname√±a aquerosa alv!!!'\n",
      " 'Twitter siempre saca mi lado filos√≥fico aunque est√° de la verga pero lo saca üòÅ #MartesDeGanarSeguidores'\n",
      " 'Comportandonos de la peor manera Dandole la espalda ala madre naturaleza Todos quieren dominar al mundo'\n",
      " 'Yo bien tonta gaste en suscribirme al canal + de BTS en V live pensando que ten√≠a que hacerlo para ver el comeback'\n",
      " 'C√ìMPRENME DULCES HIJOS DE SUS PUTAS MADRES como mantra del d√≠a.']\n",
      "clase esperada:  [0 1 1 1 1]\n",
      "clase predicha:  [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"textos: \", X_test[:5])\n",
    "print(\"clase esperada: \",Y_test[:5])\n",
    "print(\"clase predicha: \", y_pred_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrar la predicci√≥n de la clase original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 1 1 1 0 1]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test[:10])\n",
    "print(y_pred_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 1 1 1 0 1]\n",
      "[0 1 1 1 1 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# llevar a la misma forma la salida de las predicciones\n",
    "y_pred_test = y_pred_test.flatten()\n",
    "\n",
    "print(Y_test[:10])\n",
    "print(y_pred_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 1, 1, 1]),\n",
       " array(['aggressive', 'nonaggressive', 'nonaggressive', 'nonaggressive',\n",
       "        'nonaggressive'], dtype=object))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obten las primeras N predicciones\n",
    "pred =  y_pred_test[:5] \n",
    "pred_ori = le.inverse_transform(pred)\n",
    "pred, pred_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluando el desempe√±o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M√©tricas de Evaluaci√≥n\n",
    " - #### Las m√©tricas precisi√≥n, recall y F1 son fundamentales para evaluar el rendimiento de un clasificador\n",
    "\n",
    "\n",
    "<img src=\"figs/fig_precision-recall.png\" width=\"300\">\n",
    "\n",
    "##### Fuente: https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "\n",
    "<img src=\"figs/fig_matriz-confusion.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TP=True Positive\n",
    "\n",
    "TN=True Negative\n",
    "\n",
    "FP=False Positive (Error tipo I: ejemplo, se considera que el paciente est√° enfermo, pero en realidad est√° sano)\n",
    "\n",
    "FN=False Negative ( Error tipo II: ejemplo, se considera que el paciente est√° sano, pero en realidad est√° enfermo)\n",
    "\n",
    "\n",
    "$$ Accuracy = \\frac{total~ TP + total~TN}{total~muestras} $$\n",
    "\n",
    "$$ Precision_c = \\frac{ TP_c}{TP_c + FP_c} $$\n",
    "\n",
    "$$ Recall_c = \\frac{ TP_c}{TP_c + FN_c} $$\n",
    "\n",
    "$$ F1-score_c= 2 \\times \\frac{ Precision_c \\times Recall_c}{Precision_c + Recall_c} $$\n",
    "\n",
    "$$ macro-F1-score= \\frac{ 1 }{|Clases|} \\sum{F1-score_c} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de confusi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(Y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para la clase 0, la precisi√≥n es la siguiente\n",
    "tp= 82\n",
    "fp = 31+11+33\n",
    "tp/(tp + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P= 0.6739858700229261\n",
      "R= 0.6718582467556475\n",
      "F1= 0.6728873534143642\n",
      "Acc= 0.7332035053554041\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "print(\"P=\", precision_score(Y_test, y_pred_test, average='macro'))\n",
    "print(\"R=\", recall_score(Y_test, y_pred_test, average='macro'))\n",
    "print(\"F1=\", f1_score(Y_test, y_pred_test, average='macro'))\n",
    "print(\"Acc=\", accuracy_score(Y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecci√≥n del desempe√±o por clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[156 140]\n",
      " [134 597]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(Y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5379    0.5270    0.5324       296\n",
      "           1     0.8100    0.8167    0.8134       731\n",
      "\n",
      "    accuracy                         0.7332      1027\n",
      "   macro avg     0.6740    0.6719    0.6729      1027\n",
      "weighted avg     0.7316    0.7332    0.7324      1027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, y_pred_test, digits=4, zero_division='warn'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
