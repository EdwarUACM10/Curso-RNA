{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementar un MLP con PyTorch para clasificación basado en el dataset de agresividad\n",
    "\n",
    "<img src=\"figs/fig-MLP_XOR.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "1. **Definir los preprocesamientos para el texto**:  \n",
    "   - convertir a minúsculas\n",
    "   - normalizar el texto: borrar símbolos, puntuación, caracteres duplicados, etc.\n",
    "\n",
    "2. **Separar los datos para entrenamiento y prueba**:  \n",
    "   - Crear los dataset de entrenamiento y test con al función train_test_split \n",
    "\n",
    "3. **Construir la matriz de Documento-Término**:  \n",
    "   - Definir los parámetros para usar unigramas\n",
    "   - Usar la clase TfidfVectorizer para construir la matriz con los datos de entrenamiento\n",
    "\n",
    "   \n",
    "4. **Preparar los lotes de datos (minibatches) para el entrenamiento de la red**:  \n",
    "   - Definir los minibatches con la matriz TFIDF construida\n",
    "\n",
    "5. **Definir la arquitectura de la red**:  \n",
    "   - Definir entradas, salidas,  capas de la red y funciones de activación\n",
    "\n",
    "6. **Entrenar el modelo**:  \n",
    "   - Definir los parámetros de las red como: número de épocas, learning_rate, número de neuronas para las capas ocultas, etc.\n",
    "   \n",
    "7. **Evaluar el modelo**:  \n",
    "   - Después del entrenamiento, probar la red con las entradas del conjunto de test y evaluar el desempeño con las métricas: Precisión, Recall, F1-score o F1-Measure y Accuracy.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "_STOPWORDS = stopwords.words(\"spanish\")  # agregar más palabras a esta lista si es necesario\n",
    "\n",
    "# Normalización del texto\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n",
    "\n",
    "# Preprocesamiento personalizado \n",
    "def mi_preprocesamiento(texto):\n",
    "    #convierte a minúsculas el texto antes de normalizar\n",
    "    tokens = word_tokenize(texto.lower())\n",
    "    texto = \" \".join(tokens)\n",
    "    texto = normaliza_texto(texto)\n",
    "    return texto\n",
    "    \n",
    "# Tokenizador personalizado \n",
    "def mi_tokenizador(texto):\n",
    "    # Elimina stopwords: palabras que no se consideran de contenido y que no agregan valor semántico al texto\n",
    "    #print(\"antes: \", texto)\n",
    "    texto = [t for t in texto.split() if t not in _STOPWORDS]\n",
    "    #print(\"después:\",texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de los datos y minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klass\n",
      "nonaggressive    3655\n",
      "aggressive       1477\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "# colocar la semilla para la generación de números aleatorios para la reproducibilidad de experimentos\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# Cargar los datos de agresividad y separa los documentos (X) y las clases (Y)\n",
    "dataset = pd.read_json(\"./data_aggressiveness_es.json\", lines=True)\n",
    "print(dataset.klass.value_counts()) # muestra el conteo de cada clase\n",
    "# Extracción de los textos en arreglos de numpy\n",
    "X = dataset['text'].to_numpy()\n",
    "# Extracción de las etiquetas o clases de entrenamiento\n",
    "Y = dataset['klass'].to_numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 1 0 1]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Codificar las etiquetas de los datos a una forma categórica numérica: LabelEncoder.\n",
    "le = LabelEncoder()\n",
    "Y_encoded= le.fit_transform(Y)# Normalizar las etiquetas a una codificación ordinal para entrada del clasificador\n",
    "print(Y_encoded)\n",
    "Y_encoded = Y_encoded[:, np.newaxis] #Agregar una dimensión extra para que sea compatible con PyTorch\n",
    "print(Y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: (4105,)\n",
      "Tamaño del conjunto de pruebas: (1027,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# TODO: Dividir el conjunto de datos en conjunto de entrenamiento (80%) y conjunto de pruebas (20%)\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y_encoded, test_size=0.2, stratify=Y_encoded, random_state=42)\n",
    "print(\"Tamaño del conjunto de entrenamiento:\", X_train.shape)\n",
    "print(\"Tamaño del conjunto de pruebas:\", X_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\edwar\\miniconda3\\envs\\RNA\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TODO: Crear la matriz Documento-Término con el dataset de entrenamiento: tfidfVectorizer\n",
    "vec_tfidf = TfidfVectorizer(\n",
    "    analyzer=\"word\", preprocessor=mi_preprocesamiento, tokenizer=mi_tokenizador, ngram_range=(1,1)\n",
    ")\n",
    "\n",
    "#Entrenar vectorizador con el conjunto de entrenamiento\n",
    "X_train_tfidf = vec_tfidf.fit_transform(X_train)\n",
    "\n",
    "#Transformar el conjunto de prueba usando el mismo vocabulario\n",
    "X_test_tfidf = vec_tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crear minibatches en PyTorch usando DataLoader\n",
    "def create_minibatches(X, Y, batch_size):\n",
    "    # Recibe los documentos en X y las etiquetas en Y\n",
    "    dataset = TensorDataset(X, Y) # Cargar los datos en un dataset de tensores\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de la arquitectura de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir la red neuronal en PyTorch heredando de la clase base de Redes Neuronales: Module\n",
    "class MLP(nn.Module):\n",
    "    # Definir los parámetros para la creación de la clase MLP\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        # TODO: Definición de capas, funciones de activación e inicialización de pesos\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        if self.fc1.bias is not None:\n",
    "            nn.init.zeros_(self.fc1.bias)\n",
    "        if self.fc2.bias is not None:\n",
    "            nn.init.zeros_(self.fc2.bias) \n",
    "\n",
    "    def forward(self, X):\n",
    "        # Definición del orden de conexión de las capas y aplición de las funciones de activación\n",
    "        # TODO: DEFINIR EL ORDEN DE LAS CAPAS Y FUNCIONES DE ACTIVACIÓN\n",
    "        out = self.fc1(X)\n",
    "        out = self.sigmoid(out)  # Aplicamos la sigmoide en la capa oculta\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)  # Aplicamos la sigmoide en la capa de salida\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir la red neuronal en PyTorch heredando de la clase base de Redes Neuronales: Module\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        # Definición de capas, funciones de activación e inicialización de pesos\n",
    "        #AQUI PODEMOSS HACER UNA LIUSTA DE CAPAS INTERMEDIAS Y GUARDARLAS EN UNA LISTA.\n",
    "        #capas_intermedias.append(nn.Linear(1000,128 ))\n",
    "        #capas_intermedias.append(nn.Linear(128,64 ))\n",
    "        #capas_intermedias.append(nn.Linear(64,1 ))\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fcExt = nn.Linear(hidden_size, 32 )\n",
    "        self.fc2 = nn.Linear(32, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fcExt.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "        if self.fc1.bias is not None:\n",
    "            nn.init.zeros_(self.fc1.bias)\n",
    "        if self.fc1.bias is not None:\n",
    "            nn.init.zeros_(self.fcExt.bias)\n",
    "        if self.fc2.bias is not None:\n",
    "            nn.init.zeros_(self.fc2.bias)        \n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Definición del orden de conexión de las capas y aplición de las funciones de activación\n",
    "        out = self.fc1(X)\n",
    "        out = self.sigmoid(out)  # Aplicamos la sigmoide en la capa oculta\n",
    "        out = self.fcExt(out)\n",
    "        out = self.sigmoid(out)  # Aplicamos la sigmoide en la capa oculta\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)  # Aplicamos la sigmoide en la capa de salida\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edwar\\AppData\\Local\\Temp\\ipykernel_13324\\1152278739.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
      "C:\\Users\\edwar\\AppData\\Local\\Temp\\ipykernel_13324\\1152278739.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_test = torch.tensor(Y_test, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: Convertir los datos de entrenamiento y etiquetas a tensores  de PyTorch\n",
    "X_train = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor(X_test_tfidf.toarray(), dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test, dtype=torch.float32)\n",
    "\n",
    "# Establecer los parámetros de la red\n",
    "\n",
    "# Parámetros de la red\n",
    "input_size = X_train.shape[1] #Aqui se define el tamaño de entrada basado en la matriz TFIDF\n",
    "hidden_size = 64 # --> ? \n",
    "output_size = 1  # La salida es el resultado de la sigmoide para un clasificador \n",
    "                 # binario: 0 o 1\n",
    "epochs = 500    # variar el número de épocas, para probar que funciona la programación \n",
    "                 # solo usar 2 épocas, para entrenamiento total usar por ejemplo 1000 épocas\n",
    "learning_rate = 0.1   # Generalmente se usan learning rate pequeños entre [0,1] como (0.1, 0.3), \n",
    "                      #\n",
    "\n",
    "# Se recomiendan tamaños de batch_size potencias de 2: 16, 32, 64, 128, 256\n",
    "# Entre mayor el número más cantidad de memoria se requiere para el procesamiento\n",
    "batch_size = 32 # definir el tamaño del lote de procesamiento \n",
    "\n",
    "# Crear el modelo de la red \n",
    "# TODO: Ajustar los parámetros de acuerdo a su definición particular\n",
    "model = MLP(input_size, hidden_size, output_size)\n",
    "\n",
    "# TODO: Definir la función de pérdida\n",
    "# Mean Square Error (MSE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Definir el optimizador\n",
    "# Parámetros del optimizador: parámetros del modelo y learning rate \n",
    "# Stochastic Gradient Descent (SGD)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento en PyTorch\n",
      "Época 1/500, Pérdida: 0.21036820870268252\n",
      "Época 2/500, Pérdida: 0.20555176453072896\n",
      "Época 3/500, Pérdida: 0.206590351092723\n",
      "Época 4/500, Pérdida: 0.20517626664666241\n",
      "Época 5/500, Pérdida: 0.20495725509732268\n",
      "Época 6/500, Pérdida: 0.20495351086291233\n",
      "Época 7/500, Pérdida: 0.20547803191020508\n",
      "Época 8/500, Pérdida: 0.20504393890615583\n",
      "Época 9/500, Pérdida: 0.20487466541140578\n",
      "Época 10/500, Pérdida: 0.20562935014103734\n",
      "Época 11/500, Pérdida: 0.2052052412615266\n",
      "Época 12/500, Pérdida: 0.20590917766094208\n",
      "Época 13/500, Pérdida: 0.20505262582108033\n",
      "Época 14/500, Pérdida: 0.2057738792180091\n",
      "Época 15/500, Pérdida: 0.20511709812075593\n",
      "Época 16/500, Pérdida: 0.20501304718182067\n",
      "Época 17/500, Pérdida: 0.2050035379884779\n",
      "Época 18/500, Pérdida: 0.2057142594988032\n",
      "Época 19/500, Pérdida: 0.2054660220940908\n",
      "Época 20/500, Pérdida: 0.20580914778302806\n",
      "Época 21/500, Pérdida: 0.20483312155156172\n",
      "Época 22/500, Pérdida: 0.20542792571607485\n",
      "Época 23/500, Pérdida: 0.20499147324599037\n",
      "Época 24/500, Pérdida: 0.2056350485067959\n",
      "Época 25/500, Pérdida: 0.2051170988138332\n",
      "Época 26/500, Pérdida: 0.20554256901260495\n",
      "Época 27/500, Pérdida: 0.20491643712040067\n",
      "Época 28/500, Pérdida: 0.20468716115452523\n",
      "Época 29/500, Pérdida: 0.20583407355602398\n",
      "Época 30/500, Pérdida: 0.2049381506535434\n",
      "Época 31/500, Pérdida: 0.20567056005315262\n",
      "Época 32/500, Pérdida: 0.20517613178537797\n",
      "Época 33/500, Pérdida: 0.20564481096212253\n",
      "Época 34/500, Pérdida: 0.2055907503579014\n",
      "Época 35/500, Pérdida: 0.20559696818506995\n",
      "Época 36/500, Pérdida: 0.2044485006337018\n",
      "Época 37/500, Pérdida: 0.2056126306916392\n",
      "Época 38/500, Pérdida: 0.20490836138411087\n",
      "Época 39/500, Pérdida: 0.20557718140672343\n",
      "Época 40/500, Pérdida: 0.20489639321038888\n",
      "Época 41/500, Pérdida: 0.20440577247808145\n",
      "Época 42/500, Pérdida: 0.20503426707068154\n",
      "Época 43/500, Pérdida: 0.20476196619660356\n",
      "Época 44/500, Pérdida: 0.20602439291948496\n",
      "Época 45/500, Pérdida: 0.20507140307463417\n",
      "Época 46/500, Pérdida: 0.20542506873607635\n",
      "Época 47/500, Pérdida: 0.2048470073545626\n",
      "Época 48/500, Pérdida: 0.20555711312349453\n",
      "Época 49/500, Pérdida: 0.20553094552930937\n",
      "Época 50/500, Pérdida: 0.2049405328756155\n",
      "Época 51/500, Pérdida: 0.20511580634024718\n",
      "Época 52/500, Pérdida: 0.2053797828365666\n",
      "Época 53/500, Pérdida: 0.20472820253335228\n",
      "Época 54/500, Pérdida: 0.2049917786620384\n",
      "Época 55/500, Pérdida: 0.20515450326971305\n",
      "Época 56/500, Pérdida: 0.20481916257115299\n",
      "Época 57/500, Pérdida: 0.20497271187545718\n",
      "Época 58/500, Pérdida: 0.20488268142753793\n",
      "Época 59/500, Pérdida: 0.2045269443314205\n",
      "Época 60/500, Pérdida: 0.2049456775303959\n",
      "Época 61/500, Pérdida: 0.2050560113302497\n",
      "Época 62/500, Pérdida: 0.2044044638326926\n",
      "Época 63/500, Pérdida: 0.20478527028431265\n",
      "Época 64/500, Pérdida: 0.20464224576256995\n",
      "Época 65/500, Pérdida: 0.20472899830156518\n",
      "Época 66/500, Pérdida: 0.204339131250862\n",
      "Época 67/500, Pérdida: 0.20530999654023221\n",
      "Época 68/500, Pérdida: 0.2049554342678351\n",
      "Época 69/500, Pérdida: 0.2047440577731576\n",
      "Época 70/500, Pérdida: 0.204964004861292\n",
      "Época 71/500, Pérdida: 0.20507200264422468\n",
      "Época 72/500, Pérdida: 0.20502408495707106\n",
      "Época 73/500, Pérdida: 0.20502706045328184\n",
      "Época 74/500, Pérdida: 0.20431758740613626\n",
      "Época 75/500, Pérdida: 0.20449639227270155\n",
      "Época 76/500, Pérdida: 0.20509164774602698\n",
      "Época 77/500, Pérdida: 0.20469948429931967\n",
      "Época 78/500, Pérdida: 0.20475540847279305\n",
      "Época 79/500, Pérdida: 0.20446111365806224\n",
      "Época 80/500, Pérdida: 0.2043264057169589\n",
      "Época 81/500, Pérdida: 0.20490217220413592\n",
      "Época 82/500, Pérdida: 0.20502695516329403\n",
      "Época 83/500, Pérdida: 0.20437191022458925\n",
      "Época 84/500, Pérdida: 0.20492146400980246\n",
      "Época 85/500, Pérdida: 0.20465915263161177\n",
      "Época 86/500, Pérdida: 0.20527317819669266\n",
      "Época 87/500, Pérdida: 0.20489156500313632\n",
      "Época 88/500, Pérdida: 0.2047707869331966\n",
      "Época 89/500, Pérdida: 0.20487150630747625\n",
      "Época 90/500, Pérdida: 0.20621872629768165\n",
      "Época 91/500, Pérdida: 0.20448264629803886\n",
      "Época 92/500, Pérdida: 0.20540814309619193\n",
      "Época 93/500, Pérdida: 0.2049512194339619\n",
      "Época 94/500, Pérdida: 0.2044771633637968\n",
      "Época 95/500, Pérdida: 0.20487664362718894\n",
      "Época 96/500, Pérdida: 0.20403704480376356\n",
      "Época 97/500, Pérdida: 0.20448301946238953\n",
      "Época 98/500, Pérdida: 0.20448765147101972\n",
      "Época 99/500, Pérdida: 0.20500042791976486\n",
      "Época 100/500, Pérdida: 0.2041072724170463\n",
      "Época 101/500, Pérdida: 0.20446472528368928\n",
      "Época 102/500, Pérdida: 0.20414911089248436\n",
      "Época 103/500, Pérdida: 0.20548188524652822\n",
      "Época 104/500, Pérdida: 0.20540358230125072\n",
      "Época 105/500, Pérdida: 0.2041726080022117\n",
      "Época 106/500, Pérdida: 0.20460209608540053\n",
      "Época 107/500, Pérdida: 0.20435056587060293\n",
      "Época 108/500, Pérdida: 0.2042505411907684\n",
      "Época 109/500, Pérdida: 0.20375650705293166\n",
      "Época 110/500, Pérdida: 0.2040111557688824\n",
      "Época 111/500, Pérdida: 0.20553908820531164\n",
      "Época 112/500, Pérdida: 0.2041527877251307\n",
      "Época 113/500, Pérdida: 0.20462958341421084\n",
      "Época 114/500, Pérdida: 0.20370010425185048\n",
      "Época 115/500, Pérdida: 0.20450927016809006\n",
      "Época 116/500, Pérdida: 0.2044739822546641\n",
      "Época 117/500, Pérdida: 0.20378023738315862\n",
      "Época 118/500, Pérdida: 0.20396981498067693\n",
      "Época 119/500, Pérdida: 0.20442430039708928\n",
      "Época 120/500, Pérdida: 0.20389222012933836\n",
      "Época 121/500, Pérdida: 0.2045998166697894\n",
      "Época 122/500, Pérdida: 0.20414436891559481\n",
      "Época 123/500, Pérdida: 0.2032063016018202\n",
      "Época 124/500, Pérdida: 0.20438320790381395\n",
      "Época 125/500, Pérdida: 0.20378551990255828\n",
      "Época 126/500, Pérdida: 0.20443408537742702\n",
      "Época 127/500, Pérdida: 0.20437498431104098\n",
      "Época 128/500, Pérdida: 0.20393203464589377\n",
      "Época 129/500, Pérdida: 0.2036877718082694\n",
      "Época 130/500, Pérdida: 0.20405275646106216\n",
      "Época 131/500, Pérdida: 0.20430403150791346\n",
      "Época 132/500, Pérdida: 0.20438532230927964\n",
      "Época 133/500, Pérdida: 0.20356692200483278\n",
      "Época 134/500, Pérdida: 0.20403788475565207\n",
      "Época 135/500, Pérdida: 0.20332849585963775\n",
      "Época 136/500, Pérdida: 0.20362726212009902\n",
      "Época 137/500, Pérdida: 0.20375400862490484\n",
      "Época 138/500, Pérdida: 0.2038236165693564\n",
      "Época 139/500, Pérdida: 0.20397926532020866\n",
      "Época 140/500, Pérdida: 0.2033428583265275\n",
      "Época 141/500, Pérdida: 0.203939598082572\n",
      "Época 142/500, Pérdida: 0.2034697464501211\n",
      "Época 143/500, Pérdida: 0.20385776829811952\n",
      "Época 144/500, Pérdida: 0.20361362807741462\n",
      "Época 145/500, Pérdida: 0.20266015902739162\n",
      "Época 146/500, Pérdida: 0.20268805885268737\n",
      "Época 147/500, Pérdida: 0.20307942710874616\n",
      "Época 148/500, Pérdida: 0.20333799823772075\n",
      "Época 149/500, Pérdida: 0.202669555480166\n",
      "Época 150/500, Pérdida: 0.20349458298941916\n",
      "Época 151/500, Pérdida: 0.20269950311775356\n",
      "Época 152/500, Pérdida: 0.20318515194478884\n",
      "Época 153/500, Pérdida: 0.2032890046058699\n",
      "Época 154/500, Pérdida: 0.20318449109561684\n",
      "Época 155/500, Pérdida: 0.20271641050660333\n",
      "Época 156/500, Pérdida: 0.20271821424018505\n",
      "Época 157/500, Pérdida: 0.20296071637277455\n",
      "Época 158/500, Pérdida: 0.2021974118412003\n",
      "Época 159/500, Pérdida: 0.20258632429348405\n",
      "Época 160/500, Pérdida: 0.20276755845361902\n",
      "Época 161/500, Pérdida: 0.201632448984671\n",
      "Época 162/500, Pérdida: 0.20204428577607916\n",
      "Época 163/500, Pérdida: 0.20181474239789238\n",
      "Época 164/500, Pérdida: 0.2015558748628742\n",
      "Época 165/500, Pérdida: 0.2011868794297063\n",
      "Época 166/500, Pérdida: 0.20203850562720335\n",
      "Época 167/500, Pérdida: 0.2014795354632444\n",
      "Época 168/500, Pérdida: 0.20108851155107335\n",
      "Época 169/500, Pérdida: 0.20078309284624205\n",
      "Época 170/500, Pérdida: 0.20068673812603766\n",
      "Época 171/500, Pérdida: 0.20107035165609316\n",
      "Época 172/500, Pérdida: 0.2009590574244196\n",
      "Época 173/500, Pérdida: 0.2003276971883552\n",
      "Época 174/500, Pérdida: 0.19966736372358115\n",
      "Época 175/500, Pérdida: 0.20029286745675776\n",
      "Época 176/500, Pérdida: 0.1993445451985034\n",
      "Época 177/500, Pérdida: 0.20051376839247784\n",
      "Época 178/500, Pérdida: 0.2005966690390609\n",
      "Época 179/500, Pérdida: 0.1991633111538813\n",
      "Época 180/500, Pérdida: 0.19914731342894162\n",
      "Época 181/500, Pérdida: 0.19886309549558995\n",
      "Época 182/500, Pérdida: 0.19956271535204362\n",
      "Época 183/500, Pérdida: 0.19889365725739058\n",
      "Época 184/500, Pérdida: 0.19847930540410122\n",
      "Época 185/500, Pérdida: 0.19828423794156821\n",
      "Época 186/500, Pérdida: 0.19766443344049675\n",
      "Época 187/500, Pérdida: 0.19783034418211426\n",
      "Época 188/500, Pérdida: 0.1972787349030029\n",
      "Época 189/500, Pérdida: 0.19666908721822177\n",
      "Época 190/500, Pérdida: 0.19698964532955673\n",
      "Época 191/500, Pérdida: 0.1962782578874928\n",
      "Época 192/500, Pérdida: 0.19562441948079323\n",
      "Época 193/500, Pérdida: 0.19593476042035937\n",
      "Época 194/500, Pérdida: 0.19498428147892619\n",
      "Época 195/500, Pérdida: 0.19510308832161186\n",
      "Época 196/500, Pérdida: 0.1939067168291225\n",
      "Época 197/500, Pérdida: 0.19332180420557657\n",
      "Época 198/500, Pérdida: 0.1929201086362203\n",
      "Época 199/500, Pérdida: 0.19238496069298233\n",
      "Época 200/500, Pérdida: 0.19219255066195198\n",
      "Época 201/500, Pérdida: 0.19130403300126395\n",
      "Época 202/500, Pérdida: 0.19107132999933968\n",
      "Época 203/500, Pérdida: 0.19031229615211487\n",
      "Época 204/500, Pérdida: 0.18953587727029195\n",
      "Época 205/500, Pérdida: 0.18879051310147427\n",
      "Época 206/500, Pérdida: 0.18850583380968997\n",
      "Época 207/500, Pérdida: 0.1873369095630424\n",
      "Época 208/500, Pérdida: 0.18654965366735016\n",
      "Época 209/500, Pérdida: 0.1852788855863172\n",
      "Época 210/500, Pérdida: 0.1848606994563295\n",
      "Época 211/500, Pérdida: 0.1845718139363814\n",
      "Época 212/500, Pérdida: 0.18311734041271283\n",
      "Época 213/500, Pérdida: 0.18178513438202615\n",
      "Época 214/500, Pérdida: 0.18127672553293464\n",
      "Época 215/500, Pérdida: 0.17976110191770303\n",
      "Época 216/500, Pérdida: 0.17869219888550367\n",
      "Época 217/500, Pérdida: 0.17797137386808098\n",
      "Época 218/500, Pérdida: 0.17624717679365662\n",
      "Época 219/500, Pérdida: 0.17491669115401054\n",
      "Época 220/500, Pérdida: 0.1738862197181975\n",
      "Época 221/500, Pérdida: 0.17262785177129183\n",
      "Época 222/500, Pérdida: 0.1714720313632211\n",
      "Época 223/500, Pérdida: 0.169831038272196\n",
      "Época 224/500, Pérdida: 0.16925735292277594\n",
      "Época 225/500, Pérdida: 0.1673032322595286\n",
      "Época 226/500, Pérdida: 0.16579095453254936\n",
      "Época 227/500, Pérdida: 0.16436034475648126\n",
      "Época 228/500, Pérdida: 0.16230535842189492\n",
      "Época 229/500, Pérdida: 0.16030569530503694\n",
      "Época 230/500, Pérdida: 0.1589075008104014\n",
      "Época 231/500, Pérdida: 0.15806221644314686\n",
      "Época 232/500, Pérdida: 0.15555810529825298\n",
      "Época 233/500, Pérdida: 0.15358678661575614\n",
      "Época 234/500, Pérdida: 0.15190370438634887\n",
      "Época 235/500, Pérdida: 0.15025692987580633\n",
      "Época 236/500, Pérdida: 0.14793212486560955\n",
      "Época 237/500, Pérdida: 0.147330232599909\n",
      "Época 238/500, Pérdida: 0.14391670621527258\n",
      "Época 239/500, Pérdida: 0.14225785925175793\n",
      "Época 240/500, Pérdida: 0.1404776312122049\n",
      "Época 241/500, Pérdida: 0.13946893216334572\n",
      "Época 242/500, Pérdida: 0.13729249157531317\n",
      "Época 243/500, Pérdida: 0.13453140202187752\n",
      "Época 244/500, Pérdida: 0.13283702176670695\n",
      "Época 245/500, Pérdida: 0.13167733392974204\n",
      "Época 246/500, Pérdida: 0.1300643146500107\n",
      "Época 247/500, Pérdida: 0.12718088087416435\n",
      "Época 248/500, Pérdida: 0.12664492815270906\n",
      "Época 249/500, Pérdida: 0.12417665796802026\n",
      "Época 250/500, Pérdida: 0.12253189502760421\n",
      "Época 251/500, Pérdida: 0.12068570506318595\n",
      "Época 252/500, Pérdida: 0.11756864822534628\n",
      "Época 253/500, Pérdida: 0.11655669509209404\n",
      "Época 254/500, Pérdida: 0.1153659083006918\n",
      "Época 255/500, Pérdida: 0.11359344379499901\n",
      "Época 256/500, Pérdida: 0.1113473639585251\n",
      "Época 257/500, Pérdida: 0.11042357859916466\n",
      "Época 258/500, Pérdida: 0.10823247497973516\n",
      "Época 259/500, Pérdida: 0.10692408210200857\n",
      "Época 260/500, Pérdida: 0.10604887138041415\n",
      "Época 261/500, Pérdida: 0.10394712459555892\n",
      "Época 262/500, Pérdida: 0.10294391801075418\n",
      "Época 263/500, Pérdida: 0.10017010532030764\n",
      "Época 264/500, Pérdida: 0.09850952895574791\n",
      "Época 265/500, Pérdida: 0.09843431153269701\n",
      "Época 266/500, Pérdida: 0.09686767878805021\n",
      "Época 267/500, Pérdida: 0.09437708028180655\n",
      "Época 268/500, Pérdida: 0.0932326315100803\n",
      "Época 269/500, Pérdida: 0.0919655891525191\n",
      "Época 270/500, Pérdida: 0.09095586928748345\n",
      "Época 271/500, Pérdida: 0.09045509110356487\n",
      "Época 272/500, Pérdida: 0.08712861863102099\n",
      "Época 273/500, Pérdida: 0.08649292033772137\n",
      "Época 274/500, Pérdida: 0.0852431460002134\n",
      "Época 275/500, Pérdida: 0.08361037201139816\n",
      "Época 276/500, Pérdida: 0.08208770284703536\n",
      "Época 277/500, Pérdida: 0.07946655275516731\n",
      "Época 278/500, Pérdida: 0.07968234270811081\n",
      "Época 279/500, Pérdida: 0.07895692218412724\n",
      "Época 280/500, Pérdida: 0.07756108598595904\n",
      "Época 281/500, Pérdida: 0.07601891800241414\n",
      "Época 282/500, Pérdida: 0.073859851691843\n",
      "Época 283/500, Pérdida: 0.07339023150388123\n",
      "Época 284/500, Pérdida: 0.0718539749640365\n",
      "Época 285/500, Pérdida: 0.07042658011349597\n",
      "Época 286/500, Pérdida: 0.06939838514771572\n",
      "Época 287/500, Pérdida: 0.06762386667867039\n",
      "Época 288/500, Pérdida: 0.06672548083949459\n",
      "Época 289/500, Pérdida: 0.06562789831339388\n",
      "Época 290/500, Pérdida: 0.06396081310140994\n",
      "Época 291/500, Pérdida: 0.06242866728945758\n",
      "Época 292/500, Pérdida: 0.06158634906187076\n",
      "Época 293/500, Pérdida: 0.05995167440973049\n",
      "Época 294/500, Pérdida: 0.059645526977472524\n",
      "Época 295/500, Pérdida: 0.05805835393510123\n",
      "Época 296/500, Pérdida: 0.05683334945708282\n",
      "Época 297/500, Pérdida: 0.0562294376364281\n",
      "Época 298/500, Pérdida: 0.054824507600346274\n",
      "Época 299/500, Pérdida: 0.05453683786787266\n",
      "Época 300/500, Pérdida: 0.05220199617477812\n",
      "Época 301/500, Pérdida: 0.05128823105938906\n",
      "Época 302/500, Pérdida: 0.05120047913907572\n",
      "Época 303/500, Pérdida: 0.049496747473933435\n",
      "Época 304/500, Pérdida: 0.04823927065112101\n",
      "Época 305/500, Pérdida: 0.047916011978662756\n",
      "Época 306/500, Pérdida: 0.04570252880049768\n",
      "Época 307/500, Pérdida: 0.04530737139717784\n",
      "Época 308/500, Pérdida: 0.04389519582668594\n",
      "Época 309/500, Pérdida: 0.043431736442238786\n",
      "Época 310/500, Pérdida: 0.04257532014229963\n",
      "Época 311/500, Pérdida: 0.04137204934720152\n",
      "Época 312/500, Pérdida: 0.04066063784958087\n",
      "Época 313/500, Pérdida: 0.039261029903278795\n",
      "Época 314/500, Pérdida: 0.03866334512916415\n",
      "Época 315/500, Pérdida: 0.03763567470014095\n",
      "Época 316/500, Pérdida: 0.037200448435348595\n",
      "Época 317/500, Pérdida: 0.03652727848655263\n",
      "Época 318/500, Pérdida: 0.035719586810631344\n",
      "Época 319/500, Pérdida: 0.0344004479070851\n",
      "Época 320/500, Pérdida: 0.03380749087289784\n",
      "Época 321/500, Pérdida: 0.03306101471214563\n",
      "Época 322/500, Pérdida: 0.03195695995620286\n",
      "Época 323/500, Pérdida: 0.031231069857933263\n",
      "Época 324/500, Pérdida: 0.03123005994328464\n",
      "Época 325/500, Pérdida: 0.03007935162980196\n",
      "Época 326/500, Pérdida: 0.02902354257426752\n",
      "Época 327/500, Pérdida: 0.02887679388847693\n",
      "Época 328/500, Pérdida: 0.028055574701622475\n",
      "Época 329/500, Pérdida: 0.02740910028417905\n",
      "Época 330/500, Pérdida: 0.026525199030663155\n",
      "Época 331/500, Pérdida: 0.026631623636498007\n",
      "Época 332/500, Pérdida: 0.025745443281421598\n",
      "Época 333/500, Pérdida: 0.025384456366878146\n",
      "Época 334/500, Pérdida: 0.024316679198987955\n",
      "Época 335/500, Pérdida: 0.023873355142165754\n",
      "Época 336/500, Pérdida: 0.02339297894734976\n",
      "Época 337/500, Pérdida: 0.0228432910372467\n",
      "Época 338/500, Pérdida: 0.02253425289162023\n",
      "Época 339/500, Pérdida: 0.022242329007577756\n",
      "Época 340/500, Pérdida: 0.021202190252122027\n",
      "Época 341/500, Pérdida: 0.02103876788891101\n",
      "Época 342/500, Pérdida: 0.020252319550964723\n",
      "Época 343/500, Pérdida: 0.019725266161786263\n",
      "Época 344/500, Pérdida: 0.019531495861940144\n",
      "Época 345/500, Pérdida: 0.01913693343657394\n",
      "Época 346/500, Pérdida: 0.018617423467857892\n",
      "Época 347/500, Pérdida: 0.018569009497675093\n",
      "Época 348/500, Pérdida: 0.018027233071769625\n",
      "Época 349/500, Pérdida: 0.017430264546081077\n",
      "Época 350/500, Pérdida: 0.017298959469956944\n",
      "Época 351/500, Pérdida: 0.01680926036797175\n",
      "Época 352/500, Pérdida: 0.016583952431877453\n",
      "Época 353/500, Pérdida: 0.016177409505399393\n",
      "Época 354/500, Pérdida: 0.01599894582777638\n",
      "Época 355/500, Pérdida: 0.015545315643440383\n",
      "Época 356/500, Pérdida: 0.015160156016837257\n",
      "Época 357/500, Pérdida: 0.014859569854601178\n",
      "Época 358/500, Pérdida: 0.01445035498170543\n",
      "Época 359/500, Pérdida: 0.014062060987172548\n",
      "Época 360/500, Pérdida: 0.013794169169236986\n",
      "Época 361/500, Pérdida: 0.013501405701503273\n",
      "Época 362/500, Pérdida: 0.013260531851068832\n",
      "Época 363/500, Pérdida: 0.012838272834485468\n",
      "Época 364/500, Pérdida: 0.012707912327014199\n",
      "Época 365/500, Pérdida: 0.012525091417018295\n",
      "Época 366/500, Pérdida: 0.012137434533761107\n",
      "Época 367/500, Pérdida: 0.01210265201444254\n",
      "Época 368/500, Pérdida: 0.011675074523271517\n",
      "Época 369/500, Pérdida: 0.011396480018504022\n",
      "Época 370/500, Pérdida: 0.011104711051669347\n",
      "Época 371/500, Pérdida: 0.010939037324427518\n",
      "Época 372/500, Pérdida: 0.010737924455744468\n",
      "Época 373/500, Pérdida: 0.010612861628738023\n",
      "Época 374/500, Pérdida: 0.010408482815738218\n",
      "Época 375/500, Pérdida: 0.01014830019729313\n",
      "Época 376/500, Pérdida: 0.010046880340290278\n",
      "Época 377/500, Pérdida: 0.009827978937893875\n",
      "Época 378/500, Pérdida: 0.009618104809115445\n",
      "Época 379/500, Pérdida: 0.009384818268622192\n",
      "Época 380/500, Pérdida: 0.009142050515975832\n",
      "Época 381/500, Pérdida: 0.008938698244259455\n",
      "Época 382/500, Pérdida: 0.008908219651730602\n",
      "Época 383/500, Pérdida: 0.0086776096510968\n",
      "Época 384/500, Pérdida: 0.00867855394360169\n",
      "Época 385/500, Pérdida: 0.008357767598674626\n",
      "Época 386/500, Pérdida: 0.008272017621211419\n",
      "Época 387/500, Pérdida: 0.008116220264847195\n",
      "Época 388/500, Pérdida: 0.007911970580805295\n",
      "Época 389/500, Pérdida: 0.007879359783900744\n",
      "Época 390/500, Pérdida: 0.007692757329737493\n",
      "Época 391/500, Pérdida: 0.007484094665725102\n",
      "Época 392/500, Pérdida: 0.007424829231733962\n",
      "Época 393/500, Pérdida: 0.007245704486547399\n",
      "Época 394/500, Pérdida: 0.007166084894809381\n",
      "Época 395/500, Pérdida: 0.007134326594672346\n",
      "Época 396/500, Pérdida: 0.006982124776962076\n",
      "Época 397/500, Pérdida: 0.006878514609898823\n",
      "Época 398/500, Pérdida: 0.006747821912660386\n",
      "Época 399/500, Pérdida: 0.006604126986664857\n",
      "Época 400/500, Pérdida: 0.006542294034409488\n",
      "Época 401/500, Pérdida: 0.006468529592795196\n",
      "Época 402/500, Pérdida: 0.006427525383846242\n",
      "Época 403/500, Pérdida: 0.006267239864872292\n",
      "Época 404/500, Pérdida: 0.00611930630393615\n",
      "Época 405/500, Pérdida: 0.006037121940974522\n",
      "Época 406/500, Pérdida: 0.00598524982658352\n",
      "Época 407/500, Pérdida: 0.005915836949089123\n",
      "Época 408/500, Pérdida: 0.005775817085144131\n",
      "Época 409/500, Pérdida: 0.005689434498700119\n",
      "Época 410/500, Pérdida: 0.005638823841785102\n",
      "Época 411/500, Pérdida: 0.005568484825356467\n",
      "Época 412/500, Pérdida: 0.005465971525260355\n",
      "Época 413/500, Pérdida: 0.005369363778158878\n",
      "Época 414/500, Pérdida: 0.005311367398871528\n",
      "Época 415/500, Pérdida: 0.005286772200267386\n",
      "Época 416/500, Pérdida: 0.005128037342404614\n",
      "Época 417/500, Pérdida: 0.005123253989343842\n",
      "Época 418/500, Pérdida: 0.005032483789737719\n",
      "Época 419/500, Pérdida: 0.004974767022455732\n",
      "Época 420/500, Pérdida: 0.004930068533903696\n",
      "Época 421/500, Pérdida: 0.004873799531027501\n",
      "Época 422/500, Pérdida: 0.004813903386798478\n",
      "Época 423/500, Pérdida: 0.004762788564002849\n",
      "Época 424/500, Pérdida: 0.004704471487125974\n",
      "Época 425/500, Pérdida: 0.004704710589757897\n",
      "Época 426/500, Pérdida: 0.004586170228963443\n",
      "Época 427/500, Pérdida: 0.004509525244528529\n",
      "Época 428/500, Pérdida: 0.004460601386065458\n",
      "Época 429/500, Pérdida: 0.004413775486922137\n",
      "Época 430/500, Pérdida: 0.004346678091227546\n",
      "Época 431/500, Pérdida: 0.004319843560883183\n",
      "Época 432/500, Pérdida: 0.004287264631996148\n",
      "Época 433/500, Pérdida: 0.004227425015527871\n",
      "Época 434/500, Pérdida: 0.004192532409278517\n",
      "Época 435/500, Pérdida: 0.004143040863681556\n",
      "Época 436/500, Pérdida: 0.0041166911296033875\n",
      "Época 437/500, Pérdida: 0.004039230301629665\n",
      "Época 438/500, Pérdida: 0.0040211541823638505\n",
      "Época 439/500, Pérdida: 0.003984031596856351\n",
      "Época 440/500, Pérdida: 0.0039194161099446725\n",
      "Época 441/500, Pérdida: 0.003919601839310093\n",
      "Época 442/500, Pérdida: 0.0038989425949073693\n",
      "Época 443/500, Pérdida: 0.0038422735934976\n",
      "Época 444/500, Pérdida: 0.003807282122213018\n",
      "Época 445/500, Pérdida: 0.0037420048827825243\n",
      "Época 446/500, Pérdida: 0.0037298155659737514\n",
      "Época 447/500, Pérdida: 0.00368076355725368\n",
      "Época 448/500, Pérdida: 0.0036612880734777325\n",
      "Época 449/500, Pérdida: 0.003640446540278693\n",
      "Época 450/500, Pérdida: 0.0036075832863706487\n",
      "Época 451/500, Pérdida: 0.003549999937364586\n",
      "Época 452/500, Pérdida: 0.00351682230839217\n",
      "Época 453/500, Pérdida: 0.0035106099617681933\n",
      "Época 454/500, Pérdida: 0.0034790304182602686\n",
      "Época 455/500, Pérdida: 0.003422073057452\n",
      "Época 456/500, Pérdida: 0.0034131311997410286\n",
      "Época 457/500, Pérdida: 0.0033770619188579765\n",
      "Época 458/500, Pérdida: 0.003346109570115949\n",
      "Época 459/500, Pérdida: 0.0033174499164537576\n",
      "Época 460/500, Pérdida: 0.0033113227294425864\n",
      "Época 461/500, Pérdida: 0.0032661554039015556\n",
      "Época 462/500, Pérdida: 0.0032727457470145046\n",
      "Época 463/500, Pérdida: 0.0032304342328065413\n",
      "Época 464/500, Pérdida: 0.003183203759285542\n",
      "Época 465/500, Pérdida: 0.0031631834342057794\n",
      "Época 466/500, Pérdida: 0.0031537746766026227\n",
      "Época 467/500, Pérdida: 0.003097842668558565\n",
      "Época 468/500, Pérdida: 0.0030617901528355224\n",
      "Época 469/500, Pérdida: 0.0030412576741145475\n",
      "Época 470/500, Pérdida: 0.0030060863526639088\n",
      "Época 471/500, Pérdida: 0.0029738323366839815\n",
      "Época 472/500, Pérdida: 0.0029562906544410095\n",
      "Época 473/500, Pérdida: 0.0029196481147326183\n",
      "Época 474/500, Pérdida: 0.0028986938693921637\n",
      "Época 475/500, Pérdida: 0.0028843369074129724\n",
      "Época 476/500, Pérdida: 0.002856404303082142\n",
      "Época 477/500, Pérdida: 0.0028098393994195805\n",
      "Época 478/500, Pérdida: 0.002792107314954317\n",
      "Época 479/500, Pérdida: 0.002753606757831348\n",
      "Época 480/500, Pérdida: 0.002742940131773407\n",
      "Época 481/500, Pérdida: 0.002703674832925691\n",
      "Época 482/500, Pérdida: 0.002697613305960919\n",
      "Época 483/500, Pérdida: 0.002669818448480363\n",
      "Época 484/500, Pérdida: 0.0026458287575505963\n",
      "Época 485/500, Pérdida: 0.002615559673344702\n",
      "Época 486/500, Pérdida: 0.0026194680659930663\n",
      "Época 487/500, Pérdida: 0.0026208590251364738\n",
      "Época 488/500, Pérdida: 0.0025744760862927048\n",
      "Época 489/500, Pérdida: 0.0025613959210874265\n",
      "Época 490/500, Pérdida: 0.0025272781296467135\n",
      "Época 491/500, Pérdida: 0.0025233883205837346\n",
      "Época 492/500, Pérdida: 0.002498504296017363\n",
      "Época 493/500, Pérdida: 0.0024838872355871016\n",
      "Época 494/500, Pérdida: 0.0024660710848203693\n",
      "Época 495/500, Pérdida: 0.002454075444753231\n",
      "Época 496/500, Pérdida: 0.00244157118794992\n",
      "Época 497/500, Pérdida: 0.0024460406934250466\n",
      "Época 498/500, Pérdida: 0.0024221538500174477\n",
      "Época 499/500, Pérdida: 0.002398408786935169\n",
      "Época 500/500, Pérdida: 0.002392772218309949\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Entrenamiento\n",
    "print(\"Iniciando entrenamiento en PyTorch\")\n",
    "\n",
    "# Poner el modelo en modo de entrenamiento\n",
    "model.train()  \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    lossTotal = 0\n",
    "    #definir el batch_size\n",
    "    dataloader = create_minibatches(X_train, Y_train, batch_size=batch_size)\n",
    "    for X_tr, y_tr in dataloader:\n",
    "        \n",
    "        # TODO: Inicializar los gradientes en cero para cada época\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        # TODO: Propagación hacia adelante\n",
    "        # invoca al método forward de la clase MLP con los datos de entrenamiento\n",
    "        y_pred =  model(X_tr)\n",
    "        \n",
    "        # Calcular el error MSE, de acuerdo a lo predicho (y_pred)  y la clase objetivo (y)\n",
    "        loss = criterion(y_pred, y_tr)\n",
    "        \n",
    "        # TODO: Acumular el error \n",
    "        lossTotal += loss.item()\n",
    "        \n",
    "        # TODO: Propagación hacia atrás: cálculo de los gradientes de los pesos y bias\n",
    "        loss.backward()\n",
    "        \n",
    "        # TODO: Actualización de los pesos: regla de actualización basado en el gradiente:\n",
    "        # Regla delta:  W = W - learning_rate * dE/dW\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Época {epoch+1}/{epochs}, Pérdida: {lossTotal/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modo para predicción de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x10609 and 2x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m X_tr, y_tr \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[32m      6\u001b[39m     optimizer.zero_grad()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     y_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     loss = criterion(y_pred, y_tr)\n\u001b[32m      9\u001b[39m     lossTotal += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\edwar\\miniconda3\\envs\\RNA\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\edwar\\miniconda3\\envs\\RNA\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mMLP.forward\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     29\u001b[39m out = \u001b[38;5;28mself\u001b[39m.fc1(X)\n\u001b[32m     30\u001b[39m out = \u001b[38;5;28mself\u001b[39m.sigmoid(out)  \u001b[38;5;66;03m# Aplicamos la sigmoide en la capa oculta\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfcExt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m out = \u001b[38;5;28mself\u001b[39m.sigmoid(out)  \u001b[38;5;66;03m# Aplicamos la sigmoide en la capa oculta\u001b[39;00m\n\u001b[32m     33\u001b[39m out = \u001b[38;5;28mself\u001b[39m.fc2(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\edwar\\miniconda3\\envs\\RNA\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\edwar\\miniconda3\\envs\\RNA\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\edwar\\miniconda3\\envs\\RNA\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (32x10609 and 2x64)"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    lossTotal = 0\n",
    "    dataloader = create_minibatches(X_train, Y_train, batch_size=batch_size)\n",
    "    for X_tr, y_tr in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_tr)\n",
    "        loss = criterion(y_pred, y_tr)\n",
    "        lossTotal += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Época {epoch+1}/{epochs}, Pérdida: {lossTotal/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones binarias:\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        ...,\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Evaluación / predicción\n",
    "# -----------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test)\n",
    "\n",
    "y_pred_final = torch.where(y_pred_test >= 0.5, 1, 0)\n",
    "\n",
    "print(\"Predicciones binarias:\")\n",
    "print(y_pred_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[189 107]\n",
      " [ 83 648]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.6949    0.6385    0.6655       296\n",
      "         1.0     0.8583    0.8865    0.8721       731\n",
      "\n",
      "    accuracy                         0.8150      1027\n",
      "   macro avg     0.7766    0.7625    0.7688      1027\n",
      "weighted avg     0.8112    0.8150    0.8126      1027\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Evaluar el modelo con las predicciones obtenidas y las etiquetas esperadas: \n",
    "# classification_report y  matriz de confusión (métricas Precisión, Recall, F1-measaure, Accuracy)\n",
    "# Convertir tensores PyTorch a numpy para usar con sklearn\n",
    "y_true = Y_test.numpy()\n",
    "y_pred_test = y_pred_final.numpy()\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(Y_test, y_pred_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(Y_test, y_pred_test))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, y_pred_test, digits=4, zero_division='warn'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aggressive', 'nonaggressive'], dtype=object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klass\n",
      "nonaggressive    3655\n",
      "aggressive       1477\n",
      "Name: count, dtype: int64\n",
      "Tamaño del entrenamiento: (4105,)\n",
      "Tamaño del test: (1027,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\edwar\\miniconda3\\envs\\RNA\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones TF-IDF entrenamiento: (4105, 10609)\n",
      "Dimensiones TF-IDF prueba: (1027, 10609)\n",
      "Época 1/100, Pérdida: 0.20580342053905015\n",
      "Época 2/100, Pérdida: 0.20484161989171376\n",
      "Época 3/100, Pérdida: 0.20465088808952375\n",
      "Época 4/100, Pérdida: 0.2049386042148568\n",
      "Época 5/100, Pérdida: 0.20435842825460804\n",
      "Época 6/100, Pérdida: 0.20462141422800315\n",
      "Época 7/100, Pérdida: 0.20371243455844332\n",
      "Época 8/100, Pérdida: 0.2040328115455864\n",
      "Época 9/100, Pérdida: 0.20353337313777717\n",
      "Época 10/100, Pérdida: 0.20391448003839152\n",
      "Época 11/100, Pérdida: 0.203110803236333\n",
      "Época 12/100, Pérdida: 0.2026834396659866\n",
      "Época 13/100, Pérdida: 0.20232452856477842\n",
      "Época 14/100, Pérdida: 0.20230802853209104\n",
      "Época 15/100, Pérdida: 0.20206304500962413\n",
      "Época 16/100, Pérdida: 0.20245230151701343\n",
      "Época 17/100, Pérdida: 0.2022572436998057\n",
      "Época 18/100, Pérdida: 0.2006153842044431\n",
      "Época 19/100, Pérdida: 0.20124588773925176\n",
      "Época 20/100, Pérdida: 0.2007592695281487\n",
      "Época 21/100, Pérdida: 0.19980791436378345\n",
      "Época 22/100, Pérdida: 0.19918318058169165\n",
      "Época 23/100, Pérdida: 0.19874439769706062\n",
      "Época 24/100, Pérdida: 0.19848224971183512\n",
      "Época 25/100, Pérdida: 0.19753975646440372\n",
      "Época 26/100, Pérdida: 0.19710967337438304\n",
      "Época 27/100, Pérdida: 0.1971424976869147\n",
      "Época 28/100, Pérdida: 0.19576969806307046\n",
      "Época 29/100, Pérdida: 0.19518767430107722\n",
      "Época 30/100, Pérdida: 0.19375094268904175\n",
      "Época 31/100, Pérdida: 0.19394241775884186\n",
      "Época 32/100, Pérdida: 0.19225910975951557\n",
      "Época 33/100, Pérdida: 0.191670141132303\n",
      "Época 34/100, Pérdida: 0.19094868916873783\n",
      "Época 35/100, Pérdida: 0.18918481376744056\n",
      "Época 36/100, Pérdida: 0.18841576593559842\n",
      "Época 37/100, Pérdida: 0.1873427293905916\n",
      "Época 38/100, Pérdida: 0.18596786168194557\n",
      "Época 39/100, Pérdida: 0.18481623820325202\n",
      "Época 40/100, Pérdida: 0.18304293596929358\n",
      "Época 41/100, Pérdida: 0.1816081003278725\n",
      "Época 42/100, Pérdida: 0.18054936173581337\n",
      "Época 43/100, Pérdida: 0.17905184707438299\n",
      "Época 44/100, Pérdida: 0.17766951694507008\n",
      "Época 45/100, Pérdida: 0.1756335934118707\n",
      "Época 46/100, Pérdida: 0.17427633286908614\n",
      "Época 47/100, Pérdida: 0.17248882971299712\n",
      "Época 48/100, Pérdida: 0.17082508422376574\n",
      "Época 49/100, Pérdida: 0.16971664084482563\n",
      "Época 50/100, Pérdida: 0.16689403383999832\n",
      "Época 51/100, Pérdida: 0.16604072063468223\n",
      "Época 52/100, Pérdida: 0.1636827716300654\n",
      "Época 53/100, Pérdida: 0.16185381430988163\n",
      "Época 54/100, Pérdida: 0.1599282673740572\n",
      "Época 55/100, Pérdida: 0.1577091263823731\n",
      "Época 56/100, Pérdida: 0.15648255462563315\n",
      "Época 57/100, Pérdida: 0.15451800961827122\n",
      "Época 58/100, Pérdida: 0.15162286116171253\n",
      "Época 59/100, Pérdida: 0.15021758312864822\n",
      "Época 60/100, Pérdida: 0.1480689799023229\n",
      "Época 61/100, Pérdida: 0.14567039340965507\n",
      "Época 62/100, Pérdida: 0.14452738002982252\n",
      "Época 63/100, Pérdida: 0.1426909288694692\n",
      "Época 64/100, Pérdida: 0.14003255489722702\n",
      "Época 65/100, Pérdida: 0.13831754687220552\n",
      "Época 66/100, Pérdida: 0.13673390979452651\n",
      "Época 67/100, Pérdida: 0.13463357241116752\n",
      "Época 68/100, Pérdida: 0.13299422578294148\n",
      "Época 69/100, Pérdida: 0.13112164918304414\n",
      "Época 70/100, Pérdida: 0.12898516481698946\n",
      "Época 71/100, Pérdida: 0.12760399351286333\n",
      "Época 72/100, Pérdida: 0.12578186387008475\n",
      "Época 73/100, Pérdida: 0.12406709050947382\n",
      "Época 74/100, Pérdida: 0.12206912121569463\n",
      "Época 75/100, Pérdida: 0.12078759749961454\n",
      "Época 76/100, Pérdida: 0.11930438263933787\n",
      "Época 77/100, Pérdida: 0.11785633255575978\n",
      "Época 78/100, Pérdida: 0.1157581327093202\n",
      "Época 79/100, Pérdida: 0.1146747552493746\n",
      "Época 80/100, Pérdida: 0.11288657022076984\n",
      "Época 81/100, Pérdida: 0.11135395957055942\n",
      "Época 82/100, Pérdida: 0.11046273593408193\n",
      "Época 83/100, Pérdida: 0.10915485076433004\n",
      "Época 84/100, Pérdida: 0.10736820904552474\n",
      "Época 85/100, Pérdida: 0.1059115313340065\n",
      "Época 86/100, Pérdida: 0.10477484349710073\n",
      "Época 87/100, Pérdida: 0.10344075743657674\n",
      "Época 88/100, Pérdida: 0.10203887675275174\n",
      "Época 89/100, Pérdida: 0.10100455180734627\n",
      "Época 90/100, Pérdida: 0.09956840529691341\n",
      "Época 91/100, Pérdida: 0.09815370294359303\n",
      "Época 92/100, Pérdida: 0.09677330941654914\n",
      "Época 93/100, Pérdida: 0.09623130922054135\n",
      "Época 94/100, Pérdida: 0.09453383342240208\n",
      "Época 95/100, Pérdida: 0.09350681972018508\n",
      "Época 96/100, Pérdida: 0.09197906488480494\n",
      "Época 97/100, Pérdida: 0.09052550024533457\n",
      "Época 98/100, Pérdida: 0.08974370288178903\n",
      "Época 99/100, Pérdida: 0.08852550346953\n",
      "Época 100/100, Pérdida: 0.08678180839895278\n",
      "Predicciones binarias:\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        ...,\n",
      "        [1],\n",
      "        [1],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# -----------------------------\n",
    "# 🔹 Semillas para reproducibilidad\n",
    "# -----------------------------\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# -----------------------------\n",
    "# 🔹 Cargar dataset\n",
    "# -----------------------------\n",
    "dataset = pd.read_json(\"./data_aggressiveness_es.json\", lines=True)\n",
    "print(dataset.klass.value_counts())\n",
    "\n",
    "X = dataset['text'].to_numpy()\n",
    "Y = dataset['klass'].to_numpy()\n",
    "\n",
    "# -----------------------------\n",
    "# 🔹 Codificar etiquetas a números\n",
    "# -----------------------------\n",
    "le = LabelEncoder()\n",
    "Y_encoded = le.fit_transform(Y)\n",
    "Y_encoded = Y_encoded[:, np.newaxis]  # agregar dimensión extra para PyTorch\n",
    "\n",
    "# -----------------------------\n",
    "# 🔹 Dividir datos en entrenamiento y prueba\n",
    "# -----------------------------\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y_encoded, test_size=0.2, stratify=Y_encoded, random_state=42\n",
    ")\n",
    "print(\"Tamaño del entrenamiento:\", X_train.shape)\n",
    "print(\"Tamaño del test:\", X_test.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# 🔹 Vectorización TF-IDF\n",
    "# Importante: entrenar solo con X_train\n",
    "# -----------------------------\n",
    "vec_tfidf = TfidfVectorizer(\n",
    "    analyzer=\"word\", preprocessor=mi_preprocesamiento, tokenizer=mi_tokenizador, ngram_range=(1,1)\n",
    ")\n",
    "\n",
    "# 🔹 Entrenar vectorizador con el conjunto de entrenamiento\n",
    "X_train_tfidf = vec_tfidf.fit_transform(X_train)\n",
    "\n",
    "# 🔹 Transformar el conjunto de prueba usando el mismo vocabulario\n",
    "X_test_tfidf = vec_tfidf.transform(X_test)\n",
    "\n",
    "print(\"Dimensiones TF-IDF entrenamiento:\", X_train_tfidf.shape)\n",
    "print(\"Dimensiones TF-IDF prueba:\", X_test_tfidf.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# 🔹 Crear tensores PyTorch\n",
    "# -----------------------------\n",
    "X_train = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor(X_test_tfidf.toarray(), dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test, dtype=torch.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# 🔹 Función para crear minibatches\n",
    "# -----------------------------\n",
    "def create_minibatches(X, Y, batch_size):\n",
    "    dataset = TensorDataset(X, Y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "# -----------------------------\n",
    "# 🔹 Definir la red neuronal\n",
    "# -----------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        if self.fc1.bias is not None:\n",
    "            nn.init.zeros_(self.fc1.bias)\n",
    "        if self.fc2.bias is not None:\n",
    "            nn.init.zeros_(self.fc2.bias) \n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.fc1(X)\n",
    "        out = self.sigmoid(out)  # capa oculta\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)  # capa de salida\n",
    "        return out\n",
    "\n",
    "# -----------------------------\n",
    "# 🔹 Parámetros de la red\n",
    "# -----------------------------\n",
    "input_size = X_train.shape[1]  # tamaño de la entrada = número de características TF-IDF\n",
    "hidden_size = 10               # puedes ajustar\n",
    "output_size = 1\n",
    "epochs = 100\n",
    "learning_rate = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "# -----------------------------\n",
    "# 🔹 Crear modelo, criterio y optimizador\n",
    "# -----------------------------\n",
    "model = MLP(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# -----------------------------\n",
    "# 🔹 Entrenamiento\n",
    "# -----------------------------\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    lossTotal = 0\n",
    "    dataloader = create_minibatches(X_train, Y_train, batch_size=batch_size)\n",
    "    for X_tr, y_tr in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_tr)\n",
    "        loss = criterion(y_pred, y_tr)\n",
    "        lossTotal += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Época {epoch+1}/{epochs}, Pérdida: {lossTotal/len(dataloader)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 🔹 Evaluación / predicción\n",
    "# -----------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test)\n",
    "\n",
    "y_pred_final = torch.where(y_pred_test >= 0.5, 1, 0)\n",
    "\n",
    "print(\"Predicciones binarias:\")\n",
    "print(y_pred_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[137 159]\n",
      " [ 24 707]]\n",
      "Accuracy: 0.8218\n",
      "Reporte de clasificación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   aggressive       0.85      0.46      0.60       296\n",
      "nonaggressive       0.82      0.97      0.89       731\n",
      "\n",
      "     accuracy                           0.82      1027\n",
      "    macro avg       0.83      0.72      0.74      1027\n",
      " weighted avg       0.83      0.82      0.80      1027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Convertir tensores PyTorch a numpy para usar con sklearn\n",
    "y_true = Y_test.numpy()\n",
    "y_pred = y_pred_final.numpy()\n",
    "\n",
    "# 🔹 Matriz de confusión\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Matriz de confusión:\")\n",
    "print(cm)\n",
    "\n",
    "# 🔹 Accuracy\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "# 🔹 Reporte de clasificación\n",
    "report = classification_report(y_true, y_pred, target_names=le.classes_)\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
