{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa4118f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PRUEBA DE PREPROCESAMIENTO\n",
      "======================================================================\n",
      "Texto original: ¡¡¡ESTE es un TEXTO de ejemplo!!! con números 123 y acentos: áéíóú\n",
      "----------------------------------------------------------------------\n",
      "only_normalizar_texto:\n",
      "  → este es un texto de ejemplo con numeros y acentos aeiou\n",
      "\n",
      "normalizar_txt_sin_StopWords:\n",
      "  → texto ejemplo numeros acentos aeiou\n",
      "\n",
      "normalizar_txt_sin_StopWords_mas_stemming:\n",
      "  → text ejempl numer acent aeiou\n",
      "\n",
      "======================================================================\n",
      "EJEMPLOS DE VECTORIZADORES\n",
      "======================================================================\n",
      "✓ CountVectorizer creado: normalización básica + unigramas\n",
      "✓ TfidfVectorizer creado: sin stopwords + uni+bigramas\n",
      "✓ CountVectorizer creado: con stemming + unigramas\n",
      "\n",
      "======================================================================\n",
      "¡CONFIGURACIÓN LISTA PARA EXPERIMENTOS!\n",
      "======================================================================\n",
      "\n",
      "Puedes usar:\n",
      "  preprocessor=mi_preprocesamiento('tipo_procesamiento')\n",
      "\n",
      "Tipos disponibles:\n",
      "  - only_normalizar_texto\n",
      "  - normalizar_txt_sin_StopWords\n",
      "  - normalizar_txt_sin_StopWords_mas_stemming\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURACIÓN COMPLETA DE PREPROCESAMIENTO PARA EXPERIMENTOS\n",
    "# =============================================================================\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Descargar recursos de NLTK (ejecutar solo una vez si es necesario)\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Inicializar stemmer y stopwords\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "stop_words_es = set(stopwords.words('spanish'))\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "# Constantes para normalización\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "NUMBERS = \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def only_normalizar_texto(input_str, punct=False, accents=False, num=False, max_dup=2, lowercase=True):\n",
    "    \"\"\"\n",
    "    Normalización avanzada de texto con control granular de parámetros\n",
    "    \n",
    "    Args:\n",
    "        input_str: Texto a normalizar\n",
    "        punct: False elimina puntuación, True la mantiene\n",
    "        accents: False elimina acentos, True los mantiene  \n",
    "        num: False elimina números, True los mantiene\n",
    "        max_dup: Número máximo de caracteres duplicados consecutivos\n",
    "        lowercase: True convierte a minúsculas\n",
    "    \n",
    "    Returns:\n",
    "        Texto normalizado\n",
    "    \"\"\"\n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    \n",
    "    if lowercase:\n",
    "        texto = texto.lower()\n",
    "    \n",
    "    return texto\n",
    "\n",
    "def normalizar_txt_sin_StopWords(texto):\n",
    "    \"\"\"\n",
    "    Normaliza el texto (eliminando puntuación, acentos y números) y elimina stopwords\n",
    "    \"\"\"\n",
    "    # Usar función de normalización con configuración estándar para NLP\n",
    "    texto_normalizado = only_normalizar_texto(\n",
    "        texto, \n",
    "        punct=False,    # Eliminar puntuación\n",
    "        accents=False,  # Eliminar acentos  \n",
    "        num=False,      # Eliminar números\n",
    "        max_dup=2,      # Máximo 2 duplicados\n",
    "        lowercase=True  # Convertir a minúsculas\n",
    "    )\n",
    "    \n",
    "    if not texto_normalizado:\n",
    "        return \"\"\n",
    "    \n",
    "    # Tokenizar y eliminar stopwords (español e inglés)\n",
    "    palabras = texto_normalizado.split()\n",
    "    palabras_filtradas = [palabra for palabra in palabras \n",
    "                         if palabra not in stop_words_es and palabra not in stop_words_en]\n",
    "    \n",
    "    # Reconstruir el texto\n",
    "    return ' '.join(palabras_filtradas)\n",
    "\n",
    "def normalizar_txt_sin_StopWords_mas_stemming(texto):\n",
    "    \"\"\"\n",
    "    Normaliza, elimina stopwords y aplica stemming\n",
    "    \"\"\"\n",
    "    # Primero normalizar y eliminar stopwords\n",
    "    texto_sin_stopwords = normalizar_txt_sin_StopWords(texto)\n",
    "    \n",
    "    if not texto_sin_stopwords:\n",
    "        return \"\"\n",
    "    \n",
    "    # Aplicar stemming a cada palabra\n",
    "    palabras = texto_sin_stopwords.split()\n",
    "    palabras_stemmed = [stemmer.stem(palabra) for palabra in palabras]\n",
    "    \n",
    "    # Reconstruir el texto\n",
    "    return ' '.join(palabras_stemmed)\n",
    "\n",
    "def mi_preprocesamiento(tipo_procesamiento):\n",
    "    \"\"\"\n",
    "    Función dispatcher que retorna la función de preprocesamiento especificada\n",
    "    \n",
    "    Args:\n",
    "        tipo_procesamiento: \n",
    "            'only_normalizar_texto' - Solo normalización básica\n",
    "            'normalizar_txt_sin_StopWords' - Normalización + sin stopwords  \n",
    "            'normalizar_txt_sin_StopWords_mas_stemming' - Normalización + sin stopwords + stemming\n",
    "    \n",
    "    Returns:\n",
    "        Función de preprocesamiento correspondiente\n",
    "    \"\"\"\n",
    "    procesadores = {\n",
    "        'only_normalizar_texto': lambda x: only_normalizar_texto(\n",
    "            x, punct=False, accents=False, num=False, max_dup=2, lowercase=True\n",
    "        ),\n",
    "        'normalizar_txt_sin_StopWords': normalizar_txt_sin_StopWords,\n",
    "        'normalizar_txt_sin_StopWords_mas_stemming': normalizar_txt_sin_StopWords_mas_stemming\n",
    "    }\n",
    "    \n",
    "    if tipo_procesamiento not in procesadores:\n",
    "        raise ValueError(f\"Tipo de procesamiento no válido. Opciones: {list(procesadores.keys())}\")\n",
    "    \n",
    "    return procesadores[tipo_procesamiento]\n",
    "\n",
    "# =============================================================================\n",
    "# EJEMPLOS DE USO Y PRUEBAS\n",
    "# =============================================================================\n",
    "\n",
    "def probar_preprocesamiento():\n",
    "    \"\"\"Función para probar todos los tipos de preprocesamiento\"\"\"\n",
    "    \n",
    "    texto_ejemplo = \"¡¡¡ESTE es un TEXTO de ejemplo!!! con números 123 y acentos: áéíóú\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"PRUEBA DE PREPROCESAMIENTO\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Texto original: {texto_ejemplo}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Probar cada tipo de procesamiento\n",
    "    tipos = [\n",
    "        'only_normalizar_texto',\n",
    "        'normalizar_txt_sin_StopWords', \n",
    "        'normalizar_txt_sin_StopWords_mas_stemming'\n",
    "    ]\n",
    "    \n",
    "    for tipo in tipos:\n",
    "        resultado = mi_preprocesamiento(tipo)(texto_ejemplo)\n",
    "        print(f\"{tipo}:\")\n",
    "        print(f\"  → {resultado}\")\n",
    "        print()\n",
    "\n",
    "def crear_vectorizadores_ejemplo():\n",
    "    \"\"\"Ejemplo de cómo crear vectorizadores con diferentes configuraciones\"\"\"\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"EJEMPLOS DE VECTORIZADORES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Configuración 1: CountVectorizer con normalización básica y unigramas\n",
    "    vec1 = CountVectorizer(\n",
    "        analyzer=\"word\",\n",
    "        preprocessor=mi_preprocesamiento('only_normalizar_texto'),\n",
    "        ngram_range=(1,1)\n",
    "    ) \n",
    "    print(\"✓ CountVectorizer creado: normalización básica + unigramas\")\n",
    "    \n",
    "    # Configuración 2: TF-IDF sin stopwords y unigramas+bigramas\n",
    "    vec2 = TfidfVectorizer(\n",
    "        analyzer=\"word\",\n",
    "        preprocessor=mi_preprocesamiento('normalizar_txt_sin_StopWords'),\n",
    "        ngram_range=(1,2)\n",
    "    )\n",
    "    print(\"✓ TfidfVectorizer creado: sin stopwords + uni+bigramas\")\n",
    "    \n",
    "    # Configuración 3: CountVectorizer con stemming\n",
    "    vec3 = CountVectorizer(\n",
    "        analyzer=\"word\", \n",
    "        preprocessor=mi_preprocesamiento('normalizar_txt_sin_StopWords_mas_stemming'),\n",
    "        ngram_range=(1,1)\n",
    "    )\n",
    "    print(\"✓ CountVectorizer creado: con stemming + unigramas\")\n",
    "    \n",
    "    return vec1, vec2, vec3\n",
    "\n",
    "# =============================================================================\n",
    "# EJECUCIÓN DE PRUEBAS\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Probar el preprocesamiento\n",
    "    probar_preprocesamiento()\n",
    "    \n",
    "    # Crear ejemplos de vectorizadores\n",
    "    vec1, vec2, vec3 = crear_vectorizadores_ejemplo()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"¡CONFIGURACIÓN LISTA PARA EXPERIMENTOS!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nPuedes usar:\")\n",
    "    print(\"  preprocessor=mi_preprocesamiento('tipo_procesamiento')\")\n",
    "    print(\"\\nTipos disponibles:\")\n",
    "    print(\"  - only_normalizar_texto\")\n",
    "    print(\"  - normalizar_txt_sin_StopWords\") \n",
    "    print(\"  - normalizar_txt_sin_StopWords_mas_stemming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1416b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo directo en tu código\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorizador con normalización básica\n",
    "vec_basico = TfidfVectorizer(\n",
    "    preprocessor=mi_preprocesamiento('only_normalizar_texto'),\n",
    "    ngram_range=(1,1)\n",
    ")\n",
    "\n",
    "# Vectorizador sin stopwords  \n",
    "vec_sin_stopwords = TfidfVectorizer(\n",
    "    preprocessor=mi_preprocesamiento('normalizar_txt_sin_StopWords'),\n",
    "    ngram_range=(1,2)\n",
    ")\n",
    "\n",
    "# Vectorizador con stemming\n",
    "vec_stemming = TfidfVectorizer(\n",
    "    preprocessor=mi_preprocesamiento('normalizar_txt_sin_StopWords_mas_stemming'),\n",
    "    ngram_range=(1,1)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
