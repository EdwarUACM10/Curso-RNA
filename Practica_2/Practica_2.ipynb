{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PASO 1 ‚Äî Leer los .json y preparar los tweet\n",
    "import json\n",
    "\n",
    "def load_hateval_json(file_path):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                texts.append(data[\"text\"])\n",
    "                labels.append(int(data[\"klass\"]))\n",
    "            except Exception as e:\n",
    "                print(f\"Error leyendo l√≠nea: {e}\")\n",
    "    \n",
    "    return texts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0326a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paso 2 ‚Äî Cargar los datos de entrenamiento y prueba\n",
    "X_train_es, y_train_es = load_hateval_json(\"hateval_es_train_spanish.json\")\n",
    "X_test_es, y_test_es = load_hateval_json(\"hateval_es_test_spanish.json\")\n",
    "\n",
    "X_train_in, y_train_in = load_hateval_json(\"hateval_en_train_ingles.json\")\n",
    "X_test_in, y_test_in = load_hateval_json(\"hateval_en_test_ingles.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ddc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES train: 4500\n",
      "ES test: 500\n",
      "EN train: 9000\n",
      "EN test: 1000\n"
     ]
    }
   ],
   "source": [
    "#Comprobar las cargas\n",
    "print(\"ES train:\", len(X_train_es))\n",
    "print(\"ES test:\", len(X_test_es))\n",
    "print(\"EN train:\", len(X_train_in))\n",
    "print(\"EN test:\", len(X_test_in))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24a2710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#librer√≠as para preprocesamiento de texto  y tokenizaci√≥n\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab3af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar recursos necesarios UNA SOLA VEZ\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdd98a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PASO 3 ‚Äî Preprocesamiento de texto y tokenizaci√≥n \n",
    "\n",
    "# Configurar herramientas por idioma\n",
    "stopwords_es = set(stopwords.words('spanish'))\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "stemmer_es = SnowballStemmer('spanish')\n",
    "stemmer_en = SnowballStemmer('english')\n",
    "\n",
    "# Funciones de preprocesamiento\n",
    "def limpiar_texto(texto):\n",
    "    \"\"\"\n",
    "    Limpia el texto antes de tokenizar:\n",
    "    - Convierte a min√∫sculas\n",
    "    - Elimina URLs, menciones, hashtags, emojis y caracteres raros\n",
    "    \"\"\"\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", texto)  # URLs\n",
    "    texto = re.sub(r\"@\\w+\", \"\", texto)  # menciones\n",
    "    texto = re.sub(r\"#\", \"\", texto)  # hashtags\n",
    "    texto = re.sub(r\"[^a-z√°√©√≠√≥√∫√±√º\\s]\", \"\", texto)  # solo letras\n",
    "    return texto\n",
    "\n",
    "# Funci√≥n para preprocesar texto en espa√±ol\n",
    "def preprocesar_espaniol(texto):\n",
    "    texto = limpiar_texto(texto)\n",
    "    tokens = texto.split()\n",
    "    tokens = [t for t in tokens if t not in stopwords_es]  # eliminar stopwords\n",
    "    tokens = [stemmer_es.stem(t) for t in tokens]  # aplicar stemming\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Funci√≥n para preprocesar texto en ingl√©s\n",
    "def preprocesar_ingles(texto):\n",
    "    texto = limpiar_texto(texto)\n",
    "    tokens = texto.split()\n",
    "    tokens = [t for t in tokens if t not in stopwords_en]  # eliminar stopwords\n",
    "    tokens = [stemmer_en.stem(t) for t in tokens]  # aplicar stemming\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee6de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar los conjuntos de datos\n",
    "X_entrenamiento_es = [preprocesar_espaniol(t) for t in X_train_es]\n",
    "X_prueba_es = [preprocesar_espaniol(t) for t in X_test_es]\n",
    "\n",
    "X_entrenamiento_en = [preprocesar_ingles(t) for t in X_train_in]\n",
    "X_prueba_en = [preprocesar_ingles(t) for t in X_test_in]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d229fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PASO 4 ‚Äî Vectorizaci√≥n de texto dependiendo del m√©todo elegido\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def vectorizar_textos(textos_entrenamiento, textos_prueba, metodo='tfidf', ngramas=(1,1)):\n",
    "    \"\"\"\n",
    "    Convierte los textos en vectores num√©ricos.\n",
    "    \n",
    "    Par√°metros:\n",
    "    textos_entrenamiento: lista de strings de entrenamiento\n",
    "    textos_prueba: lista de strings de prueba\n",
    "    metodo: 'tf' para frecuencia de t√©rminos, 'tfidf' para TF-IDF\n",
    "    ngramas: tupla para definir ngramas, ej. (1,1) unigramas, (1,2) uni+bi\n",
    "    \n",
    "    Retorna:\n",
    "    X_entrenamiento_vect: matriz de entrenamiento\n",
    "    X_prueba_vect: matriz de prueba\n",
    "    vectorizador: objeto vectorizador para transformar nuevos datos si es necesario\n",
    "    \"\"\"\n",
    "    if metodo == 'tf':\n",
    "        vectorizador = CountVectorizer(ngram_range=ngramas)\n",
    "    elif metodo == 'tfidf':\n",
    "        vectorizador = TfidfVectorizer(ngram_range=ngramas)\n",
    "    else:\n",
    "        raise ValueError(\"M√©todo debe ser 'tf' o 'tfidf'\")\n",
    "    \n",
    "    # Ajustar el vectorizador solo con los datos de entrenamiento\n",
    "    X_entrenamiento_vect = vectorizador.fit_transform(textos_entrenamiento)\n",
    "    X_prueba_vect = vectorizador.transform(textos_prueba)\n",
    "    \n",
    "    return X_entrenamiento_vect.toarray(), X_prueba_vect.toarray(), vectorizador\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca34a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de vectorizaci√≥n para espa√±ol usando TF-IDF y unigramas\n",
    "X_entrenamiento_es_vect, X_prueba_es_vect, vectorizador_es = vectorizar_textos(\n",
    "    X_entrenamiento_es, X_prueba_es, metodo='tfidf', ngramas=(1,1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65d772ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de vectorizaci√≥n para ingl√©s usando TF-IDF y unigramas\n",
    "X_entrenamiento_en_vect, X_prueba_en_vect, vectorizador_en = vectorizar_textos(\n",
    "    X_entrenamiento_en, X_prueba_en, metodo='tfidf', ngramas=(1,1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f78aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLASE MLP_TODO.PY Y FUNCIONES AUXILIARES\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "# ====================================================\n",
    "# Funciones para activaci√≥n y  su derivada\n",
    "# ====================================================\n",
    "\n",
    "# Funci√≥n de activaci√≥n sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivada de la sigmoide\n",
    "def sigmoid_derivative(x):\n",
    "    # return sigmoid(x) * (1 - sigmoid(x))\n",
    "    return x * (1 - x)\n",
    "\n",
    "# ====================================================\n",
    "# Funciones para manejo de la semilla\n",
    "# ====================================================\n",
    "\n",
    "# Establece la semilla para la generaci√≥n de n√∫meros aleatorios\n",
    "def seed(random_state=33):\n",
    "    np.random.seed(random_state)\n",
    "    random.seed(random_state)\n",
    "\n",
    "# ====================================================\n",
    "# Funciones para inicializaci√≥n y normalizaci√≥n\n",
    "# ====================================================\n",
    "\n",
    "# Inicializaci√≥n Xavier\n",
    "def xavier_initialization(input_size, output_size): \n",
    "    return np.random.randn(input_size, output_size) * np.sqrt(1 / input_size)\n",
    "# Inicializaci√≥n normal\n",
    "def normal_initialization(input_size, output_size):\n",
    "    return np.random.randn(input_size, output_size)\n",
    "# Normalizaci√≥n Z-score\n",
    "def zscore_normalization(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X_norm = (X - mean) / std\n",
    "    return X_norm\n",
    "\n",
    "#Funci√≥n para crear minibatches\n",
    "def create_minibatches(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    Genera los lotes de datos (batchs) de acuerdo al par√°metro batch_size de forma aleatoria para el procesamiento. \n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.random.permutation(n_samples)  # Mezcla los √≠ndices aleatoriamente\n",
    "    X_shuffled, y_shuffled = X[indices], y[indices]  # Reordena X e y seg√∫n los √≠ndices aleatorios\n",
    "    \n",
    "    # Divide los datos en minibatches\n",
    "    for X_batch, y_batch in zip(np.array_split(X_shuffled, np.ceil(n_samples / batch_size)), \n",
    "                                np.array_split(y_shuffled, np.ceil(n_samples / batch_size))):\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "    \n",
    "class MLP_TODO:\n",
    "    def __init__(self, num_entradas, num_neuronas_ocultas, num_salidas, epochs, batch_size=128, learning_rate=0.2, normalizacion=\"none\",inicializacion=\"xavier\",random_state=42):\n",
    "\n",
    "        # ====================================================\n",
    "        # Inicializaci√≥n general del modelo\n",
    "        # ====================================================\n",
    "\n",
    "        # üîπ NUEVO: Usar el par√°metro random_state recibido para controlar la semilla\n",
    "        seed(33)\n",
    "        self.random_state = random_state  # üîπ NUEVO: guardar la semilla para usarla tambi√©n en create_minibatches\n",
    "\n",
    "        # Definir la tasa de aprendizaje\n",
    "        self.learning_rate = learning_rate\n",
    "        # Definir el n√∫mero de √©pocas\n",
    "        self.epochs = epochs\n",
    "        # Definir el tama√±o del batch de procesamiento\n",
    "        self.batch_size = batch_size\n",
    "        # Definir el tipo de normalizaci√≥n\n",
    "        self.normalizacion = normalizacion\n",
    "        # Definir el tipo de inicializaci√≥n\n",
    "        self.inicializacion = inicializacion\n",
    "        # definir las \n",
    "        self.num_neuronas_ocultas = num_neuronas_ocultas\n",
    "\n",
    "        # Inicializaci√≥n de pesos y bias\n",
    "        self.W1 = self.inicializar_pesos(num_entradas, self.num_neuronas_ocultas) # Pesos entre capa de entrada y capa oculta\n",
    "        self.b1 = np.zeros((1, self.num_neuronas_ocultas))   # Bias de la capa oculta\n",
    "        self.W2 = self.inicializar_pesos(self.num_neuronas_ocultas,num_salidas)  # Pesos entre capa oculta y capa de salida\n",
    "        self.b2 = np.zeros((1, num_salidas)) # Bias de la capa de salida\n",
    "\n",
    "        # Historial de errores\n",
    "        self.errores_history = []\n",
    "        # Historial de accuracy\n",
    "        self.accuracy_history = []\n",
    "\n",
    "    # ====================================================\n",
    "    # Funciones para forward, backward, update, predict y train\n",
    "    # ====================================================\n",
    "\n",
    "    def forward(self, X):\n",
    "        #implementar el forward pass\n",
    "        #----------------------------------------------\n",
    "        # 1. Propagaci√≥n hacia adelante (Forward pass)\n",
    "        #----------------------------------------------\n",
    "        # Calcular la suma ponderada Z (z_c1) para la capa oculta \n",
    "        self.X = X\n",
    "        self.z_c1 = X@self.W1 + self.b1\n",
    "        #Calcular la activaci√≥n de la capa oculta usando la funci√≥n sigmoide\n",
    "        self.a_c1 = sigmoid(self.z_c1)  # Activaci√≥n capa oculta\n",
    "        #Calcular la suma ponderada Z (z_c2)  para la capa de salida \n",
    "        self.z_c2  = self.a_c1 @ self.W2 + self.b2\n",
    "        #Calcular la activaci√≥n de la capa de salida usando la funci√≥n sigmoide\n",
    "        y_pred = sigmoid(self.z_c2)  # Activaci√≥n capa salida\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "    def loss_function_MSE(self, y_pred, y):\n",
    "        #----------------------------------------------\n",
    "        # 2. C√°lculo del error con MSE\n",
    "        #----------------------------------------------\n",
    "        #Calcular el error cuadr√°tico medio (MSE)\n",
    "        self.y_pred = y_pred\n",
    "        self.y = y\n",
    "        error = 0.5 * np.mean((y_pred - y) ** 2)\n",
    "        return error\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        #implementar el backward pass\n",
    "        # calcular los gradientes para la arquitectura de la figura anterior\n",
    "        #----------------------------------------------\n",
    "        # 3. Propagaci√≥n hacia atr√°s (Backward pass)\n",
    "        #----------------------------------------------\n",
    "        \n",
    "        #----------------------------------------------\n",
    "        # Gradiente de la salida\n",
    "        #----------------------------------------------\n",
    "        #Calcular la derivada del error con respecto a la salida y\n",
    "        dE_dy_pred = (self.y_pred - self.y)  # Derivada del error respecto a la predicci√≥n con  N ejemplos\n",
    "        #Calcular la derivada de la activaci√≥n de la salida con respecto a z_c2 \n",
    "        d_y_pred_d_zc2 = sigmoid_derivative(self.y_pred)\n",
    "        #Calcular delta de la capa de salida\n",
    "        delta_c2 = dE_dy_pred * d_y_pred_d_zc2  # (N, 1)\n",
    "\n",
    "        #----------------------------------------------\n",
    "        # Gradiente en la capa oculta\n",
    "        #----------------------------------------------\n",
    "        # calcular la derivada de las suma ponderada respecto a las activaciones de la capa 1\n",
    "        d_zc2_d_a_c1 = self.W2  \n",
    "        #Propagar el error hacia la capa oculta, calcular deltas de la capa 1\n",
    "        delta_c1 = delta_c2 @ d_zc2_d_a_c1.T * sigmoid_derivative(self.a_c1)  \n",
    "\n",
    "        #calcula el gradiente de la funci√≥n de error respecto a los pesos de la capa 2\n",
    "        self.dE_dW2 = self.a_c1.T @ delta_c2\n",
    "        self.dE_db2 = np.sum(delta_c2, axis=0, keepdims=True)\n",
    "        self.dE_dW1 = self.X.T @ delta_c1 \n",
    "        self.dE_db1 = np.sum(delta_c1, axis=0, keepdims=True) \n",
    "\n",
    "\n",
    "    def update(self):  # Ejecuci√≥n de la actualizaci√≥n de param√°metros\n",
    "        #implementar la actualizaci√≥n de los pesos y el bias\n",
    "        #----------------------------------------------\n",
    "        # Actualizaci√≥n de pesos de la capa de salida\n",
    "        #---------------------------------------------- \n",
    "        #Actualizar los pesos y bias de la capa de salida\n",
    "        self.W2 = self.W2 - self.dE_dW2 * self.learning_rate\n",
    "        self.b2 = self.b2 - self.dE_db2 * self.learning_rate\n",
    "        #----------------------------------------------\n",
    "        # Actuailzaci√≥n de pesos de la capa oculta\n",
    "        #----------------------------------------------\n",
    "        #calcula el gradiente de la funci√≥n de error respecto a los pesos de la capa 1\n",
    "        self.W1 = self.W1 - self.dE_dW1 * self.learning_rate\n",
    "        self.b1 = self.b1 - self.dE_db1 * self.learning_rate\n",
    "\n",
    "    def predict(self, X):  # Predecir la categor√≠a para datos nuevos\n",
    "        # TODO: implementar la predicci√≥n \n",
    "        y_pred = self.forward(X)\n",
    "        # Obtener la clase para el clasificador binario\n",
    "        y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "        return y_pred\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        #implementar el entrenamiento de la red\n",
    "            # üîπ Normalizar los datos seg√∫n el tipo configurado\n",
    "        X = self.normalize(X)\n",
    "        for epoch in range(self.epochs):\n",
    "            num_batch = 0\n",
    "            epoch_error  = 0\n",
    "            for X_batch, y_batch in create_minibatches(X, Y, self.batch_size):\n",
    "                y_pred = self.forward(X_batch)\n",
    "                error = self.loss_function_MSE(y_pred, y_batch)\n",
    "                epoch_error += error    \n",
    "                self.backward() # c√°lculo de los gradientes\n",
    "                self.update() # actualizaci√≥n de los pesos y bias\n",
    "                num_batch += 1\n",
    "                # Imprimir el error cada N √©pocas\n",
    "                if epoch % 100 == 0:\n",
    "                    print(f\"√âpoca {epoch}, Error batch {num_batch}: {error}\")\n",
    "            # Guardar el error promedio de la √©poca\n",
    "            self.errores_history.append(epoch_error/num_batch)\n",
    "            #Calcular Accuracy en todo el dataset\n",
    "            acc_epoch = self.accuracy(X, Y)\n",
    "            self.accuracy_history.append(acc_epoch)\n",
    "            # Imprimir el error y accuracy cada N √©pocas\n",
    "            if epoch % 100 == 0:\n",
    "                    print(f\"√âpoca {epoch}, Error: {epoch_error/num_batch}%\")\n",
    "\n",
    "    \n",
    "    # ====================================================\n",
    "    # Funciones para inicializaci√≥n, normalizaci√≥n y accuracy\n",
    "    # ====================================================\n",
    "    # Normalizaci√≥n de los datos\n",
    "    def normalize(self, X):\n",
    "        if self.normalizacion == \"z-score\":\n",
    "            return zscore_normalization(X)  # üîπ Llamada a la funci√≥n existente\n",
    "        else:  # sin normalizar\n",
    "            return X\n",
    "        \n",
    "    # Inicializaci√≥n de los pesos  \n",
    "    def inicializar_pesos(self, tama√±o_entrada, tama√±o_salida):\n",
    "        if self.inicializacion == \"xavier\":\n",
    "            return xavier_initialization(tama√±o_entrada, tama√±o_salida)\n",
    "        elif self.inicializacion == \"normal\":\n",
    "            return normal_initialization(tama√±o_entrada, tama√±o_salida)\n",
    "        else:\n",
    "            raise ValueError(\"Tipo de inicializaci√≥n no soportado\")\n",
    "    # C√°lculo de accuracy    \n",
    "    def accuracy(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        acc = np.mean(y_pred == y)  # compara predicciones con valores reales\n",
    "        return acc\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05a4ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necesitamos reshappear los vectores etiquetas\n",
    "#Convertir tus labels (y_train, y_test) a formato compatible\n",
    "#MLP espera arrays de N√ó1, no listas de enteros.\n",
    "y_train_es = np.array(y_train_es).reshape(-1,1)\n",
    "y_test_es = np.array(y_test_es).reshape(-1,1)\n",
    "y_train_en = np.array(y_train_in).reshape(-1,1)\n",
    "y_test_en = np.array(y_test_in).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3bad989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√âpoca 0, Error batch 1: 0.12258864110527798\n",
      "√âpoca 0, Error batch 2: 0.12547158331657152\n",
      "√âpoca 0, Error batch 3: 0.11374285018796809\n",
      "√âpoca 0, Error batch 4: 0.13111889198647653\n",
      "√âpoca 0, Error batch 5: 0.17712875986431312\n",
      "√âpoca 0, Error batch 6: 0.17455272258982013\n",
      "√âpoca 0, Error batch 7: 0.12131263681900087\n",
      "√âpoca 0, Error batch 8: 0.11302445490276229\n",
      "√âpoca 0, Error batch 9: 0.12489577525597682\n",
      "√âpoca 0, Error batch 10: 0.1253095132423369\n",
      "√âpoca 0, Error batch 11: 0.11029598387435884\n",
      "√âpoca 0, Error batch 12: 0.1450444744084053\n",
      "√âpoca 0, Error batch 13: 0.2199631709167975\n",
      "√âpoca 0, Error batch 14: 0.15494323008077254\n",
      "√âpoca 0, Error batch 15: 0.16942218183773294\n",
      "√âpoca 0, Error batch 16: 0.17702404541087646\n",
      "√âpoca 0, Error batch 17: 0.1117343669127069\n",
      "√âpoca 0, Error batch 18: 0.1285659221714199\n",
      "√âpoca 0, Error batch 19: 0.12521872406926182\n",
      "√âpoca 0, Error batch 20: 0.13746868446204585\n",
      "√âpoca 0, Error batch 21: 0.1996431847243984\n",
      "√âpoca 0, Error batch 22: 0.12811375076531514\n",
      "√âpoca 0, Error batch 23: 0.11605695594766066\n",
      "√âpoca 0, Error batch 24: 0.12017099558122898\n",
      "√âpoca 0, Error batch 25: 0.11125383673839265\n",
      "√âpoca 0, Error batch 26: 0.12373638016720152\n",
      "√âpoca 0, Error batch 27: 0.12718798627619643\n",
      "√âpoca 0, Error batch 28: 0.18395272675010818\n",
      "√âpoca 0, Error batch 29: 0.1768974247968817\n",
      "√âpoca 0, Error batch 30: 0.1261452190006302\n",
      "√âpoca 0, Error batch 31: 0.11954500266184545\n",
      "√âpoca 0, Error batch 32: 0.12626550300248202\n",
      "√âpoca 0, Error batch 33: 0.1297044997298662\n",
      "√âpoca 0, Error batch 34: 0.12905937241867288\n",
      "√âpoca 0, Error batch 35: 0.14011450874839537\n",
      "√âpoca 0, Error batch 36: 0.235176876190767\n",
      "√âpoca 0, Error batch 37: 0.21049718794149921\n",
      "√âpoca 0, Error batch 38: 0.2197447414906183\n",
      "√âpoca 0, Error batch 39: 0.08532863994763402\n",
      "√âpoca 0, Error batch 40: 0.19956162188905024\n",
      "√âpoca 0, Error batch 41: 0.2517308403825594\n",
      "√âpoca 0, Error batch 42: 0.1439144242909892\n",
      "√âpoca 0, Error batch 43: 0.23745300781244152\n",
      "√âpoca 0, Error batch 44: 0.15544184747481615\n",
      "√âpoca 0, Error batch 45: 0.14476949729134356\n",
      "√âpoca 0, Error batch 46: 0.2050681767001796\n",
      "√âpoca 0, Error batch 47: 0.11583508711746356\n",
      "√âpoca 0, Error batch 48: 0.14009348617220846\n",
      "√âpoca 0, Error batch 49: 0.14443851405555194\n",
      "√âpoca 0, Error batch 50: 0.09839918223987887\n",
      "√âpoca 0, Error batch 51: 0.13260829903894694\n",
      "√âpoca 0, Error batch 52: 0.1512660938367526\n",
      "√âpoca 0, Error batch 53: 0.14506258287397159\n",
      "√âpoca 0, Error batch 54: 0.15894720259518996\n",
      "√âpoca 0, Error batch 55: 0.11229089387957006\n",
      "√âpoca 0, Error batch 56: 0.15501256335342395\n",
      "√âpoca 0, Error batch 57: 0.12070729728428117\n",
      "√âpoca 0, Error batch 58: 0.12530182109693194\n",
      "√âpoca 0, Error batch 59: 0.15308553895052168\n",
      "√âpoca 0, Error batch 60: 0.23887842566437595\n",
      "√âpoca 0, Error batch 61: 0.1850593196842762\n",
      "√âpoca 0, Error batch 62: 0.14134426034450587\n",
      "√âpoca 0, Error batch 63: 0.16746558303610204\n",
      "√âpoca 0, Error batch 64: 0.14725787456816486\n",
      "√âpoca 0, Error batch 65: 0.10120937415713918\n",
      "√âpoca 0, Error batch 66: 0.11922068797970192\n",
      "√âpoca 0, Error batch 67: 0.12324809415822935\n",
      "√âpoca 0, Error batch 68: 0.12395808990770862\n",
      "√âpoca 0, Error batch 69: 0.13256426238641608\n",
      "√âpoca 0, Error batch 70: 0.1254495472691296\n",
      "√âpoca 0, Error batch 71: 0.2376209723229646\n",
      "√âpoca 0, Error batch 72: 0.14752818710269086\n",
      "√âpoca 0, Error batch 73: 0.25007047873096205\n",
      "√âpoca 0, Error batch 74: 0.19340301941127488\n",
      "√âpoca 0, Error batch 75: 0.11734831917177124\n",
      "√âpoca 0, Error batch 76: 0.13749870826098462\n",
      "√âpoca 0, Error batch 77: 0.22810514812954608\n",
      "√âpoca 0, Error batch 78: 0.1900735021786964\n",
      "√âpoca 0, Error batch 79: 0.1398078369539996\n",
      "√âpoca 0, Error batch 80: 0.1549697386008766\n",
      "√âpoca 0, Error batch 81: 0.26034764694991697\n",
      "√âpoca 0, Error batch 82: 0.18270942339211851\n",
      "√âpoca 0, Error batch 83: 0.15120709843405586\n",
      "√âpoca 0, Error batch 84: 0.2247071292569947\n",
      "√âpoca 0, Error batch 85: 0.20473025969012063\n",
      "√âpoca 0, Error batch 86: 0.2068652621873539\n",
      "√âpoca 0, Error batch 87: 0.1745465850976568\n",
      "√âpoca 0, Error batch 88: 0.17541682255744398\n",
      "√âpoca 0, Error batch 89: 0.2039888402457687\n",
      "√âpoca 0, Error batch 90: 0.10306074443404843\n",
      "√âpoca 0, Error batch 91: 0.11267652387853809\n",
      "√âpoca 0, Error batch 92: 0.120787751673231\n",
      "√âpoca 0, Error batch 93: 0.12615673611746983\n",
      "√âpoca 0, Error batch 94: 0.14635845321340446\n",
      "√âpoca 0, Error batch 95: 0.1568526263587224\n",
      "√âpoca 0, Error batch 96: 0.12101641703698524\n",
      "√âpoca 0, Error batch 97: 0.1275906161497143\n",
      "√âpoca 0, Error batch 98: 0.13075066617997322\n",
      "√âpoca 0, Error batch 99: 0.1530305519190063\n",
      "√âpoca 0, Error batch 100: 0.2534755596193174\n",
      "√âpoca 0, Error batch 101: 0.13095120546596112\n",
      "√âpoca 0, Error batch 102: 0.14770424070086546\n",
      "√âpoca 0, Error batch 103: 0.1527886287215502\n",
      "√âpoca 0, Error batch 104: 0.2281232356641646\n",
      "√âpoca 0, Error batch 105: 0.10724848102086931\n",
      "√âpoca 0, Error batch 106: 0.11727889012634994\n",
      "√âpoca 0, Error batch 107: 0.12046767162146753\n",
      "√âpoca 0, Error batch 108: 0.12559249759857422\n",
      "√âpoca 0, Error batch 109: 0.13025978119571396\n",
      "√âpoca 0, Error batch 110: 0.1456618601887136\n",
      "√âpoca 0, Error batch 111: 0.1778827705032206\n",
      "√âpoca 0, Error batch 112: 0.18888524802161127\n",
      "√âpoca 0, Error batch 113: 0.15512329264867963\n",
      "√âpoca 0, Error batch 114: 0.15162015327161027\n",
      "√âpoca 0, Error batch 115: 0.1680587794418686\n",
      "√âpoca 0, Error batch 116: 0.14541649082922564\n",
      "√âpoca 0, Error batch 117: 0.1747369973917105\n",
      "√âpoca 0, Error batch 118: 0.11367167387912286\n",
      "√âpoca 0, Error batch 119: 0.10852801489470384\n",
      "√âpoca 0, Error batch 120: 0.12070429460534962\n",
      "√âpoca 0, Error batch 121: 0.12309880086343503\n",
      "√âpoca 0, Error batch 122: 0.1162427289922898\n",
      "√âpoca 0, Error batch 123: 0.12800584220838548\n",
      "√âpoca 0, Error batch 124: 0.13780573468897878\n",
      "√âpoca 0, Error batch 125: 0.1307541413163687\n",
      "√âpoca 0, Error batch 126: 0.12300845687022607\n",
      "√âpoca 0, Error batch 127: 0.14404779009790925\n",
      "√âpoca 0, Error batch 128: 0.12243143089819541\n",
      "√âpoca 0, Error batch 129: 0.12570902537686873\n",
      "√âpoca 0, Error batch 130: 0.1322441194339961\n",
      "√âpoca 0, Error batch 131: 0.130398096231564\n",
      "√âpoca 0, Error batch 132: 0.12435570060391704\n",
      "√âpoca 0, Error batch 133: 0.1246151749551465\n",
      "√âpoca 0, Error batch 134: 0.1394064544756537\n",
      "√âpoca 0, Error batch 135: 0.1885029638179506\n",
      "√âpoca 0, Error batch 136: 0.12230766744863981\n",
      "√âpoca 0, Error batch 137: 0.1260107790074112\n",
      "√âpoca 0, Error batch 138: 0.19472927959387482\n",
      "√âpoca 0, Error batch 139: 0.14167707379908204\n",
      "√âpoca 0, Error batch 140: 0.13676710802159103\n",
      "√âpoca 0, Error batch 141: 0.1771108224879515\n",
      "√âpoca 0, Error: 0.1506858150923608%\n",
      "√âpoca 100, Error batch 1: 0.0011097162781005985\n",
      "√âpoca 100, Error batch 2: 0.016442516088927793\n",
      "√âpoca 100, Error batch 3: 0.0017174076216403336\n",
      "√âpoca 100, Error batch 4: 0.0007917633335026699\n",
      "√âpoca 100, Error batch 5: 0.0005584589844514195\n",
      "√âpoca 100, Error batch 6: 0.016704782821225984\n",
      "√âpoca 100, Error batch 7: 0.0016908584928964324\n",
      "√âpoca 100, Error batch 8: 0.0005405495802734016\n",
      "√âpoca 100, Error batch 9: 0.015778798024096735\n",
      "√âpoca 100, Error batch 10: 0.000680994214983508\n",
      "√âpoca 100, Error batch 11: 0.0009681488710517043\n",
      "√âpoca 100, Error batch 12: 0.0006458516082084645\n",
      "√âpoca 100, Error batch 13: 0.0015557434596952001\n",
      "√âpoca 100, Error batch 14: 0.0031836436352855667\n",
      "√âpoca 100, Error batch 15: 0.016115545333363682\n",
      "√âpoca 100, Error batch 16: 0.0011451700623469104\n",
      "√âpoca 100, Error batch 17: 0.001443276459871164\n",
      "√âpoca 100, Error batch 18: 0.0005987699703756003\n",
      "√âpoca 100, Error batch 19: 0.001582138908287512\n",
      "√âpoca 100, Error batch 20: 0.0012856429521444202\n",
      "√âpoca 100, Error batch 21: 0.0019100800127505495\n",
      "√âpoca 100, Error batch 22: 0.001790398066157645\n",
      "√âpoca 100, Error batch 23: 0.0008566029279389539\n",
      "√âpoca 100, Error batch 24: 0.0004995093099217408\n",
      "√âpoca 100, Error batch 25: 0.0010156817847706025\n",
      "√âpoca 100, Error batch 26: 0.015940824546913188\n",
      "√âpoca 100, Error batch 27: 0.016331011411909267\n",
      "√âpoca 100, Error batch 28: 0.001123026427615518\n",
      "√âpoca 100, Error batch 29: 0.0008139377101529393\n",
      "√âpoca 100, Error batch 30: 0.0013997074049300277\n",
      "√âpoca 100, Error batch 31: 0.0008758103389145302\n",
      "√âpoca 100, Error batch 32: 0.0018187379089192612\n",
      "√âpoca 100, Error batch 33: 0.0008270474049627442\n",
      "√âpoca 100, Error batch 34: 0.0005549441675524488\n",
      "√âpoca 100, Error batch 35: 0.0008222767833532707\n",
      "√âpoca 100, Error batch 36: 0.000571829282027556\n",
      "√âpoca 100, Error batch 37: 0.0005364228862725768\n",
      "√âpoca 100, Error batch 38: 0.0008283026405131605\n",
      "√âpoca 100, Error batch 39: 0.03210397624196396\n",
      "√âpoca 100, Error batch 40: 0.001960377654981021\n",
      "√âpoca 100, Error batch 41: 0.0009536567339920399\n",
      "√âpoca 100, Error batch 42: 0.0014482566105878064\n",
      "√âpoca 100, Error batch 43: 0.0009316749367140742\n",
      "√âpoca 100, Error batch 44: 0.0019412084167497587\n",
      "√âpoca 100, Error batch 45: 0.0014626758411501954\n",
      "√âpoca 100, Error batch 46: 0.0008353943507178258\n",
      "√âpoca 100, Error batch 47: 0.01588402759434323\n",
      "√âpoca 100, Error batch 48: 0.016372750385532042\n",
      "√âpoca 100, Error batch 49: 0.0012429900334885585\n",
      "√âpoca 100, Error batch 50: 0.0006872646356969891\n",
      "√âpoca 100, Error batch 51: 0.01670151073282736\n",
      "√âpoca 100, Error batch 52: 0.016672940868432247\n",
      "√âpoca 100, Error batch 53: 0.0009333824563680104\n",
      "√âpoca 100, Error batch 54: 0.0013769006853187628\n",
      "√âpoca 100, Error batch 55: 0.000585168680972322\n",
      "√âpoca 100, Error batch 56: 0.03157944442544836\n",
      "√âpoca 100, Error batch 57: 0.0005686232594768016\n",
      "√âpoca 100, Error batch 58: 0.0007328480383513658\n",
      "√âpoca 100, Error batch 59: 0.000592916766162569\n",
      "√âpoca 100, Error batch 60: 0.016377671266688962\n",
      "√âpoca 100, Error batch 61: 0.00046593737144436597\n",
      "√âpoca 100, Error batch 62: 0.001858882657398208\n",
      "√âpoca 100, Error batch 63: 0.0008952191759140648\n",
      "√âpoca 100, Error batch 64: 0.0005642550014338654\n",
      "√âpoca 100, Error batch 65: 0.0016991976495600017\n",
      "√âpoca 100, Error batch 66: 0.0009654003251215442\n",
      "√âpoca 100, Error batch 67: 0.0003283407473159651\n",
      "√âpoca 100, Error batch 68: 0.016310800766110814\n",
      "√âpoca 100, Error batch 69: 0.0017112561428974497\n",
      "√âpoca 100, Error batch 70: 0.0007578997462841033\n",
      "√âpoca 100, Error batch 71: 0.0010326251610350053\n",
      "√âpoca 100, Error batch 72: 0.01602389498078097\n",
      "√âpoca 100, Error batch 73: 0.002436519276281858\n",
      "√âpoca 100, Error batch 74: 0.000797444072282336\n",
      "√âpoca 100, Error batch 75: 0.0007877575143552838\n",
      "√âpoca 100, Error batch 76: 0.0023053159186462186\n",
      "√âpoca 100, Error batch 77: 0.0008175724096138061\n",
      "√âpoca 100, Error batch 78: 0.0007615224138906223\n",
      "√âpoca 100, Error batch 79: 0.0011509422055959928\n",
      "√âpoca 100, Error batch 80: 0.01676238907288962\n",
      "√âpoca 100, Error batch 81: 0.0013499663723675892\n",
      "√âpoca 100, Error batch 82: 0.000568913952498096\n",
      "√âpoca 100, Error batch 83: 0.01638109062996834\n",
      "√âpoca 100, Error batch 84: 0.0009482642318866364\n",
      "√âpoca 100, Error batch 85: 0.0009729572733831607\n",
      "√âpoca 100, Error batch 86: 0.0006273034081418121\n",
      "√âpoca 100, Error batch 87: 0.0008720789801825365\n",
      "√âpoca 100, Error batch 88: 0.017006454889177663\n",
      "√âpoca 100, Error batch 89: 0.0016813048919655923\n",
      "√âpoca 100, Error batch 90: 0.0012572080813059129\n",
      "√âpoca 100, Error batch 91: 0.0014993271082054055\n",
      "√âpoca 100, Error batch 92: 0.016334282869872537\n",
      "√âpoca 100, Error batch 93: 0.019205236095502443\n",
      "√âpoca 100, Error batch 94: 0.0005828437865374031\n",
      "√âpoca 100, Error batch 95: 0.0015923636722108823\n",
      "√âpoca 100, Error batch 96: 0.0009513715579469429\n",
      "√âpoca 100, Error batch 97: 0.003736444290467218\n",
      "√âpoca 100, Error batch 98: 0.0018291138593587459\n",
      "√âpoca 100, Error batch 99: 0.0016664384324289962\n",
      "√âpoca 100, Error batch 100: 0.0009405810247074052\n",
      "√âpoca 100, Error batch 101: 0.016937586595282363\n",
      "√âpoca 100, Error batch 102: 0.016800202901844006\n",
      "√âpoca 100, Error batch 103: 0.015913973604619942\n",
      "√âpoca 100, Error batch 104: 0.0008615090863398628\n",
      "√âpoca 100, Error batch 105: 0.017885365438301052\n",
      "√âpoca 100, Error batch 106: 0.0006141473825353506\n",
      "√âpoca 100, Error batch 107: 0.000793887407835979\n",
      "√âpoca 100, Error batch 108: 0.0018434390099975606\n",
      "√âpoca 100, Error batch 109: 0.0008465484997027446\n",
      "√âpoca 100, Error batch 110: 0.001689162171491608\n",
      "√âpoca 100, Error batch 111: 0.001202554479961181\n",
      "√âpoca 100, Error batch 112: 0.0015789774796152452\n",
      "√âpoca 100, Error batch 113: 0.0006529953776729156\n",
      "√âpoca 100, Error batch 114: 0.0004878345100097689\n",
      "√âpoca 100, Error batch 115: 0.016576785122056768\n",
      "√âpoca 100, Error batch 116: 0.0020840095677399106\n",
      "√âpoca 100, Error batch 117: 0.002135308286278681\n",
      "√âpoca 100, Error batch 118: 0.0008217401766449453\n",
      "√âpoca 100, Error batch 119: 0.0008179911375361335\n",
      "√âpoca 100, Error batch 120: 0.0007038375612435552\n",
      "√âpoca 100, Error batch 121: 0.001037449988657457\n",
      "√âpoca 100, Error batch 122: 0.03206019519937613\n",
      "√âpoca 100, Error batch 123: 0.0004935596058819923\n",
      "√âpoca 100, Error batch 124: 0.01663460502167043\n",
      "√âpoca 100, Error batch 125: 0.0004743476657045081\n",
      "√âpoca 100, Error batch 126: 0.0016377009295322508\n",
      "√âpoca 100, Error batch 127: 0.001841161928862376\n",
      "√âpoca 100, Error batch 128: 0.0022276782688734668\n",
      "√âpoca 100, Error batch 129: 0.0006942344183327081\n",
      "√âpoca 100, Error batch 130: 0.00032379119646056204\n",
      "√âpoca 100, Error batch 131: 0.0008866802953773206\n",
      "√âpoca 100, Error batch 132: 0.028691326149612888\n",
      "√âpoca 100, Error batch 133: 0.0025623012868222898\n",
      "√âpoca 100, Error batch 134: 0.001609209056239427\n",
      "√âpoca 100, Error batch 135: 0.0039505163106237275\n",
      "√âpoca 100, Error batch 136: 0.002738083051245217\n",
      "√âpoca 100, Error batch 137: 0.00304013740583986\n",
      "√âpoca 100, Error batch 138: 0.0014778806163319655\n",
      "√âpoca 100, Error batch 139: 0.0010120577956547075\n",
      "√âpoca 100, Error batch 140: 0.001337255881759846\n",
      "√âpoca 100, Error batch 141: 0.0006044131440789758\n",
      "√âpoca 100, Error: 0.00468099820824741%\n",
      "√âpoca 200, Error batch 1: 0.0002782692202971906\n",
      "√âpoca 200, Error batch 2: 0.03145089648529545\n",
      "√âpoca 200, Error batch 3: 0.00019779680157208646\n",
      "√âpoca 200, Error batch 4: 0.0002562445540564213\n",
      "√âpoca 200, Error batch 5: 0.00021787838022760249\n",
      "√âpoca 200, Error batch 6: 0.00015133397343506294\n",
      "√âpoca 200, Error batch 7: 0.015769732355252496\n",
      "√âpoca 200, Error batch 8: 0.01586571412101465\n",
      "√âpoca 200, Error batch 9: 0.0002531918766811942\n",
      "√âpoca 200, Error batch 10: 0.015751995849139232\n",
      "√âpoca 200, Error batch 11: 0.00035885113787600985\n",
      "√âpoca 200, Error batch 12: 0.00043592104608754467\n",
      "√âpoca 200, Error batch 13: 0.00026504249122529086\n",
      "√âpoca 200, Error batch 14: 0.00028834552470770997\n",
      "√âpoca 200, Error batch 15: 0.015846499539016935\n",
      "√âpoca 200, Error batch 16: 0.000332643548883059\n",
      "√âpoca 200, Error batch 17: 0.0004034773900617474\n",
      "√âpoca 200, Error batch 18: 0.0002053065336425205\n",
      "√âpoca 200, Error batch 19: 0.00024591940724639865\n",
      "√âpoca 200, Error batch 20: 0.015722602503833297\n",
      "√âpoca 200, Error batch 21: 0.0008085090254292977\n",
      "√âpoca 200, Error batch 22: 0.00021930216743972452\n",
      "√âpoca 200, Error batch 23: 0.00033365379923470305\n",
      "√âpoca 200, Error batch 24: 0.0005195279586169921\n",
      "√âpoca 200, Error batch 25: 0.0001962884176428138\n",
      "√âpoca 200, Error batch 26: 0.00029517913443129174\n",
      "√âpoca 200, Error batch 27: 8.298933796111278e-05\n",
      "√âpoca 200, Error batch 28: 0.00013591857032391427\n",
      "√âpoca 200, Error batch 29: 0.0010582487336988841\n",
      "√âpoca 200, Error batch 30: 0.0001869364201337949\n",
      "√âpoca 200, Error batch 31: 0.00026337019821255625\n",
      "√âpoca 200, Error batch 32: 0.00017172095547735993\n",
      "√âpoca 200, Error batch 33: 0.0003835058008045645\n",
      "√âpoca 200, Error batch 34: 0.00024061732990934197\n",
      "√âpoca 200, Error batch 35: 0.00021479317209171858\n",
      "√âpoca 200, Error batch 36: 0.0002176798251368421\n",
      "√âpoca 200, Error batch 37: 0.0002980199665279352\n",
      "√âpoca 200, Error batch 38: 0.0002959733369854237\n",
      "√âpoca 200, Error batch 39: 0.00027910292307280524\n",
      "√âpoca 200, Error batch 40: 0.015936934207510123\n",
      "√âpoca 200, Error batch 41: 0.015572900374568724\n",
      "√âpoca 200, Error batch 42: 0.0003705028339420975\n",
      "√âpoca 200, Error batch 43: 0.01578698336536548\n",
      "√âpoca 200, Error batch 44: 0.0004457244751280402\n",
      "√âpoca 200, Error batch 45: 0.0003330408072920347\n",
      "√âpoca 200, Error batch 46: 0.00026923500707249113\n",
      "√âpoca 200, Error batch 47: 0.00024450468310245393\n",
      "√âpoca 200, Error batch 48: 0.00013326315870595373\n",
      "√âpoca 200, Error batch 49: 0.01574251459565541\n",
      "√âpoca 200, Error batch 50: 0.0001931833700348691\n",
      "√âpoca 200, Error batch 51: 0.0005013391746528337\n",
      "√âpoca 200, Error batch 52: 0.0005294272554143548\n",
      "√âpoca 200, Error batch 53: 0.0010356986761045786\n",
      "√âpoca 200, Error batch 54: 0.0001583106870109895\n",
      "√âpoca 200, Error batch 55: 0.0005139196852360047\n",
      "√âpoca 200, Error batch 56: 0.0001998481109875284\n",
      "√âpoca 200, Error batch 57: 0.00017802837813926865\n",
      "√âpoca 200, Error batch 58: 0.00015728974119617325\n",
      "√âpoca 200, Error batch 59: 0.015762002146437033\n",
      "√âpoca 200, Error batch 60: 0.00019846987027512128\n",
      "√âpoca 200, Error batch 61: 0.00024309482135745797\n",
      "√âpoca 200, Error batch 62: 0.0004232282067071477\n",
      "√âpoca 200, Error batch 63: 0.00027208005794962905\n",
      "√âpoca 200, Error batch 64: 0.00011637432996612373\n",
      "√âpoca 200, Error batch 65: 0.00016561418892040917\n",
      "√âpoca 200, Error batch 66: 0.0003223084798090511\n",
      "√âpoca 200, Error batch 67: 0.00013231516563041478\n",
      "√âpoca 200, Error batch 68: 0.00020019156312054117\n",
      "√âpoca 200, Error batch 69: 0.0001982016387139938\n",
      "√âpoca 200, Error batch 70: 0.0005044208771780396\n",
      "√âpoca 200, Error batch 71: 0.03135842754615871\n",
      "√âpoca 200, Error batch 72: 0.0009069017722515167\n",
      "√âpoca 200, Error batch 73: 0.00022150202877808635\n",
      "√âpoca 200, Error batch 74: 0.00011575048388392544\n",
      "√âpoca 200, Error batch 75: 0.0006134565993325949\n",
      "√âpoca 200, Error batch 76: 0.01580539156129647\n",
      "√âpoca 200, Error batch 77: 0.0002768437261244835\n",
      "√âpoca 200, Error batch 78: 0.0002241660161299886\n",
      "√âpoca 200, Error batch 79: 0.015758906781035885\n",
      "√âpoca 200, Error batch 80: 0.031565517487904926\n",
      "√âpoca 200, Error batch 81: 0.00046853509189296927\n",
      "√âpoca 200, Error batch 82: 0.00043829029252726427\n",
      "√âpoca 200, Error batch 83: 0.0001228637522784504\n",
      "√âpoca 200, Error batch 84: 0.00014143679993367794\n",
      "√âpoca 200, Error batch 85: 0.00011669963912339988\n",
      "√âpoca 200, Error batch 86: 0.0003441614783423041\n",
      "√âpoca 200, Error batch 87: 0.0004483292482524772\n",
      "√âpoca 200, Error batch 88: 0.0006922392812040753\n",
      "√âpoca 200, Error batch 89: 0.00016927096775883873\n",
      "√âpoca 200, Error batch 90: 0.00031751150369503124\n",
      "√âpoca 200, Error batch 91: 0.00022339579752861215\n",
      "√âpoca 200, Error batch 92: 0.00028038861020442243\n",
      "√âpoca 200, Error batch 93: 0.0002751795131883893\n",
      "√âpoca 200, Error batch 94: 0.000386926191818111\n",
      "√âpoca 200, Error batch 95: 0.03126433895814299\n",
      "√âpoca 200, Error batch 96: 0.0003605084707389578\n",
      "√âpoca 200, Error batch 97: 0.00024413253299242872\n",
      "√âpoca 200, Error batch 98: 0.00019709733178001248\n",
      "√âpoca 200, Error batch 99: 0.0029179266964229457\n",
      "√âpoca 200, Error batch 100: 0.015822606389998488\n",
      "√âpoca 200, Error batch 101: 0.001470928521488569\n",
      "√âpoca 200, Error batch 102: 0.03152535244797208\n",
      "√âpoca 200, Error batch 103: 0.015971590651744853\n",
      "√âpoca 200, Error batch 104: 0.000647663589373217\n",
      "√âpoca 200, Error batch 105: 0.00045008777839275314\n",
      "√âpoca 200, Error batch 106: 0.011549894688705677\n",
      "√âpoca 200, Error batch 107: 0.000588402342339945\n",
      "√âpoca 200, Error batch 108: 0.0005673886122757685\n",
      "√âpoca 200, Error batch 109: 0.0002421840138653617\n",
      "√âpoca 200, Error batch 110: 0.00038554919105560114\n",
      "√âpoca 200, Error batch 111: 0.01585734004732357\n",
      "√âpoca 200, Error batch 112: 0.00030680404173073317\n",
      "√âpoca 200, Error batch 113: 0.00038705246627847404\n",
      "√âpoca 200, Error batch 114: 0.00024932625295426855\n",
      "√âpoca 200, Error batch 115: 0.01564405292716216\n",
      "√âpoca 200, Error batch 116: 0.00035679157082523974\n",
      "√âpoca 200, Error batch 117: 0.00110848016072752\n",
      "√âpoca 200, Error batch 118: 0.0005517101489337401\n",
      "√âpoca 200, Error batch 119: 0.016515533857446297\n",
      "√âpoca 200, Error batch 120: 0.0009052070356182476\n",
      "√âpoca 200, Error batch 121: 0.0015541227604472054\n",
      "√âpoca 200, Error batch 122: 0.015880083987190083\n",
      "√âpoca 200, Error batch 123: 0.002077293210643729\n",
      "√âpoca 200, Error batch 124: 0.00041634906203220185\n",
      "√âpoca 200, Error batch 125: 0.016205120586917424\n",
      "√âpoca 200, Error batch 126: 0.0002726197417799283\n",
      "√âpoca 200, Error batch 127: 0.0003476318064121699\n",
      "√âpoca 200, Error batch 128: 0.0005670335538930347\n",
      "√âpoca 200, Error batch 129: 0.0008333907062173574\n",
      "√âpoca 200, Error batch 130: 0.00016776191468371952\n",
      "√âpoca 200, Error batch 131: 0.0002364402384735706\n",
      "√âpoca 200, Error batch 132: 0.00029140375401186204\n",
      "√âpoca 200, Error batch 133: 0.00028230683396091835\n",
      "√âpoca 200, Error batch 134: 0.0004234002519010068\n",
      "√âpoca 200, Error batch 135: 0.00043799193458653353\n",
      "√âpoca 200, Error batch 136: 0.0006017893353736578\n",
      "√âpoca 200, Error batch 137: 0.0001867531080662099\n",
      "√âpoca 200, Error batch 138: 0.0002729049921264578\n",
      "√âpoca 200, Error batch 139: 0.0008212299825383864\n",
      "√âpoca 200, Error batch 140: 0.0006644528466514233\n",
      "√âpoca 200, Error batch 141: 0.0003126863560348951\n",
      "√âpoca 200, Error: 0.0036705869475655586%\n"
     ]
    }
   ],
   "source": [
    "#Ejemplo de creaci√≥n y entrenamiento del MLP para espa√±ol\n",
    "mlp_es = MLP_TODO(\n",
    "    num_entradas=X_entrenamiento_es_vect.shape[1],\n",
    "    num_neuronas_ocultas=64,\n",
    "    num_salidas=1,\n",
    "    epochs=300,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.1,\n",
    "    normalizacion=\"none\",\n",
    "    inicializacion=\"xavier\"\n",
    ")\n",
    "\n",
    "mlp_es.train(X_entrenamiento_es_vect, y_train_es)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c459d772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy espa√±ol: 0.712\n"
     ]
    }
   ],
   "source": [
    "#Evaluar el modelo en el conjunto de prueba\n",
    "predicciones_es = mlp_es.predict(X_prueba_es_vect)\n",
    "acc_es = mlp_es.accuracy(X_prueba_es_vect, y_test_es)\n",
    "print(\"Accuracy espa√±ol:\", acc_es)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c6075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Ejemplo de Entrenamiento y evaluaci√≥n del MLP para Hate Speech - Espa√±ol e Ingl√©s\n",
    "# ====================================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Convertir labels a formato N x 1 ---\n",
    "y_train_es = np.array(y_train_es).reshape(-1,1)\n",
    "y_test_es = np.array(y_test_es).reshape(-1,1)\n",
    "y_train_en = np.array(y_train_en).reshape(-1,1)\n",
    "y_test_en = np.array(y_test_en).reshape(-1,1)\n",
    "\n",
    "# ====================================================\n",
    "# FUNCIONES AUXILIARES PARA M√âTRICAS\n",
    "# ====================================================\n",
    "def mostrar_metricas(y_real, y_pred, idioma=\"\"):\n",
    "    precision = precision_score(y_real, y_pred)\n",
    "    recall = recall_score(y_real, y_pred)\n",
    "    f1 = f1_score(y_real, y_pred)\n",
    "    \n",
    "    print(f\"--- M√©tricas {idioma} ---\")\n",
    "    print(f\"Precisi√≥n: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "    print(\"------------------------\")\n",
    "    return precision, recall, f1\n",
    "\n",
    "def graficar_error(er_historial, idioma=\"\"):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(range(len(er_historial)), er_historial, marker='o')\n",
    "    plt.title(f'Error promedio por √©poca - {idioma}')\n",
    "    plt.xlabel('√âpoca')\n",
    "    plt.ylabel('Error promedio (MSE)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# ====================================================\n",
    "# ENTRENAMIENTO Y EVALUACI√ìN - ESPA√ëOL\n",
    "# ====================================================\n",
    "mlp_es = MLP_TODO(\n",
    "    num_entradas=X_entrenamiento_es_vect.shape[1],\n",
    "    num_neuronas_ocultas=64,   # puedes cambiar seg√∫n tabla de pr√°ctica\n",
    "    num_salidas=1,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.1,\n",
    "    normalizacion=\"none\",\n",
    "    inicializacion=\"xavier\"\n",
    ")\n",
    "\n",
    "print(\"=== Entrenando MLP Espa√±ol ===\")\n",
    "mlp_es.train(X_entrenamiento_es_vect, y_train_es)\n",
    "\n",
    "# Predicci√≥n y m√©tricas\n",
    "predicciones_es = mlp_es.predict(X_prueba_es_vect)\n",
    "mostrar_metricas(y_test_es, predicciones_es, idioma=\"Espa√±ol\")\n",
    "\n",
    "# Graficar error vs √©pocas\n",
    "graficar_error(mlp_es.errores_history, idioma=\"Espa√±ol\")\n",
    "\n",
    "# ====================================================\n",
    "# ENTRENAMIENTO Y EVALUACI√ìN - INGL√âS\n",
    "# ====================================================\n",
    "mlp_en = MLP_TODO(\n",
    "    num_entradas=X_entrenamiento_en_vect.shape[1],\n",
    "    num_neuronas_ocultas=64,\n",
    "    num_salidas=1,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.1,\n",
    "    normalizacion=\"none\",\n",
    "    inicializacion=\"xavier\"\n",
    ")\n",
    "\n",
    "print(\"=== Entrenando MLP Ingl√©s ===\")\n",
    "mlp_en.train(X_entrenamiento_en_vect, y_train_en)\n",
    "\n",
    "# Predicci√≥n y m√©tricas\n",
    "predicciones_en = mlp_en.predict(X_prueba_en_vect)\n",
    "mostrar_metricas(y_test_en, predicciones_en, idioma=\"Ingl√©s\")\n",
    "\n",
    "# Graficar error vs √©pocas\n",
    "graficar_error(mlp_en.errores_history, idioma=\"Ingl√©s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374f5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# EXPERIMENTOS CON TODAS LAS CONFIGURACIONES\n",
    "# ====================================================\n",
    "\n",
    "# Par√°metros a experimentar seg√∫n la pr√°ctica\n",
    "neuronas_ocultas = [64, 128, 256]  # puedes agregar 512, 1024 si quieres\n",
    "inicializaciones = [\"xavier\", \"normal\"]\n",
    "tipos_vector = [\"tf\", \"tfidf\"]  # frecuencia de t√©rminos o TF-IDF\n",
    "ngramas_posibles = [(1,1), (1,2)]  # unigramas, unigramas+bigramas\n",
    "preprocesamientos = [\"none\"]  # ya hicimos preprocesamiento, puedes definir variantes si quieres\n",
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "batch_sizes = [16, 32, 64]\n",
    "epochs_posibles = [100, 200, 300]\n",
    "\n",
    "# Diccionario para almacenar resultados\n",
    "resultados = []\n",
    "\n",
    "# ====================================================\n",
    "# FUNCION PARA EJECUTAR EXPERIMENTO\n",
    "# ====================================================\n",
    "def ejecutar_experimento(X_train_raw, y_train, X_test_raw, y_test, idioma=\"\"):\n",
    "    for n_ocultas in neuronas_ocultas:\n",
    "        for inicializacion in inicializaciones:\n",
    "            for tipo_vec in tipos_vector:\n",
    "                for ngramas in ngramas_posibles:\n",
    "                    for lr in learning_rates:\n",
    "                        for batch in batch_sizes:\n",
    "                            for epocas in epochs_posibles:\n",
    "\n",
    "                                # Vectorizar\n",
    "                                X_train_vect, X_test_vect, vectorizador = vectorizar_textos(\n",
    "                                    X_train_raw, X_test_raw,\n",
    "                                    metodo=tipo_vec,\n",
    "                                    ngramas=ngramas\n",
    "                                )\n",
    "\n",
    "                                # Crear MLP\n",
    "                                mlp = MLP_TODO(\n",
    "                                    num_entradas=X_train_vect.shape[1],\n",
    "                                    num_neuronas_ocultas=n_ocultas,\n",
    "                                    num_salidas=1,\n",
    "                                    epochs=epocas,\n",
    "                                    batch_size=batch,\n",
    "                                    learning_rate=lr,\n",
    "                                    normalizacion=\"none\",\n",
    "                                    inicializacion=inicializacion\n",
    "                                )\n",
    "\n",
    "                                print(f\"\\nEntrenando {idioma} | Neuronas {n_ocultas}, Inicial {inicializacion}, Vector {tipo_vec}, Ngramas {ngramas}, LR {lr}, Batch {batch}, Epochs {epocas}\")\n",
    "                                \n",
    "                                # Entrenamiento\n",
    "                                mlp.train(X_train_vect, y_train)\n",
    "\n",
    "                                # Predicci√≥n\n",
    "                                y_pred = mlp.predict(X_test_vect)\n",
    "\n",
    "                                # M√©tricas\n",
    "                                precision, recall, f1 = mostrar_metricas(y_test, y_pred, idioma=idioma)\n",
    "\n",
    "                                # Guardar resultados\n",
    "                                resultados.append({\n",
    "                                    \"idioma\": idioma,\n",
    "                                    \"neuronas_ocultas\": n_ocultas,\n",
    "                                    \"inicializacion\": inicializacion,\n",
    "                                    \"vector\": tipo_vec,\n",
    "                                    \"ngramas\": ngramas,\n",
    "                                    \"learning_rate\": lr,\n",
    "                                    \"batch_size\": batch,\n",
    "                                    \"epochs\": epocas,\n",
    "                                    \"precision\": precision,\n",
    "                                    \"recall\": recall,\n",
    "                                    \"f1\": f1,\n",
    "                                    \"error_final\": mlp.errores_history[-1]\n",
    "                                })\n",
    "\n",
    "# ====================================================\n",
    "# EJECUTAR PARA ESPA√ëOL\n",
    "# ====================================================\n",
    "ejecutar_experimento(X_entrenamiento_es, y_train_es, X_prueba_es, y_test_es, idioma=\"Espa√±ol\")\n",
    "\n",
    "# ====================================================\n",
    "# EJECUTAR PARA INGL√âS\n",
    "# ====================================================\n",
    "ejecutar_experimento(X_entrenamiento_en, y_train_en, X_prueba_en, y_test_en, idioma=\"Ingl√©s\")\n",
    "\n",
    "# ====================================================\n",
    "# CONVERTIR RESULTADOS A DATAFRAME\n",
    "# ====================================================\n",
    "import pandas as pd\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "df_resultados = df_resultados.sort_values(by=\"f1\", ascending=False)  # Ordenar por F1-score\n",
    "print(\"\\n--- TOP 5 CONFIGURACIONES ---\")\n",
    "print(df_resultados.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614f1ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# GRAFICAR CURVAS DE ERROR DE LAS 5 MEJORES CONFIGURACIONES\n",
    "# ====================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seleccionar las 5 mejores configuraciones seg√∫n F1-score\n",
    "top5_config = df_resultados.head(5)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for i, fila in top5_config.iterrows():\n",
    "    # Recuperar historial de errores del MLP de esa configuraci√≥n\n",
    "    # üîπ Asumimos que guardaste los objetos MLP en resultados con clave 'mlp_obj'\n",
    "    mlp_obj = fila['mlp_obj']\n",
    "    plt.plot(range(len(mlp_obj.errores_history)), mlp_obj.errores_history, marker='o', label=(\n",
    "        f\"{fila['idioma']} | Noc:{fila['neuronas_ocultas']} | Init:{fila['inicializacion']} | \"\n",
    "        f\"Vec:{fila['vector']} | LR:{fila['learning_rate']} | Batch:{fila['batch_size']} | Ep:{fila['epochs']}\"\n",
    "    ))\n",
    "\n",
    "plt.title(\"Error promedio por √©poca - Top 5 configuraciones\")\n",
    "plt.xlabel(\"√âpoca\")\n",
    "plt.ylabel(\"Error promedio (MSE)\")\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=8)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
