{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementar un MLP con PyTorch para clasificación basado en el dataset de agresividad\n",
    "\n",
    "1. **Definir los preprocesamientos para el texto**:  \n",
    "   - convertir a minúsculas\n",
    "   - normalizar el texto: borrar símbolos, puntuación, caracteres duplicados, etc.\n",
    "\n",
    "2. **Separar los datos para entrenamiento y prueba**:  \n",
    "   - Crear los dataset de entrenamiento y test con al función train_test_split \n",
    "\n",
    "3. **Construir la matriz de Documento-Término**:  \n",
    "   - Definir los parámetros para usar unigramas\n",
    "   - Usar la clase TfidfVectorizer para construir la matriz con los datos de entrenamiento\n",
    "   \n",
    "4. **Preparar los lotes de datos (minibatches) para el entrenamiento de la red**:  \n",
    "   - Definir los minibatches con la matriz TFIDF construida\n",
    "\n",
    "5. **Definir la arquitectura de la red**:  \n",
    "   - Definir una red de 2 capas, con funciones PReLU en las capas ocultas y una capa de salida\n",
    "\n",
    "6. **Entrenar el modelo**:  \n",
    "   - Definir los parámetros de las red como: número de épocas, learning_rate, número de neuronas para las capas ocultas, etc.\n",
    "   \n",
    "7. **Evaluar el modelo**:  \n",
    "   - Después del entrenamiento, probar la red con las entradas del conjunto de test y evaluar el desempeño con las métricas: Precisión, Recall, F1-score o F1-Measure y Accuracy.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de los datos y minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File ./data_aggressiveness_es.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m np.random.seed(random_state)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#cargar los datos\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m dataset = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./data_aggressiveness_es.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m#conteo de clases\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTotal de ejemplos de entrenamiento\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\edwar\\miniconda3\\envs\\RNA\\Lib\\site-packages\\pandas\\io\\json\\_json.py:791\u001b[39m, in \u001b[36mread_json\u001b[39m\u001b[34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient != \u001b[33m\"\u001b[39m\u001b[33mtable\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    789\u001b[39m     convert_axes = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m json_reader = \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[32m    813\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\edwar\\miniconda3\\envs\\RNA\\Lib\\site-packages\\pandas\\io\\json\\_json.py:904\u001b[39m, in \u001b[36mJsonReader.__init__\u001b[39m\u001b[34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[39m\n\u001b[32m    902\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = filepath_or_buffer\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mujson\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data_from_filepath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28mself\u001b[39m._preprocess_data(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\edwar\\miniconda3\\envs\\RNA\\Lib\\site-packages\\pandas\\io\\json\\_json.py:960\u001b[39m, in \u001b[36mJsonReader._get_data_from_filepath\u001b[39m\u001b[34m(self, filepath_or_buffer)\u001b[39m\n\u001b[32m    952\u001b[39m     filepath_or_buffer = \u001b[38;5;28mself\u001b[39m.handles.handle\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    954\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    955\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer.lower().endswith(\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[32m    959\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    962\u001b[39m     warnings.warn(\n\u001b[32m    963\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing literal json to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mread_json\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future version. To read from a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    967\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    968\u001b[39m     )\n",
      "\u001b[31mFileNotFoundError\u001b[39m: File ./data_aggressiveness_es.json does not exist"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# colocar la semilla para la generación de números aleatorios para la reproducibilidad de experimentos\n",
    "\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "#cargar los datos\n",
    "dataset = pd.read_json(\"./data_aggressiveness_es.json\", lines=True)\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de entrenamiento\")\n",
    "print(dataset.klass.value_counts())\n",
    "# Extracción de los textos en arreglos de numpy\n",
    "X = dataset['text'].to_numpy()\n",
    "# Extracción de las etiquetas o clases de entrenamiento\n",
    "Y = dataset['klass'].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Definir las funciones de preprocesamiento de texto vinculadas al proceso de creación de la matriz \n",
    "# Documeno-Término creada con TfidfVectorizer.\n",
    "\n",
    "_STOPWORDS = stopwords.words(\"spanish\")  # agregar más palabras a esta lista si es necesario\n",
    "\n",
    "# Normalización del texto\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "NUMBERS= \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str,\n",
    "                    punct=False,\n",
    "                    accents=False,\n",
    "                    num=False,\n",
    "                    max_dup=2):\n",
    "    \"\"\"\n",
    "        punct=False (elimina la puntuación, True deja intacta la puntuación)\n",
    "        accents=False (elimina los acentos, True deja intactos los acentos)\n",
    "        num= False (elimina los números, True deja intactos los acentos)\n",
    "        max_dup=2 (número máximo de símbolos duplicados de forma consecutiva, rrrrr => rr)\n",
    "    \"\"\"\n",
    "    \n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n",
    "\n",
    "# Preprocesamiento personalizado \n",
    "def mi_preprocesamiento(texto):\n",
    "    #convierte a minúsculas el texto antes de normalizar\n",
    "    tokens = word_tokenize(texto.lower())\n",
    "    texto = \" \".join(tokens)\n",
    "    texto = normaliza_texto(texto)\n",
    "    return texto\n",
    "    \n",
    "# Tokenizador personalizado \n",
    "def mi_tokenizador(texto):\n",
    "    # Elimina stopwords: palabras que no se consideran de contenido y que no agregan valor semántico al texto\n",
    "    #print(\"antes: \", texto)\n",
    "    texto = [t for t in texto.split() if t not in _STOPWORDS]\n",
    "    #print(\"después:\",texto)\n",
    "    return texto\n",
    "\n",
    "# TODO: Codificar las etiquetas de los datos a una forma categórica numérica: LabelEncoder.\n",
    "\n",
    "le = LabelEncoder()\n",
    "# Normalizar las etiquetas a una codificación ordinal para entrada del clasificador\n",
    "Y_encoded= le.fit_transform(Y)\n",
    "print(\"Clases:\")\n",
    "print(le.classes_)\n",
    "print(\"Clases codificadas:\")\n",
    "print(le.transform(le.classes_))\n",
    "\n",
    "# TODO: Dividir el conjunto de datos en conjunto de entrenamiento (80%) y conjunto de pruebas (20%)\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y_encoded, test_size=0.2, stratify=Y_encoded, random_state=42)\n",
    "\n",
    "# Divide el conjunto de entrenamiento en:  entrenamiento (90%) y validación (10%)\n",
    "X_train, X_val, Y_train, Y_val =  train_test_split(X_train, Y_train, test_size=0.1, stratify=Y_train, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Crear la matriz Documento-Término con el dataset de entrenamiento: tfidfVectorizer\n",
    "\n",
    "\n",
    "vec_tfidf = TfidfVectorizer(analyzer=\"word\", preprocessor=mi_preprocesamiento, tokenizer=mi_tokenizador,  ngram_range=(1,1))\n",
    "X_train_tfidf = vec_tfidf.fit_transform(X_train)\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Codificación de la salida onehot\n",
    "\n",
    "Y_train_one_hot = nn.functional.one_hot(torch.from_numpy(Y_train), num_classes=NUM_CLASSES).float()\n",
    "Y_test_one_hot = nn.functional.one_hot(torch.from_numpy(Y_test), num_classes=NUM_CLASSES).float()\n",
    "Y_val_one_hot = nn.functional.one_hot(torch.from_numpy(Y_val), num_classes=NUM_CLASSES).float()\n",
    "\n",
    "\n",
    "# Convertir a matriz densa de tipo de dato float32 (tipo de dato por default en Pytorch)\n",
    "X_train_tfidf = X_train_tfidf.toarray().astype(np.float32)\n",
    "\n",
    "# Tranforma los datos de validación al espacio de representación del entrenamiento\n",
    "X_val_tfidf = vec_tfidf.transform(X_val)\n",
    "\n",
    "# Convertir a matriz densa de tipo de dato float32 (tipo de dato por default en Pytorch)\n",
    "X_val_tfidf = X_val_tfidf.toarray().astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Crear minibatches en PyTorch usando DataLoader\n",
    "def create_minibatches(X, Y, batch_size):\n",
    "    # Recibe los documentos en X y las etiquetas en Y\n",
    "    dataset = TensorDataset(X, Y) # Cargar los datos en un dataset de tensores\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    # loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3694,), (3694,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3694, 2]), torch.Size([1027, 2]), torch.Size([411, 2]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_one_hot.shape, Y_test_one_hot.shape,  Y_val_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de la arquitectura de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir la red neuronal en PyTorch heredando de la clase base de Redes Neuronales: Module\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        # Definición de capas, funciones de activación e inicialización de pesos\n",
    "        input_size_h1 = 128\n",
    "        input_size_h2 = 8 \n",
    "        self.fc1 = nn.Linear(input_size, input_size_h1)\n",
    "        # PReLU tiene parámetros aprendibles: Se recomienda una función de activación independiente por capa\n",
    "        self.act1= nn.PReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(input_size_h1, input_size_h2)\n",
    "        # PReLU tiene parámetros aprendibles: Se recomienda una función de activación independiente por capa\n",
    "        self.act2= nn.PReLU()\n",
    "\n",
    "        self.output = nn.Linear(input_size_h2, output_size)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "\n",
    "        if self.fc1.bias is not None:\n",
    "            nn.init.zeros_(self.fc1.bias)\n",
    "        if self.fc2.bias is not None:\n",
    "            nn.init.zeros_(self.fc2.bias)        \n",
    "        if self.output.bias is not None:\n",
    "            nn.init.zeros_(self.output.bias)        \n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Definición del orden de conexión de las capas y aplición de las funciones de activación\n",
    "        x = self.fc1(X)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.output(x)\n",
    "        # Nota la última capa de salida 'output' no se activa debido a que CrossEntropyLoss usa LogSoftmax internamente. \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento en PyTorch\n",
      "Batch Error : 0.5418707728385925\n",
      "Batch Error : 0.5832555890083313\n",
      "Batch Error : 0.5617039799690247\n",
      "Batch Error : 0.4275079369544983\n",
      "Batch Error : 0.4394717812538147\n",
      "Batch Error : 0.4274562895298004\n",
      "Época 1/100, Pérdida: 0.5183003082357603\n",
      "Época 1/100\n",
      "P= 0.7067865981343994\n",
      "R= 0.6885087059640191\n",
      "F1= 0.695877378435518\n",
      "Acc= 0.7615571776155717\n",
      "Batch Error : 0.10893426090478897\n",
      "Batch Error : 0.12142549455165863\n",
      "Batch Error : 0.10180370509624481\n",
      "Época 2/100, Pérdida: 0.09554767370994749\n",
      "Época 2/100\n",
      "P= 0.7070299771912676\n",
      "R= 0.7205269855961127\n",
      "F1= 0.7125874125874125\n",
      "Acc= 0.7566909975669099\n",
      "Batch Error : 0.0131586454808712\n",
      "Batch Error : 0.003791796276345849\n",
      "Época 3/100, Pérdida: 0.012999055561898598\n",
      "Época 3/100\n",
      "P= 0.7228848991637973\n",
      "R= 0.7096951466419853\n",
      "F1= 0.7154438860971524\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 0.0019520187051966786\n",
      "Batch Error : 0.028602613136172295\n",
      "Época 4/100, Pérdida: 0.003343283452679692\n",
      "Época 4/100\n",
      "P= 0.7133286713286713\n",
      "R= 0.7205848325331174\n",
      "F1= 0.7166464103712233\n",
      "Acc= 0.7639902676399026\n",
      "Batch Error : 0.00035441582440398633\n",
      "Batch Error : 0.00019946276734117419\n",
      "Batch Error : 9.414754458703101e-05\n",
      "Época 5/100, Pérdida: 0.0004650398234310881\n",
      "Época 5/100\n",
      "P= 0.7168018018018019\n",
      "R= 0.7088129808526639\n",
      "F1= 0.7124752387755253\n",
      "Acc= 0.7688564476885644\n",
      "Época 6/100, Pérdida: 0.00016128872197566972\n",
      "Época 6/100\n",
      "P= 0.7140705298600035\n",
      "R= 0.7096372997049807\n",
      "F1= 0.7117475160724722\n",
      "Acc= 0.7664233576642335\n",
      "Batch Error : 0.0001347096695099026\n",
      "Batch Error : 0.00011497507512103766\n",
      "Época 7/100, Pérdida: 9.839439823328324e-05\n",
      "Época 7/100\n",
      "P= 0.7168018018018019\n",
      "R= 0.7088129808526639\n",
      "F1= 0.7124752387755253\n",
      "Acc= 0.7688564476885644\n",
      "Batch Error : 4.129078661208041e-05\n",
      "Batch Error : 4.346814239397645e-05\n",
      "Batch Error : 0.00013818003935739398\n",
      "Batch Error : 0.00010245407611364499\n",
      "Época 8/100, Pérdida: 7.137804823287297e-05\n",
      "Época 8/100\n",
      "P= 0.7197469746974698\n",
      "R= 0.707988662000347\n",
      "F1= 0.7131763378274039\n",
      "Acc= 0.7712895377128953\n",
      "Batch Error : 5.227040674071759e-05\n",
      "Batch Error : 2.9030134101049043e-05\n",
      "Batch Error : 9.619128832127899e-05\n",
      "Batch Error : 4.603136039804667e-05\n",
      "Batch Error : 4.647410605684854e-05\n",
      "Época 9/100, Pérdida: 5.467319718671457e-05\n",
      "Época 9/100\n",
      "P= 0.7197185276832663\n",
      "R= 0.7054578585063921\n",
      "F1= 0.7115832537028189\n",
      "Acc= 0.7712895377128953\n",
      "Batch Error : 4.890972195426002e-05\n",
      "Época 10/100, Pérdida: 4.4364292958123076e-05\n",
      "Época 10/100\n",
      "P= 0.7228848991637973\n",
      "R= 0.7096951466419853\n",
      "F1= 0.7154438860971524\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 3.519166784826666e-05\n",
      "Batch Error : 3.015731090272311e-05\n",
      "Batch Error : 1.0142477549379691e-05\n",
      "Batch Error : 3.7721398257417604e-05\n",
      "Batch Error : 3.0587409128202125e-05\n",
      "Época 11/100, Pérdida: 3.662734569184851e-05\n",
      "Época 11/100\n",
      "P= 0.7228848991637973\n",
      "R= 0.7096951466419853\n",
      "F1= 0.7154438860971524\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 2.206947101512924e-05\n",
      "Batch Error : 6.0357175243552774e-05\n",
      "Batch Error : 1.3627634871227201e-05\n",
      "Época 12/100, Pérdida: 3.106793008492818e-05\n",
      "Época 12/100\n",
      "P= 0.7228848991637973\n",
      "R= 0.7096951466419853\n",
      "F1= 0.7154438860971524\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 2.411207060504239e-05\n",
      "Batch Error : 1.7731050320435315e-05\n",
      "Batch Error : 2.5756944523891434e-05\n",
      "Batch Error : 2.6987632736563683e-05\n",
      "Batch Error : 2.459472671034746e-05\n",
      "Batch Error : 4.843357237405144e-05\n",
      "Época 13/100, Pérdida: 2.655767080984239e-05\n",
      "Época 13/100\n",
      "P= 0.7293183940242765\n",
      "R= 0.7131081159252617\n",
      "F1= 0.7200041923385014\n",
      "Acc= 0.7785888077858881\n",
      "Batch Error : 3.701822424773127e-05\n",
      "Época 14/100, Pérdida: 2.3196748927524634e-05\n",
      "Época 14/100\n",
      "P= 0.7293183940242765\n",
      "R= 0.7131081159252617\n",
      "F1= 0.7200041923385014\n",
      "Acc= 0.7785888077858881\n",
      "Batch Error : 2.1674431991414167e-05\n",
      "Batch Error : 1.742149834171869e-05\n",
      "Batch Error : 1.2683438399108127e-05\n",
      "Batch Error : 5.244773274171166e-05\n",
      "Batch Error : 2.799860158120282e-05\n",
      "Época 15/100, Pérdida: 2.0274722907021804e-05\n",
      "Época 15/100\n",
      "P= 0.7261807817589576\n",
      "R= 0.7088708277896685\n",
      "F1= 0.7161261261261261\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 1.2544243872980587e-05\n",
      "Época 16/100, Pérdida: 1.7946273632772098e-05\n",
      "Época 16/100\n",
      "P= 0.7261807817589576\n",
      "R= 0.7088708277896685\n",
      "F1= 0.7161261261261261\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 1.5982805052772164e-05\n",
      "Época 17/100, Pérdida: 1.611315863179886e-05\n",
      "Época 17/100\n",
      "P= 0.7261807817589576\n",
      "R= 0.7088708277896685\n",
      "F1= 0.7161261261261261\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 2.1509375073947012e-05\n",
      "Época 18/100, Pérdida: 1.444136457927204e-05\n",
      "Época 18/100\n",
      "P= 0.7261807817589576\n",
      "R= 0.7088708277896685\n",
      "F1= 0.7161261261261261\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 2.4910381398512982e-05\n",
      "Batch Error : 1.260858425666811e-05\n",
      "Batch Error : 1.4263499906519428e-05\n",
      "Época 19/100, Pérdida: 1.3027681438086738e-05\n",
      "Época 19/100\n",
      "P= 0.7261807817589576\n",
      "R= 0.7088708277896685\n",
      "F1= 0.7161261261261261\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 2.2214408090803772e-05\n",
      "Batch Error : 5.765560672443826e-06\n",
      "Batch Error : 9.229227543983143e-06\n",
      "Batch Error : 9.296781172452029e-06\n",
      "Batch Error : 1.1818458006018773e-05\n",
      "Batch Error : 3.6022822769155027e-06\n",
      "Batch Error : 1.2767420230375137e-05\n",
      "Época 20/100, Pérdida: 1.186362938542202e-05\n",
      "Época 20/100\n",
      "P= 0.7261807817589576\n",
      "R= 0.7088708277896685\n",
      "F1= 0.7161261261261261\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 1.2632077414309606e-05\n",
      "Batch Error : 9.001844773592893e-06\n",
      "Época 21/100, Pérdida: 1.0814568354519373e-05\n",
      "Época 21/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 1.8531802197685465e-05\n",
      "Batch Error : 3.285665343355504e-06\n",
      "Batch Error : 5.711388894269476e-06\n",
      "Batch Error : 8.963909749581944e-06\n",
      "Batch Error : 1.1020350939361379e-05\n",
      "Época 22/100, Pérdida: 9.977216599320949e-06\n",
      "Época 22/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 2.172034146497026e-05\n",
      "Batch Error : 1.2956657883478329e-05\n",
      "Batch Error : 4.709529093815945e-06\n",
      "Época 23/100, Pérdida: 9.128357070384763e-06\n",
      "Época 23/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 1.0055347047455143e-05\n",
      "Batch Error : 4.302360594010679e-06\n",
      "Batch Error : 6.589598797290819e-06\n",
      "Época 24/100, Pérdida: 8.441239656506162e-06\n",
      "Época 24/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 3.7475792851182632e-06\n",
      "Batch Error : 6.933064469194505e-06\n",
      "Batch Error : 1.2901277841592673e-05\n",
      "Batch Error : 6.301062057900708e-06\n",
      "Época 25/100, Pérdida: 7.802406108912064e-06\n",
      "Época 25/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 6.987344477238366e-06\n",
      "Batch Error : 3.934715095965657e-06\n",
      "Batch Error : 1.0791848580993246e-05\n",
      "Batch Error : 1.8318921775062336e-06\n",
      "Época 26/100, Pérdida: 7.250522606948929e-06\n",
      "Época 26/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 4.579041160468478e-06\n",
      "Batch Error : 2.961542804769124e-06\n",
      "Batch Error : 5.317613613442518e-06\n",
      "Época 27/100, Pérdida: 6.7371654181599674e-06\n",
      "Época 27/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 7.0256069193419535e-06\n",
      "Batch Error : 3.823741735686781e-06\n",
      "Batch Error : 5.469476036523702e-06\n",
      "Época 28/100, Pérdida: 6.291784098595662e-06\n",
      "Época 28/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 5.34188438905403e-06\n",
      "Batch Error : 6.029136784491129e-06\n",
      "Batch Error : 4.188081220490858e-06\n",
      "Batch Error : 4.718552645499585e-06\n",
      "Época 29/100, Pérdida: 5.887277650580127e-06\n",
      "Época 29/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 1.7881252460938413e-06\n",
      "Batch Error : 5.583101483352948e-06\n",
      "Batch Error : 4.108791927137645e-06\n",
      "Época 30/100, Pérdida: 5.5298301833380685e-06\n",
      "Época 30/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 5.344630153558683e-06\n",
      "Batch Error : 9.894766662910115e-06\n",
      "Época 31/100, Pérdida: 5.205711008313991e-06\n",
      "Época 31/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 2.934536041721003e-06\n",
      "Batch Error : 6.762055363651598e-06\n",
      "Batch Error : 3.542676495271735e-06\n",
      "Batch Error : 6.573764949280303e-06\n",
      "Época 32/100, Pérdida: 4.88291122784521e-06\n",
      "Época 32/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 7.4396684794919565e-06\n",
      "Batch Error : 3.6395035749592353e-06\n",
      "Época 33/100, Pérdida: 4.6193015350581845e-06\n",
      "Época 33/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 8.345803507836536e-06\n",
      "Batch Error : 7.793391887389589e-06\n",
      "Época 34/100, Pérdida: 4.342919416150584e-06\n",
      "Época 34/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 4.500891009229235e-06\n",
      "Batch Error : 6.309051968855783e-06\n",
      "Época 35/100, Pérdida: 4.132253291666937e-06\n",
      "Época 35/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 3.5239838780398713e-06\n",
      "Batch Error : 3.0733231142221484e-06\n",
      "Batch Error : 3.988689968537074e-06\n",
      "Época 36/100, Pérdida: 3.9109240239260295e-06\n",
      "Época 36/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 3.02209673463949e-06\n",
      "Batch Error : 3.92722677133861e-06\n",
      "Batch Error : 5.7054135140788276e-06\n",
      "Batch Error : 7.09224786987761e-06\n",
      "Época 37/100, Pérdida: 3.6979387389159267e-06\n",
      "Época 37/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 1.726644313748693e-06\n",
      "Batch Error : 2.416741608612938e-06\n",
      "Época 38/100, Pérdida: 3.5038313532415238e-06\n",
      "Época 38/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 5.565211267821724e-06\n",
      "Batch Error : 2.021871523538721e-06\n",
      "Batch Error : 4.92462822876405e-06\n",
      "Batch Error : 1.3923173582952586e-06\n",
      "Batch Error : 4.1403313844057266e-06\n",
      "Batch Error : 1.6111723653011722e-06\n",
      "Época 39/100, Pérdida: 3.333784689104881e-06\n",
      "Época 39/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 3.54255848833418e-06\n",
      "Batch Error : 2.1000976175855612e-06\n",
      "Época 40/100, Pérdida: 3.180044051658052e-06\n",
      "Época 40/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 1.9594840523495805e-06\n",
      "Batch Error : 3.0536966733052395e-06\n",
      "Batch Error : 1.1539029856066918e-06\n",
      "Batch Error : 3.416947265577619e-06\n",
      "Época 41/100, Pérdida: 3.0379793033403234e-06\n",
      "Época 41/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Época 42/100, Pérdida: 2.884305402027355e-06\n",
      "Época 42/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 6.098674475651933e-06\n",
      "Batch Error : 1.9566800801840145e-06\n",
      "Batch Error : 3.5239168028056156e-06\n",
      "Batch Error : 1.34109598093346e-06\n",
      "Época 43/100, Pérdida: 2.7558923593146732e-06\n",
      "Época 43/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 2.242574282718124e-06\n",
      "Batch Error : 1.2256058425919036e-06\n",
      "Época 44/100, Pérdida: 2.630944130754054e-06\n",
      "Época 44/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 3.180328576490865e-06\n",
      "Batch Error : 1.9678504941111896e-06\n",
      "Batch Error : 1.1390003464839538e-06\n",
      "Batch Error : 2.375762960582506e-06\n",
      "Batch Error : 1.3286312423588242e-06\n",
      "Época 45/100, Pérdida: 2.511083507748669e-06\n",
      "Época 45/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 2.3906486603664234e-06\n",
      "Batch Error : 2.5750655368028674e-06\n",
      "Batch Error : 6.053574566067255e-07\n",
      "Batch Error : 2.185773610108299e-06\n",
      "Época 46/100, Pérdida: 2.4037581738411973e-06\n",
      "Época 46/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 1.6624040881652036e-06\n",
      "Época 47/100, Pérdida: 2.3041975980269287e-06\n",
      "Época 47/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 2.8683919026661897e-06\n",
      "Batch Error : 1.1268923572060885e-06\n",
      "Batch Error : 2.301789209013805e-06\n",
      "Época 48/100, Pérdida: 2.2112508578833637e-06\n",
      "Época 48/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 4.732612978841644e-06\n",
      "Batch Error : 3.3274825455009704e-06\n",
      "Batch Error : 1.5618104498571483e-06\n",
      "Batch Error : 1.8393193386145867e-06\n",
      "Batch Error : 1.333640966549865e-06\n",
      "Época 49/100, Pérdida: 2.1153091928493362e-06\n",
      "Época 49/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 2.1494577140401816e-06\n",
      "Batch Error : 5.727616212425346e-07\n",
      "Época 50/100, Pérdida: 2.0416562890581e-06\n",
      "Época 50/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 2.4213002234318992e-06\n",
      "Batch Error : 2.68677808890061e-06\n",
      "Época 51/100, Pérdida: 1.954783460311366e-06\n",
      "Época 51/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 7.450544785569946e-07\n",
      "Batch Error : 1.7107761323131854e-06\n",
      "Batch Error : 1.473340489610564e-06\n",
      "Época 52/100, Pérdida: 1.8817065234417958e-06\n",
      "Época 52/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 2.695197963475948e-06\n",
      "Batch Error : 3.0916291962057585e-06\n",
      "Batch Error : 9.015138857648708e-07\n",
      "Batch Error : 1.387641191286093e-06\n",
      "Batch Error : 2.825535830197623e-06\n",
      "Época 53/100, Pérdida: 1.8044505485830024e-06\n",
      "Época 53/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 5.997690664116817e-07\n",
      "Batch Error : 1.6716672917027608e-06\n",
      "Batch Error : 2.137012188541121e-06\n",
      "Época 54/100, Pérdida: 1.7431504312330147e-06\n",
      "Época 54/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 3.2761984130047495e-06\n",
      "Época 55/100, Pérdida: 1.6717852633581938e-06\n",
      "Época 55/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 1.3159281024854863e-06\n",
      "Época 56/100, Pérdida: 1.609290116032847e-06\n",
      "Época 56/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 2.028383960350766e-06\n",
      "Batch Error : 1.1790289136115462e-06\n",
      "Batch Error : 1.4845106761640636e-06\n",
      "Época 57/100, Pérdida: 1.5676586987614552e-06\n",
      "Época 57/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 3.4280942600162234e-06\n",
      "Batch Error : 1.1920730003112112e-06\n",
      "Batch Error : 9.182786016026512e-07\n",
      "Batch Error : 1.0132595207323902e-06\n",
      "Batch Error : 1.2590975302373408e-06\n",
      "Época 58/100, Pérdida: 1.5029358019875934e-06\n",
      "Época 58/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 2.3207917365652975e-06\n",
      "Batch Error : 1.3559858871303732e-06\n",
      "Batch Error : 3.262341351728537e-06\n",
      "Época 59/100, Pérdida: 1.4451870869975366e-06\n",
      "Época 59/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 7.832385335859726e-07\n",
      "Batch Error : 1.5105940747162094e-06\n",
      "Batch Error : 1.2004583140878822e-06\n",
      "Batch Error : 1.7871705040306551e-06\n",
      "Época 60/100, Pérdida: 1.3962794445558694e-06\n",
      "Época 60/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 1.4397930954146432e-06\n",
      "Batch Error : 7.571618425572524e-07\n",
      "Época 61/100, Pérdida: 1.3502507174272414e-06\n",
      "Época 61/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 4.954624159836385e-07\n",
      "Época 62/100, Pérdida: 1.309220141953621e-06\n",
      "Época 62/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 8.00933946720761e-07\n",
      "Batch Error : 2.4873713755368954e-06\n",
      "Batch Error : 5.811427286062099e-07\n",
      "Batch Error : 9.48079900808807e-07\n",
      "Época 63/100, Pérdida: 1.25655148116462e-06\n",
      "Época 63/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 1.924941216202569e-06\n",
      "Batch Error : 1.757366476340394e-06\n",
      "Época 64/100, Pérdida: 1.2179769667313692e-06\n",
      "Época 64/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 1.5878765680099605e-06\n",
      "Batch Error : 5.895239496567228e-07\n",
      "Batch Error : 1.4379328376890044e-06\n",
      "Batch Error : 1.1073332188971108e-06\n",
      "Época 65/100, Pérdida: 1.1782821890596221e-06\n",
      "Época 65/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Época 66/100, Pérdida: 1.1411929706198486e-06\n",
      "Época 66/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 1.4146637568046572e-06\n",
      "Batch Error : 1.744318296914571e-06\n",
      "Batch Error : 2.4158027827070327e-06\n",
      "Época 67/100, Pérdida: 1.1029051175479129e-06\n",
      "Época 67/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 1.5971381799317896e-06\n",
      "Batch Error : 1.6828716979944147e-06\n",
      "Batch Error : 8.540150702174287e-07\n",
      "Batch Error : 1.2116361176595092e-06\n",
      "Batch Error : 5.941811878074077e-07\n",
      "Batch Error : 8.074479751485342e-07\n",
      "Batch Error : 6.198865776241291e-07\n",
      "Época 68/100, Pérdida: 1.0693183808457277e-06\n",
      "Época 68/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 5.541349423765496e-07\n",
      "Batch Error : 2.6290258574590553e-06\n",
      "Batch Error : 5.234015247879142e-07\n",
      "Batch Error : 3.827728392025165e-07\n",
      "Batch Error : 1.0858790346901515e-06\n",
      "Época 69/100, Pérdida: 1.0380518102104768e-06\n",
      "Época 69/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 6.668191758762987e-07\n",
      "Batch Error : 7.320164741031476e-07\n",
      "Época 70/100, Pérdida: 1.0083871459222603e-06\n",
      "Época 70/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 2.69045108325372e-06\n",
      "Época 71/100, Pérdida: 9.755273718634147e-07\n",
      "Época 71/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 6.612335141653602e-07\n",
      "Batch Error : 6.789313147237408e-07\n",
      "Época 72/100, Pérdida: 9.453779586598604e-07\n",
      "Época 72/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 7.953330509735679e-07\n",
      "Batch Error : 7.860296022954572e-07\n",
      "Batch Error : 6.677555575151928e-07\n",
      "Batch Error : 7.655339686607476e-07\n",
      "Batch Error : 9.778763114809408e-07\n",
      "Batch Error : 5.094309472042369e-07\n",
      "Época 73/100, Pérdida: 9.163665497218408e-07\n",
      "Época 73/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 6.7706616846408e-07\n",
      "Batch Error : 7.105956569830596e-07\n",
      "Batch Error : 6.277086299633083e-07\n",
      "Batch Error : 7.683335070396424e-07\n",
      "Época 74/100, Pérdida: 8.912064112893921e-07\n",
      "Época 74/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 4.898742531622702e-07\n",
      "Batch Error : 1.2321216900090803e-06\n",
      "Batch Error : 1.8364713696428225e-06\n",
      "Batch Error : 7.804384836163081e-07\n",
      "Batch Error : 8.431273386122484e-07\n",
      "Época 75/100, Pérdida: 8.647460110123888e-07\n",
      "Época 75/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 9.266493634640938e-07\n",
      "Batch Error : 1.9495089418342104e-06\n",
      "Época 76/100, Pérdida: 8.442274675197605e-07\n",
      "Época 76/100\n",
      "P= 0.7230172739881477\n",
      "R= 0.7046335396540753\n",
      "F1= 0.7122217119280837\n",
      "Acc= 0.7737226277372263\n",
      "Batch Error : 5.839355026182602e-07\n",
      "Batch Error : 8.549393442081055e-07\n",
      "Batch Error : 4.749732624986791e-07\n",
      "Época 77/100, Pérdida: 8.156185923122268e-07\n",
      "Época 77/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 7.655440867893049e-07\n",
      "Batch Error : 9.313112627751252e-07\n",
      "Batch Error : 1.0914950507867616e-06\n",
      "Época 78/100, Pérdida: 7.937789548420902e-07\n",
      "Época 78/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 5.848688715559547e-07\n",
      "Batch Error : 2.392748683632817e-06\n",
      "Época 79/100, Pérdida: 7.772681144537086e-07\n",
      "Época 79/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 1.1566900184334372e-06\n",
      "Batch Error : 8.586746389482869e-07\n",
      "Batch Error : 8.800871569292212e-07\n",
      "Época 80/100, Pérdida: 7.487128641514573e-07\n",
      "Época 80/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 6.863780299681821e-07\n",
      "Batch Error : 4.451693484952557e-07\n",
      "Batch Error : 6.100070208958641e-07\n",
      "Batch Error : 1.076592639037699e-06\n",
      "Batch Error : 6.881579679429706e-07\n",
      "Época 81/100, Pérdida: 7.268369684205591e-07\n",
      "Época 81/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 7.152486887207488e-07\n",
      "Batch Error : 7.450545922438323e-07\n",
      "Época 82/100, Pérdida: 7.051116861883453e-07\n",
      "Época 82/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 1.242368966813956e-06\n",
      "Batch Error : 3.0919741789148247e-07\n",
      "Batch Error : 3.5669540920935106e-07\n",
      "Batch Error : 7.171142328843416e-07\n",
      "Batch Error : 1.0188510941588902e-06\n",
      "Batch Error : 5.718291617995419e-07\n",
      "Batch Error : 3.147861775687488e-07\n",
      "Batch Error : 1.4546642432833323e-06\n",
      "Época 83/100, Pérdida: 6.882099385087525e-07\n",
      "Época 83/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 1.1734566669474589e-06\n",
      "Batch Error : 6.640237870669807e-07\n",
      "Batch Error : 5.066378321316733e-07\n",
      "Batch Error : 3.911546855306369e-07\n",
      "Batch Error : 7.366719501078478e-07\n",
      "Época 84/100, Pérdida: 6.689690787527132e-07\n",
      "Época 84/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 7.636707550773281e-07\n",
      "Batch Error : 1.9147369130223524e-06\n",
      "Época 85/100, Pérdida: 6.518868659590038e-07\n",
      "Época 85/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 2.3408335891872412e-07\n",
      "Época 86/100, Pérdida: 6.325005174325082e-07\n",
      "Época 86/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 3.883607462284999e-07\n",
      "Batch Error : 3.0733585276720987e-07\n",
      "Batch Error : 8.940623956732452e-07\n",
      "Época 87/100, Pérdida: 6.183593365778568e-07\n",
      "Época 87/100\n",
      "P= 0.7263468494193794\n",
      "R= 0.7063400242957135\n",
      "F1= 0.7144971307762005\n",
      "Acc= 0.7761557177615572\n",
      "Batch Error : 4.4237586394046957e-07\n",
      "Batch Error : 4.6007247078705404e-07\n",
      "Batch Error : 6.603034421459597e-07\n",
      "Época 88/100, Pérdida: 6.000333986000034e-07\n",
      "Época 88/100\n",
      "P= 0.7294950195435632\n",
      "R= 0.7105773124313067\n",
      "F1= 0.718410492316727\n",
      "Acc= 0.7785888077858881\n",
      "Batch Error : 5.327132726051786e-07\n",
      "Batch Error : 1.0523822311370168e-06\n",
      "Batch Error : 4.107112658857659e-07\n",
      "Época 89/100, Pérdida: 5.858629933676451e-07\n",
      "Época 89/100\n",
      "P= 0.7294950195435632\n",
      "R= 0.7105773124313067\n",
      "F1= 0.718410492316727\n",
      "Acc= 0.7785888077858881\n",
      "Época 90/100, Pérdida: 5.684029330582078e-07\n",
      "Época 90/100\n",
      "P= 0.7294950195435632\n",
      "R= 0.7105773124313067\n",
      "F1= 0.718410492316727\n",
      "Acc= 0.7785888077858881\n",
      "Batch Error : 1.017893964672112e-06\n",
      "Época 91/100, Pérdida: 5.562083874243901e-07\n",
      "Época 91/100\n",
      "P= 0.7294950195435632\n",
      "R= 0.7105773124313067\n",
      "F1= 0.718410492316727\n",
      "Acc= 0.7785888077858881\n",
      "Batch Error : 5.578577315645816e-07\n",
      "Época 92/100, Pérdida: 5.411989370674652e-07\n",
      "Época 92/100\n",
      "P= 0.7294950195435632\n",
      "R= 0.7105773124313067\n",
      "F1= 0.718410492316727\n",
      "Acc= 0.7785888077858881\n",
      "Batch Error : 1.0290899581377744e-06\n",
      "Batch Error : 9.08964295831538e-07\n",
      "Época 93/100, Pérdida: 5.268422245896138e-07\n",
      "Época 93/100\n",
      "P= 0.7294950195435632\n",
      "R= 0.7105773124313067\n",
      "F1= 0.718410492316727\n",
      "Acc= 0.7785888077858881\n",
      "Batch Error : 3.166491922002024e-07\n",
      "Época 94/100, Pérdida: 5.1509375891038e-07\n",
      "Época 94/100\n",
      "P= 0.7294950195435632\n",
      "R= 0.7105773124313067\n",
      "F1= 0.718410492316727\n",
      "Acc= 0.7785888077858881\n",
      "Batch Error : 8.288707249448635e-07\n",
      "Batch Error : 6.593742796212609e-07\n",
      "Batch Error : 4.1350486412738974e-07\n",
      "Batch Error : 1.344791030533088e-06\n",
      "Batch Error : 4.3026997786910215e-07\n",
      "Batch Error : 2.156601794922608e-07\n",
      "Época 95/100, Pérdida: 5.002944545077329e-07\n",
      "Época 95/100\n",
      "P= 0.7294950195435632\n",
      "R= 0.7105773124313067\n",
      "F1= 0.718410492316727\n",
      "Acc= 0.7785888077858881\n",
      "Batch Error : 9.219908179147751e-07\n",
      "Batch Error : 2.4214341465267353e-07\n",
      "Batch Error : 5.364400408325309e-07\n",
      "Batch Error : 3.164458917126467e-07\n",
      "Época 96/100, Pérdida: 4.88130223322185e-07\n",
      "Época 96/100\n",
      "P= 0.7294950195435632\n",
      "R= 0.7105773124313067\n",
      "F1= 0.718410492316727\n",
      "Acc= 0.7785888077858881\n",
      "Batch Error : 2.1513525894079066e-07\n",
      "Batch Error : 1.8905836896010442e-07\n",
      "Batch Error : 2.477308669313061e-07\n",
      "Época 97/100, Pérdida: 4.776885138481106e-07\n",
      "Época 97/100\n",
      "P= 0.7294950195435632\n",
      "R= 0.7105773124313067\n",
      "F1= 0.718410492316727\n",
      "Acc= 0.7785888077858881\n",
      "Batch Error : 4.0698660086491145e-07\n",
      "Batch Error : 1.825390540943772e-07\n",
      "Época 98/100, Pérdida: 4.6668392344443044e-07\n",
      "Época 98/100\n",
      "P= 0.7294950195435632\n",
      "R= 0.7105773124313067\n",
      "F1= 0.718410492316727\n",
      "Acc= 0.7785888077858881\n",
      "Batch Error : 4.6565725142500014e-07\n",
      "Batch Error : 4.6379702212107077e-07\n",
      "Batch Error : 3.166489364048175e-07\n",
      "Batch Error : 1.9837136733258376e-07\n",
      "Época 99/100, Pérdida: 4.5302435041619527e-07\n",
      "Época 99/100\n",
      "P= 0.7294950195435632\n",
      "R= 0.7105773124313067\n",
      "F1= 0.718410492316727\n",
      "Acc= 0.7785888077858881\n",
      "Batch Error : 4.423760060490167e-07\n",
      "Batch Error : 6.230502549442463e-07\n",
      "Época 100/100, Pérdida: 4.4602921840233366e-07\n",
      "Época 100/100\n",
      "P= 0.7294950195435632\n",
      "R= 0.7105773124313067\n",
      "F1= 0.718410492316727\n",
      "Acc= 0.7785888077858881\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Establecer los parámetros de la red\n",
    "\n",
    "# Parámetros de la red\n",
    "input_size =  X_train_tfidf.shape[1]\n",
    "\n",
    "output_size = 2   # 2 clases\n",
    "\n",
    "epochs = 100 # variar el número de épocas, para probar que funciona la programación \n",
    "                 # solo usar 2 épocas, para entrenamiento total usar por ejemplo 1000 épocas\n",
    "learning_rate = 0.01 # Generalmente se usan learning rate pequeños (0.001), \n",
    "\n",
    "# Se recomiendan tamaños de batch_size potencias de 2: 16, 32, 64, 128, 256\n",
    "# Entre mayor el número más cantidad de memoria se requiere para el procesamiento\n",
    "batch_size = 128 # definir el tamaño del lote de procesamiento \n",
    "\n",
    "\n",
    "# TODO: Convertir los datos de entrenamiento y etiquetas a tensores  de PyTorch\n",
    "\n",
    "X_train_t = torch.from_numpy(X_train_tfidf)\n",
    "Y_train_t = Y_train_one_hot\n",
    "\n",
    "X_val_t = torch.from_numpy(X_val_tfidf)\n",
    "\n",
    "# Crear la red\n",
    "model = MLP(input_size, output_size)\n",
    "\n",
    "# Definir la función de pérdida\n",
    "# Mean Square Error (MSE)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = nn.BCELoss() \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# Definir el optimizador\n",
    "#Parámetros del optimizador: parámetros del modelo y learning rate \n",
    "# Stochastic Gradient Descent (SGD)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Entrenamiento\n",
    "print(\"Iniciando entrenamiento en PyTorch\")\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "# Poner el modelo en modo de entrenamiento\n",
    "    model.train()  \n",
    "    lossTotal = 0\n",
    "    #definir el batch_size\n",
    "    dataloader = create_minibatches(X_train_t, Y_train_t, batch_size=batch_size)\n",
    "    for X_tr, y_tr in dataloader:\n",
    "        # inicializar los gradientes en cero para cada época\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Propagación hacia adelante\n",
    "        y_pred = model(X_tr)  #invoca al método forward de la clase MLP\n",
    "        # Calcular el error MSE\n",
    "        loss = criterion(y_pred, y_tr)\n",
    "        #Acumular el error \n",
    "        lossTotal += loss.item()\n",
    "        \n",
    "        # Propagación hacia atrás: cálculo de los gradientes de los pesos y bias\n",
    "        loss.backward()\n",
    "        \n",
    "        # actualización de los pesos: regla de actualización basado en el gradiente:\n",
    "        #  W = W - learning_rate * dE/dW\n",
    "        optimizer.step()\n",
    "        if np.random.random() < 0.1:\n",
    "            print(f\"Batch Error : {loss.item()}\")\n",
    "\n",
    "    print(f\"Época {epoch+1}/{epochs}, Pérdida: {lossTotal/len(dataloader)}\")\n",
    "    \n",
    "    # Evalúa el modelo con el conjunto de validación\n",
    "    model.eval()  # Establecer el modo del modelo a \"evaluación\"\n",
    "    with torch.no_grad():  # No  calcular gradientes \n",
    "        y_pred = model(X_val_t)\n",
    "        # Aplica softmax para obtener las probabilidades en la evaluación\n",
    "        y_pred = torch.softmax(y_pred, dim=1)\n",
    "        # Obtiene una única clase, la más probable\n",
    "        y_pred = torch.argmax(y_pred, dim=1)        \n",
    "        print(f\"Época {epoch+1}/{epochs}\")\n",
    "        print(\"P=\", precision_score(Y_val, y_pred, average='macro'))\n",
    "        print(\"R=\", recall_score(Y_val, y_pred, average='macro'))\n",
    "        print(\"F1=\", f1_score(Y_val, y_pred, average='macro'))\n",
    "        print(\"Acc=\", accuracy_score(Y_val, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modo para predicción de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1,  ..., 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Transformar el dataset de test con los mismos preprocesamientos y al  espacio de \n",
    "# representación vectorial que el modelo entrenado, es decir, al espacio de la matriz TFIDF\n",
    "\n",
    "# Convertir los datos de prueba a tensores de PyTorch\n",
    "\n",
    "X_test_tfidf = vec_tfidf.transform(X_test)\n",
    "\n",
    "# Convertir a matriz densa de tipo de dato float32 (tipo de dato por default en Pytorch)\n",
    "X_test_tfidf = X_test_tfidf.toarray().astype(np.float32)\n",
    "X_t = torch.from_numpy(X_test_tfidf)\n",
    "\n",
    "# Desactivar el comportamiento de modo de  entrenamiento: por ejemplo, capas como Dropout\n",
    "model.eval()  # Establecer el modo del modelo a \"evaluación\"\n",
    "\n",
    "with torch.no_grad():  # No  calcular gradientes \n",
    "    y_pred_test= model(X_t)\n",
    "\n",
    "# y_test_pred contiene las predicciones\n",
    "\n",
    "# Obtener la clase real\n",
    "y_pred_test = torch.argmax(y_pred_test, dim=1)\n",
    "\n",
    "print(y_pred_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[159 137]\n",
      " [ 88 643]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6437    0.5372    0.5856       296\n",
      "           1     0.8244    0.8796    0.8511       731\n",
      "\n",
      "    accuracy                         0.7809      1027\n",
      "   macro avg     0.7340    0.7084    0.7184      1027\n",
      "weighted avg     0.7723    0.7809    0.7746      1027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Evaluar el modelo con las predicciones obtenidas y las etiquetas esperadas: \n",
    "# classification_report y  matriz de confusión (métricas Precisión, Recall, F1-measaure, Accuracy)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "print(confusion_matrix(Y_test, y_pred_test))\n",
    "print(classification_report(Y_test, y_pred_test, digits=4, zero_division='warn'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de datos nuevos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aggressive' 'aggressive' 'nonaggressive']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_new_data = [\"Ese perro me robo mis cosas\", \"ese hdp se llevo el dinero\", \"mi app de calendario no sirve\"]\n",
    "x_new_data_tfidf = vec_tfidf.transform(x_new_data)\n",
    "# Convertir a matriz densa de tipo de dato float32 (tipo de dato por default en Pytorch)\n",
    "x_new_data_tfidf = x_new_data_tfidf.toarray().astype(np.float32)\n",
    "X_new_t = torch.from_numpy(x_new_data_tfidf)\n",
    "\n",
    "\n",
    "model.eval()  # Establecer el modo del modelo a \"evaluación\"\n",
    "with torch.no_grad():  # No  calcular gradientes \n",
    "    y_pred = model(X_new_t)\n",
    "    y_pred = torch.argmax(y_pred, dim=1)\n",
    "    print(le.inverse_transform(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio.  Modificar la red neuronal MLP con las siguientes características:\n",
    "- ## Arquitectura:\n",
    "    - ### 4 capas ocultas\n",
    "    - ### 2 salidas\n",
    "    - ### Funciones de activación en capas ocultas ELU\n",
    "    - ### Número de neuronas por capa oculta a su consideración\n",
    "- ## Prepocesamiento:\n",
    "    - ### Normalización\n",
    "    - ### Repesentación de características: unigramas sin STOPWORDS y con stemming\n",
    "    - ### Pesado TF-IDF\n",
    "\n",
    "- ## Evaluación del rendimiento del modelo: \n",
    "    - ### 1. Paticiones train (80%), test (20%), validación (10% del train)\n",
    "    - ### 2. Validación cruzada k-folds = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de ejemplos de entrenamiento\n",
      "klass\n",
      "nonaggressive    3655\n",
      "aggressive       1477\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACIÓN INICIAL Y CARGA DE DATOS\n",
    "# ============================================================================\n",
    "\n",
    "# Semilla para reproducibilidad - asegura que los resultados sean los mismos en cada ejecución\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# Cargar datos desde archivo JSON\n",
    "dataset = pd.read_json(\"./data_aggressiveness_es.json\", lines=True)\n",
    "print(\"Total de ejemplos de entrenamiento\")\n",
    "print(dataset.klass.value_counts())  # Muestra la distribución de clases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extraer textos y etiquetas\n",
    "X = dataset['text'].to_numpy()  # Textos como array de numpy\n",
    "Y = dataset['klass'].to_numpy()  # Etiquetas como array de numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PREPROCESAMIENTO DE TEXTO\n",
    "# ============================================================================\n",
    "\n",
    "# Configuración de recursos para NLP\n",
    "_STOPWORDS = stopwords.words(\"spanish\")  # Lista de palabras vacías en español\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "NUMBERS = \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)  # Símbolos a eliminar\n",
    "\n",
    "def normaliza_texto(input_str, punct=False, accents=False, num=False, max_dup=2):\n",
    "    \"\"\"\n",
    "    Normaliza el texto eliminando puntuación, acentos, números y duplicados excesivos\n",
    "    \"\"\"\n",
    "    # Normalizar caracteres Unicode\n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    \n",
    "    # Procesar cada carácter del texto\n",
    "    for c in nfkd_f:\n",
    "        # Eliminar números si num=False\n",
    "        if not num and c in NUMBERS:\n",
    "            continue\n",
    "        # Eliminar puntuación si punct=False\n",
    "        if not punct and c in SKIP_SYMBOLS:\n",
    "            continue\n",
    "        # Eliminar acentos si accents=False\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        # Controlar caracteres duplicados consecutivos\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    \n",
    "    # Reconstruir texto y normalizar espacios\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n",
    "def mi_preprocesamiento(texto):\n",
    "    \"\"\"\n",
    "    Preprocesamiento principal: tokenización, minúsculas y normalización\n",
    "    \"\"\"\n",
    "    # Convertir a minúsculas y tokenizar\n",
    "    tokens = word_tokenize(texto.lower())\n",
    "    texto = \" \".join(tokens)  # Reunir tokens en texto\n",
    "    texto = normaliza_texto(texto)  # Aplicar normalización\n",
    "    return texto\n",
    "\n",
    "def mi_tokenizador(texto):\n",
    "    \"\"\"\n",
    "    Tokenizador personalizado que elimina stopwords\n",
    "    \"\"\"\n",
    "    texto = [t for t in texto.split() if t not in _STOPWORDS]\n",
    "    return texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases: ['aggressive' 'nonaggressive']\n",
      "Clases codificadas: [0 1]\n",
      "Train: 3694 | Val: 411 | Test: 1027\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# PREPARACIÓN DE ETIQUETAS Y PARTICIONES\n",
    "# ============================================================================\n",
    "\n",
    "# Codificar etiquetas de texto a números (ej: \"agresivo\" -> 0, \"no_agresivo\" -> 1)\n",
    "le = LabelEncoder()\n",
    "Y_encoded = le.fit_transform(Y)# Codificación ordinal de etiquetas\n",
    "print(\"Clases:\", le.classes_)#  Mostrar las clases originales\n",
    "print(\"Clases codificadas:\", le.transform(le.classes_))\n",
    "\n",
    "# 1. PRIMERA PARTICION: 80% entrenamiento+validación, 20% test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y_encoded, test_size=0.2, stratify=Y_encoded, random_state=42\n",
    ")\n",
    "\n",
    "# 2. SEGUNDA PARTICION: Del 80% anterior, 90% entrenamiento, 10% validación\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train, Y_train, test_size=0.1, stratify=Y_train, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRANSFORMACIÓN TF-IDF (REPRESENTACIÓN NUMÉRICA DE TEXTOS)\n",
    "# ============================================================================\n",
    "\n",
    "# Crear vectorizador TF-IDF para convertir textos a vectores numéricos\n",
    "vec_tfidf = TfidfVectorizer(\n",
    "    analyzer=\"word\", \n",
    "    preprocessor=mi_preprocesamiento,  # Nuestra función de preprocesamiento\n",
    "    tokenizer=mi_tokenizador,          # Nuestro tokenizador sin stopwords\n",
    "    ngram_range=(1,1)                  # Usar solo unigramas (palabras individuales)\n",
    ")\n",
    "\n",
    "# Transformar textos a matriz TF-IDF (entrenar con train, aplicar a val y test)\n",
    "X_train_tfidf = vec_tfidf.fit_transform(X_train).toarray().astype(np.float32)# Aqui se usa fit_transform porque es el conjunto de entrenamiento\n",
    "X_val_tfidf = vec_tfidf.transform(X_val).toarray().astype(np.float32)#Aqui se usa transform porque es el conjunto de validación\n",
    "X_test_tfidf = vec_tfidf.transform(X_test).toarray().astype(np.float32)#Aqui se usa transform porque es el conjunto de prueba\n",
    "\n",
    "# Convertir arrays numpy a tensores PyTorch\n",
    "X_train_tensor = torch.FloatTensor(X_train_tfidf)#\n",
    "X_val_tensor = torch.FloatTensor(X_val_tfidf)\n",
    "X_test_tensor = torch.FloatTensor(X_test_tfidf)\n",
    "Y_train_tensor = torch.LongTensor(Y_train)\n",
    "Y_val_tensor = torch.LongTensor(Y_val)\n",
    "Y_test_tensor = torch.LongTensor(Y_test)\n",
    "\n",
    "# ============================================================================\n",
    "# DEFINICIÓN DEL MODELO MLP\n",
    "# ============================================================================\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Red Neuronal Multicapa con 4 capas ocultas y activaciones ELU\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size=2):\n",
    "        super().__init__()\n",
    "        # Capas completamente conectadas con arquitectura descendente\n",
    "        self.fc1 = nn.Linear(input_size, 256)  # Capa entrada -> 256 neuronas\n",
    "        self.act1 = nn.ELU()                   # Activación ELU\n",
    "        self.fc2 = nn.Linear(256, 128)         # 256 -> 128 neuronas\n",
    "        self.act2 = nn.ELU()\n",
    "        self.fc3 = nn.Linear(128, 64)          # 128 -> 64 neuronas\n",
    "        self.act3 = nn.ELU()\n",
    "        self.fc4 = nn.Linear(64, 32)           # 64 -> 32 neuronas\n",
    "        self.act4 = nn.ELU()\n",
    "        self.output = nn.Linear(32, output_size)  # Capa salida (2 clases)\n",
    "        \n",
    "        # Inicialización de pesos\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Inicializa los pesos de todas las capas con Xavier Uniform\"\"\"\n",
    "        layers = [self.fc1, self.fc2, self.fc3, self.fc4, self.output]\n",
    "        for layer in layers:\n",
    "            nn.init.xavier_uniform_(layer.weight)  # Inicialización Xavier\n",
    "            if layer.bias is not None:\n",
    "                nn.init.zeros_(layer.bias)         # Bias en cero\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Propagación hacia adelante a través de la red\"\"\"\n",
    "        x = self.act1(self.fc1(X))  # Capa 1: Linear + ELU\n",
    "        x = self.act2(self.fc2(x))  # Capa 2: Linear + ELU\n",
    "        x = self.act3(self.fc3(x))  # Capa 3: Linear + ELU\n",
    "        x = self.act4(self.fc4(x))  # Capa 4: Linear + ELU\n",
    "        x = self.output(x)          # Capa salida (sin activación - CrossEntropy lo maneja)\n",
    "        return x\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCIÓN DE ENTRENAMIENTO\n",
    "# ============================================================================\n",
    "\n",
    "def entrenar_modelo(modelo, X_train, y_train, X_val, y_val, epochs=100, lr=0.001):\n",
    "    \"\"\"\n",
    "    Entrena el modelo y evalúa en conjunto de validación\n",
    "    \"\"\"\n",
    "    # Definir función de pérdida y optimizador\n",
    "    criterion = nn.CrossEntropyLoss()  # Pérdida para clasificación\n",
    "    optimizer = optim.Adam(modelo.parameters(), lr=lr)  # Optimizador Adam\n",
    "    \n",
    "    # Ciclo de entrenamiento\n",
    "    for epoch in range(epochs):\n",
    "        modelo.train()  # Modo entrenamiento\n",
    "        optimizer.zero_grad()  # Limpiar gradientes anteriores\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = modelo(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validación del modelo\n",
    "    modelo.eval()  # Modo evaluación\n",
    "    with torch.no_grad():  # Desactivar cálculo de gradientes para evaluación\n",
    "        val_outputs = modelo(X_val)\n",
    "        _, val_preds = torch.max(val_outputs, 1)  # Obtener predicciones (índice de clase)\n",
    "        val_acc = accuracy_score(y_val.numpy(), val_preds.numpy())  # Calcular accuracy\n",
    "    \n",
    "    return modelo, val_acc\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDACIÓN CRUZADA K-FOLD\n",
    "# ============================================================================\n",
    "\n",
    "def validacion_cruzada_kfold(X, y, k_folds=5, epochs=50):\n",
    "    \"\"\"\n",
    "    Realiza validación cruzada con k folds para evaluar robustez del modelo\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    fold_accuracies = []  # Almacenar accuracy de cada fold\n",
    "    \n",
    "    # Iterar sobre cada fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        print(f\"Fold {fold+1}/{k_folds}\")\n",
    "        \n",
    "        # Preparar datos del fold actual\n",
    "        X_train_fold = torch.FloatTensor(X[train_idx])\n",
    "        X_val_fold = torch.FloatTensor(X[val_idx])\n",
    "        y_train_fold = torch.LongTensor(y[train_idx])\n",
    "        y_val_fold = torch.LongTensor(y[val_idx])\n",
    "        \n",
    "        # Crear y entrenar modelo para este fold\n",
    "        modelo = MLP(input_size=X_train_fold.shape[1])\n",
    "        modelo, val_acc = entrenar_modelo(modelo, X_train_fold, y_train_fold, \n",
    "                                        X_val_fold, y_val_fold, epochs)\n",
    "        \n",
    "        fold_accuracies.append(val_acc)\n",
    "        print(f\"Accuracy Fold {fold+1}: {val_acc:.4f}\")\n",
    "    \n",
    "    # Estadísticas de la validación cruzada\n",
    "    print(f\"\\nK-Fold Accuracy: {np.mean(fold_accuracies):.4f} (+/- {np.std(fold_accuracies):.4f})\")\n",
    "    return fold_accuracies\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUACIÓN COMPLETA DEL MODELO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== ENTRENAMIENTO CON PARTICIÓN 80-20-10 ===\")\n",
    "# Crear y entrenar modelo final con la partición principal\n",
    "modelo_final = MLP(input_size=X_train_tensor.shape[1])\n",
    "modelo_entrenado, val_acc = entrenar_modelo(\n",
    "    modelo_final, X_train_tensor, Y_train_tensor, X_val_tensor, Y_val_tensor\n",
    ")\n",
    "\n",
    "# Evaluar modelo en conjunto de test\n",
    "modelo_entrenado.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = modelo_entrenado(X_test_tensor)\n",
    "    _, test_preds = torch.max(test_outputs, 1)\n",
    "    test_acc = accuracy_score(Y_test_tensor.numpy(), test_preds.numpy())\n",
    "\n",
    "print(f\"Accuracy Validación: {val_acc:.4f}\")\n",
    "print(f\"Accuracy Test: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\n=== VALIDACIÓN CRUZADA K-FOLD (k=5) ===\")\n",
    "# Preparar datos completos para K-Fold (usando todo el dataset)\n",
    "X_completo_tfidf = vec_tfidf.transform(X).toarray().astype(np.float32)\n",
    "kfold_accuracies = validacion_cruzada_kfold(X_completo_tfidf, Y_encoded)\n",
    "\n",
    "# ============================================================================\n",
    "# RESULTADOS FINALES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== RESULTADOS FINALES ===\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"K-Fold Mean Accuracy: {np.mean(kfold_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuración de semilla para reproducibilidad\n",
    "random_state = 42\n",
    "torch.manual_seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# Cargar los datos\n",
    "dataset = pd.read_json(\"./data_aggressiveness_es.json\", lines=True)\n",
    "print(\"Total de ejemplos de entrenamiento\")\n",
    "print(dataset.klass.value_counts())\n",
    "\n",
    "X = dataset['text'].to_numpy()\n",
    "Y = dataset['klass'].to_numpy()\n",
    "\n",
    "# Configuración para preprocesamiento en español\n",
    "_STOPWORDS = stopwords.words(\"spanish\")\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "# Normalización del texto\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "NUMBERS = \"0123456789\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normaliza_texto(input_str, punct=False, accents=False, num=False, max_dup=2):\n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not num:\n",
    "            if c in NUMBERS:\n",
    "                continue\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    texto = unicodedata.normalize('NFKD', \"\".join(n_str))\n",
    "    texto = re.sub(r'(\\s)+', r' ', texto.strip(), flags=re.IGNORECASE)\n",
    "    return texto\n",
    "\n",
    "# Preprocesamiento personalizado con stemming\n",
    "def mi_preprocesamiento(texto):\n",
    "    tokens = word_tokenize(texto.lower())\n",
    "    texto = \" \".join(tokens)\n",
    "    texto = normaliza_texto(texto)\n",
    "    return texto\n",
    "\n",
    "# Tokenizador personalizado con stemming y eliminación de stopwords\n",
    "def mi_tokenizador(texto):\n",
    "    tokens = texto.split()\n",
    "    # Filtrar stopwords y aplicar stemming\n",
    "    tokens_filtrados = []\n",
    "    for t in tokens:\n",
    "        if t not in _STOPWORDS:\n",
    "            token_stemmed = stemmer.stem(t)\n",
    "            tokens_filtrados.append(token_stemmed)\n",
    "    return tokens_filtrados\n",
    "\n",
    "# Codificar las etiquetas\n",
    "le = LabelEncoder()\n",
    "Y_encoded = le.fit_transform(Y)\n",
    "print(\"Clases:\")\n",
    "print(le.classes_)\n",
    "print(\"Clases codificadas:\")\n",
    "print(le.transform(le.classes_))\n",
    "\n",
    "# Dividir el conjunto de datos (80% train, 20% test)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y_encoded, test_size=0.2, stratify=Y_encoded, random_state=random_state\n",
    ")\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en train (90%) y validación (10%)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train, Y_train, test_size=0.1, stratify=Y_train, random_state=random_state\n",
    ")\n",
    "\n",
    "# Crear la matriz Documento-Término con TF-IDF (unigramas)\n",
    "vec_tfidf = TfidfVectorizer(\n",
    "    analyzer=\"word\", \n",
    "    preprocessor=mi_preprocesamiento, \n",
    "    tokenizer=mi_tokenizador,  \n",
    "    ngram_range=(1, 1)  # Unigramas\n",
    ")\n",
    "X_train_tfidf = vec_tfidf.fit_transform(X_train)\n",
    "\n",
    "# Aplicar normalización a los datos TF-IDF\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False para matrices sparse\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Codificación one-hot\n",
    "Y_train_one_hot = nn.functional.one_hot(torch.from_numpy(Y_train), num_classes=NUM_CLASSES).float()\n",
    "Y_test_one_hot = nn.functional.one_hot(torch.from_numpy(Y_test), num_classes=NUM_CLASSES).float()\n",
    "Y_val_one_hot = nn.functional.one_hot(torch.from_numpy(Y_val), num_classes=NUM_CLASSES).float()\n",
    "\n",
    "# Convertir a matrices densas\n",
    "X_train_tfidf = X_train_tfidf.toarray().astype(np.float32)\n",
    "X_val_tfidf = vec_tfidf.transform(X_val)\n",
    "X_val_tfidf = scaler.transform(X_val_tfidf)\n",
    "X_val_tfidf = X_val_tfidf.toarray().astype(np.float32)\n",
    "\n",
    "# Definir la nueva arquitectura MLP con 4 capas ocultas y ELU\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        # 4 capas ocultas con diferentes tamaños\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.act1 = nn.ELU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.act2 = nn.ELU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.act3 = nn.ELU()\n",
    "        \n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.act4 = nn.ELU()\n",
    "        \n",
    "        self.output = nn.Linear(64, output_size)\n",
    "        \n",
    "        # Inicialización de pesos\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for layer in [self.fc1, self.fc2, self.fc3, self.fc4, self.output]:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            if layer.bias is not None:\n",
    "                nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        x = self.act1(self.fc1(X))\n",
    "        x = self.act2(self.fc2(x))\n",
    "        x = self.act3(self.fc3(x))\n",
    "        x = self.act4(self.fc4(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Función para crear minibatches\n",
    "def create_minibatches(X, Y, batch_size):\n",
    "    dataset = TensorDataset(X, Y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "# Función para entrenar y evaluar el modelo\n",
    "def train_and_evaluate_model(X_train_t, Y_train_t, X_val_t, Y_val, input_size, output_size=2):\n",
    "    # Parámetros de entrenamiento\n",
    "    epochs = 100\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 128\n",
    "    \n",
    "    # Crear modelo\n",
    "    model = MLP(input_size, output_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Listas para almacenar métricas\n",
    "    train_losses = []\n",
    "    val_metrics = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loss_total = 0\n",
    "        dataloader = create_minibatches(X_train_t, Y_train_t, batch_size=batch_size)\n",
    "        \n",
    "        for X_tr, y_tr in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_tr)\n",
    "            loss = criterion(y_pred, y_tr)\n",
    "            loss_total += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_loss = loss_total / len(dataloader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        # Evaluación en validación\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred_val = model(X_val_t)\n",
    "            y_pred_val = torch.softmax(y_pred_val, dim=1)\n",
    "            y_pred_classes = torch.argmax(y_pred_val, dim=1)\n",
    "            \n",
    "            precision = precision_score(Y_val, y_pred_classes, average='macro')\n",
    "            recall = recall_score(Y_val, y_pred_classes, average='macro')\n",
    "            f1 = f1_score(Y_val, y_pred_classes, average='macro')\n",
    "            acc = accuracy_score(Y_val, y_pred_classes)\n",
    "            \n",
    "            val_metrics.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'accuracy': acc\n",
    "            })\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Época {epoch+1}/{epochs}, Pérdida: {avg_loss:.4f}\")\n",
    "            print(f\"  P={precision:.4f}, R={recall:.4f}, F1={f1:.4f}, Acc={acc:.4f}\")\n",
    "    \n",
    "    return model, train_losses, val_metrics\n",
    "\n",
    "# Entrenamiento inicial con división train/val\n",
    "print(\"=== ENTRENAMIENTO INICIAL ===\")\n",
    "input_size = X_train_tfidf.shape[1]\n",
    "X_train_t = torch.from_numpy(X_train_tfidf)\n",
    "X_val_t = torch.from_numpy(X_val_tfidf)\n",
    "\n",
    "model, train_losses, val_metrics = train_and_evaluate_model(\n",
    "    X_train_t, Y_train_one_hot, X_val_t, Y_val, input_size\n",
    ")\n",
    "\n",
    "# Validación cruzada k-folds (k=5)\n",
    "print(\"\\n=== VALIDACIÓN CRUZADA (5-FOLDS) ===\")\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "fold_metrics = []\n",
    "\n",
    "# Combinar train y validation para k-fold\n",
    "X_train_full = np.concatenate([X_train, X_val])\n",
    "Y_train_full = np.concatenate([Y_train, Y_val])\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_full)):\n",
    "    print(f\"\\n--- Fold {fold + 1}/5 ---\")\n",
    "    \n",
    "    # Preprocesamiento para este fold\n",
    "    X_fold_train, X_fold_val = X_train_full[train_idx], X_train_full[val_idx]\n",
    "    Y_fold_train, Y_fold_val = Y_train_full[train_idx], Y_train_full[val_idx]\n",
    "    \n",
    "    # TF-IDF para el fold\n",
    "    X_fold_train_tfidf = vec_tfidf.transform(X_fold_train)\n",
    "    X_fold_train_tfidf = scaler.transform(X_fold_train_tfidf)\n",
    "    X_fold_train_tfidf = X_fold_train_tfidf.toarray().astype(np.float32)\n",
    "    \n",
    "    X_fold_val_tfidf = vec_tfidf.transform(X_fold_val)\n",
    "    X_fold_val_tfidf = scaler.transform(X_fold_val_tfidf)\n",
    "    X_fold_val_tfidf = X_fold_val_tfidf.toarray().astype(np.float32)\n",
    "    \n",
    "    # Codificación one-hot\n",
    "    Y_fold_train_one_hot = nn.functional.one_hot(torch.from_numpy(Y_fold_train), num_classes=NUM_CLASSES).float()\n",
    "    \n",
    "    # Entrenar modelo para este fold\n",
    "    X_fold_train_t = torch.from_numpy(X_fold_train_tfidf)\n",
    "    X_fold_val_t = torch.from_numpy(X_fold_val_tfidf)\n",
    "    \n",
    "    fold_model, _, fold_val_metrics = train_and_evaluate_model(\n",
    "        X_fold_train_t, Y_fold_train_one_hot, X_fold_val_t, Y_fold_val, input_size\n",
    "    )\n",
    "    \n",
    "    # Guardar métricas del último epoch\n",
    "    best_metrics = fold_val_metrics[-1]\n",
    "    fold_metrics.append(best_metrics)\n",
    "    print(f\"Fold {fold + 1} - P: {best_metrics['precision']:.4f}, R: {best_metrics['recall']:.4f}, F1: {best_metrics['f1']:.4f}\")\n",
    "\n",
    "# Métricas promedio de validación cruzada\n",
    "avg_precision = np.mean([m['precision'] for m in fold_metrics])\n",
    "avg_recall = np.mean([m['recall'] for m in fold_metrics])\n",
    "avg_f1 = np.mean([m['f1'] for m in fold_metrics])\n",
    "avg_accuracy = np.mean([m['accuracy'] for m in fold_metrics])\n",
    "\n",
    "print(f\"\\n=== RESULTADOS VALIDACIÓN CRUZADA ===\")\n",
    "print(f\"Precisión promedio: {avg_precision:.4f}\")\n",
    "print(f\"Recall promedio: {avg_recall:.4f}\")\n",
    "print(f\"F1-score promedio: {avg_f1:.4f}\")\n",
    "print(f\"Accuracy promedio: {avg_accuracy:.4f}\")\n",
    "\n",
    "# Evaluación final en conjunto de test\n",
    "print(\"\\n=== EVALUACIÓN EN CONJUNTO DE TEST ===\")\n",
    "X_test_tfidf = vec_tfidf.transform(X_test)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "X_test_tfidf = X_test_tfidf.toarray().astype(np.float32)\n",
    "X_test_t = torch.from_numpy(X_test_tfidf)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(X_test_t)\n",
    "    y_pred_test_classes = torch.argmax(y_pred_test, dim=1)\n",
    "\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_matrix(Y_test, y_pred_test_classes))\n",
    "print(\"\\nReporte de clasificación:\")\n",
    "print(classification_report(Y_test, y_pred_test_classes, digits=4))\n",
    "\n",
    "# Prueba con nuevos datos\n",
    "print(\"\\n=== PREDICCIÓN CON NUEVOS DATOS ===\")\n",
    "x_new_data = [\n",
    "    \"Ese perro me robo mis cosas\", \n",
    "    \"ese hdp se llevo el dinero\", \n",
    "    \"mi app de calendario no sirve\",\n",
    "    \"gracias por tu ayuda\",\n",
    "    \"eres un idiota completo\"\n",
    "]\n",
    "x_new_data_tfidf = vec_tfidf.transform(x_new_data)\n",
    "x_new_data_tfidf = scaler.transform(x_new_data_tfidf)\n",
    "x_new_data_tfidf = x_new_data_tfidf.toarray().astype(np.float32)\n",
    "X_new_t = torch.from_numpy(x_new_data_tfidf)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_new = model(X_new_t)\n",
    "    y_pred_new_classes = torch.argmax(y_pred_new, dim=1)\n",
    "    print(\"Predicciones para nuevos datos:\")\n",
    "    for i, text in enumerate(x_new_data):\n",
    "        print(f\"  '{text}' -> {le.inverse_transform([y_pred_new_classes[i]])[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
